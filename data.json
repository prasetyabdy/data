{
"Sheet1":[
 {
  "No": 1,
  "judul": "Is Fragmentation a Threat to the Success of the Internet of Things?",
  "abstrak": "Internet of Things (IoT) aims to bring connectivity to almost every objects, i.e., things, found in the physical space. It extends connectivity to everyday things, however, such increase in the connectivity creates many prominent challenges. Context: Generally, IoT opens the door for new applications for machine-to-machine and human-to-human communications. The current trend of collaborating, distributed teams through the Internet, mobile communications, and autonomous entities, e.g., robots, is the first phase of the IoT to develop and deliver diverse services and applications. However, such collaborations is threatened by the fragmentation that we witness in the industry nowadays as it brings difficulty to integrate the diverse technologies of the various objects found in IoT systems. Diverse technologies induce interoperability issues while designing and developing various services and applications, hence, limiting the possibility of reusing thedata, more specifically, the software (including frameworks, firmware, applications programming interfaces, and user interfaces) as well as of facing issues, like security threats and bugs, when developing new services or applications. Different aspects of handling datacollection ranging from discovering smart sensors for datacollection, integrating and applying reasoning on them must be available to provide interoperability and flexibility to the diverse objects interacting in the system. However, such approaches are bound to be challenged in future IoT scenarios as they bring substantial performance impairments in settings with the very large number of collaborating devices and technologies. Objective: We raise the awareness of the community about the lack of interoperability among technologies developed for IoT and challenges that their integration poses. We also provide guidelines for researchers and practitioners interested in connecting IoT networks and devices to develop services and applications. Method: We appl...",
  "keyword": "internet of things"
 },
 {
  "No": 2,
  "judul": "Visual Object Detection and Tracking for Internet of Things Devices Based on Spatial Attention Powered Multidomain Network",
  "abstrak": "Internet of Things (IoT) has brought changes in many fields by joining physical space with the cyber space. The IoT devices are becoming increasingly complex. With the rapid deployment of cameras, tasks in IoT like visual information are more important, but IoT devices have limited computing resources, including power, computing ability, storage, etc. Some tasks that might be perfectly normal to perform on a computer would be rather challenging on an IoT device. Therefore, how to maintain acceptable performance while minimizing resources is becoming a more consequential part in IoT. In this article, we aim to solve the problem of object detection and tracking in IoT while minimizing resources. The traditional algorithms need to use a convolutional neural network (CNN) to identify different objects in each frame, and then determine the tracking target from identified objects, which typically requires a lot of computing resources. By incorporating spatial attention, and multidomain network, we proposed a novel algorithm named as spatial attention powered multidomain network (SA-MDNet). By adding the spatial attention mechanism to the original MDNet model, and using multiclass cross-entropy loss, we are able to distinguish the background and the target in different video sequences effectively and efficiently. This novel algorithm achieves similar performance on the OTB 50\/100\/2013 datasets compared to several state-of-art models, while uses only a fraction of the memory compared to MDNet.",
  "keyword": "internet of things"
 },
 {
  "No": 3,
  "judul": "Edge QoE: Computation Offloading With Deep Reinforcement Learning for Internet of Things",
  "abstrak": "In edge-enabled Internet of Things (IoT), computation offloading service is expected to offer users with better Quality of Experience (QoE) than traditional IoT. Unfortunately, the growing multiple tasks from users are occuring with the emergence of the IoT environment. Meanwhile, the current computation offloading with QoE is solved by deep reinforcement learning (DRL) with the issue of instability and slow convergence. Therefore, improving the QoE in edge-enabled IoT is still the ultimate challenge. In this article, to enhance the QoE, we propose a new QoE model to study the computation offloading. Specifically, the emerged QoE model can capture three influential elements: 1) service latency determined by local computing latency and transmission latency; 2) energy consumption according to local calculation and transmission consumption; and 3) task success rate based on the coding error probability. Moreover, we improve the deep deterministic policy gradients (DDPG) algorithm and propose a algorithm named the double-dueling-deterministic policy gradients (D\n3\nPG) based on the proposed model. Specifically, the actor network highly relies on the critic network, which makes the performance of the DDPG sensitive to the critic and thus leads to poor stability and slow convergence in the computation offloading process. To solve this, we redesign the critic network by using Double Q -learning and Dueling networks. Extensive experiments verify the better stability and faster convergence of our proposed algorithm than existing methods. In addition, experiments also indicate that our proposed algorithm can improve the QoE performance.",
  "keyword": "internet of things"
 },
 {
  "No": 4,
  "judul": "Explainable AI Over the Internet of Things (IoT): Overview, State-of-the-Art and Future Directions",
  "abstrak": "Explainable Artificial Intelligence (XAI) is transforming the field of Artificial Intelligence (AI) by enhancing the trust of end-users in machines. As the number of connected devices keeps on growing, the Internet of Things (IoT) market needs to be trustworthy for the end-users. However, existing literature still lacks a systematic and comprehensive survey work on the use of XAI for IoT. To bridge this lacking, in this paper, we address the XAI frameworks with a focus on their characteristics and support for IoT. We illustrate the widely-used XAI services for IoT applications, such as security enhancement, Internet of Medical Things (IoMT), Industrial IoT (IIoT), and Internet of City Things (IoCT). We also suggest the implementation choice of XAI models over IoT systems in these applications with appropriate examples and summarize the key inferences for future works. Moreover, we present the cutting-edge development in edge XAI structures and the support of sixth-generation (6G) communication services for IoT applications, along with key inferences. In a nutshell, this paper constitutes the first holistic compilation on the development of XAI-based frameworks tailored for the demands of future IoT use cases.",
  "keyword": "internet of things"
 },
 {
  "No": 5,
  "judul": "Recommendation Based on Large-Scale Many-Objective Optimization for the Intelligent Internet of Things System",
  "abstrak": "Recommender systems are of great significance for the generated by the Internet of Things (IoT) and are important for the intelligent IoT systems. The traditional recommendation algorithms only consider the accuracy as the optimization objective. In this article, a many-objective optimization model consisting of the F1 measure, recommendation novelty, recommendation coverage, customer satisfaction, landmark similarity, and overfitting is constructed for recommendation. Then, to improve the recommendation performance, we propose to use a large-scale many-objective optimization algorithm based on problem transformation (LSMaOA) to optimize the matrix factorization model for the recommender system in the intelligent IoT systems. The experimental results show that LSMaOA is robust and can effectively optimize the model’s six objectives. Compared with the knee point-driven evolutionary algorithm (KnEA), the grid-based evolutionary algorithm (GrEA), the large-scale multiobjective optimization framework (LSMOF), and the reference vector guided evolutionary algorithm (RVEA), the proposed algorithm can promote the F1 measure by 7.78%, 13.63%, 21.85%, and 28.63%, respectively.",
  "keyword": "internet of things"
 },
 {
  "No": 6,
  "judul": "Comments on “Biometrics-Based Privacy-Preserving User Authentication Scheme for Cloud-Based Industrial Internet of Things Deployment”",
  "abstrak": "Very recently, Das et al. (IEEE Internet of Things Journal, pp. 4900-4913, 5(6), DOI: 10.1109\/JIOT.2018.2877690, 2018) presented a biometric-based solution for security and privacy in Industrial Internet of Things architecture. Das et al. claimed that their protocol is secure against known attacks. However, this comment shows that their protocol is defenseless against stolen verifier, stolen smart device, and traceability attacks. The attacker having access to public parameters and any of the verifier and parameters stored in smart device can easily expose the session key shared among the user and the smart device. Moreover, their protocol fails to provide perfect forward secrecy. Finally, this article also provides some necessary guidelines on attack resilience for the authentication schemes based on merely the symmetric key primitives, which are overlooked by Das et al.",
  "keyword": "internet of things"
 },
 {
  "No": 7,
  "judul": "Policy-Based Secure and Trustworthy Sensing for Internet of Things in Smart Cities",
  "abstrak": "The Internet of Things (IoT), which is known as one of the key enabling technologies of smart cities, generally refers to the network of smart objects, which are embedded with sensing, computing, networking, and actuating capabilities that all together enable them to collect and exchangedata. The IoT devices are usually wirelessly networked, and they serve as a key enabling technology for many critical smart city applications, such as intelligent transportation, smart grid, smart building, and mobile healthcare. However, security has become a key challenge for the wide deployment of IoT: because of the environmental influences, the IoTdata are inherently noisy. Moreover, the IoT devices may be compromised by attackers to intentionally generate fakedata. Finally, the underlying wireless network can also be subverted. To address the security issue in IoT, we propose a policy-based secure and trustworthy sensing scheme for IoT named RealAlert, in which the trustworthiness of bothdata and the IoT devices are evaluated based on both the reporting history and the context in which thedata are collected using policy rules. Experimental results have shown that the RealAlert scheme can accurately assess the trust of the sensor nodes as well asdata in IoT.",
  "keyword": "internet of things"
 },
 {
  "No": 8,
  "judul": "Aggregated Traffic Models for Real-World Dataset in the Internet of Things",
  "abstrak": "Traffic models play a key role in the analysis, design and simulation of communication networks. The availability of accurate models is essential to investigate the impact of traffic patterns created by the introduction of new services such as those forecasted for the Internet of Things (IoT). The Poisson model has historically been a popular aggregated traffic model and has been extensively used by the IoT research community. However, the Poisson model implicitly assumes an infinite number of traffic sources, which may not be a valid assumption in various plausible application scenarios. The practical conditions under which the Poisson model is valid in the context of IoT have not been fully investigated, in particular under a finite (and possibly reduced) number of traffic sources with random inter-arrival times. In this context, this letter derives exact mathematical models for the packet inter-arrival times of aggregated IoT dataset traffic based on the superposition of a finite number of traffic sources, each of which is modelled based on real-world experimental dataset from typical IoT sensors (temperature, light and motion). The obtained exact models are used to explore the validity of the Poisson model, showing that it can be extremely inaccurate when a reduced number of traffic sources is considered. Finally, an illustrative example is presented to show the importance of having accurate and realistic models such as those presented in this letter.",
  "keyword": "internet of things"
 },
 {
  "No": 9,
  "judul": "Empirical Frequency-Dependent Wall Insertion Loss Model at 3–6 GHz for Future Internet-of-Things Applications",
  "abstrak": "A novel frequency-dependent wall insertion loss model at 3–6 GHz is proposed in this paper. The frequency-dependence of the wall insertion loss is modeled by the Fourier triangular basis neural network. A method to determine the optimal weighted vector and the number of the neurons is introduced. In addition, the impact of the wider continuous spectrum on the wall insertion loss is analyzed and extensive measurements are performed to validate the proposed model. The results obtained with the proposed model match better with the measured results than other models. The proposed model can be used in future indoor Internet-of-Things applications such as service computing.",
  "keyword": "internet of things"
 },
 {
  "No": 10,
  "judul": "Enhancing Identifier\/Locator Splitting Through Social Internet of Things",
  "abstrak": "In recent years identifier\/locator splitting (ILS) has been proposed as a promising future Internet solution to the so-called “semantic overload of addresses” problem. It is currently under debate whether the ILS approach, which entails identifier-to-locator resolution procedures, could ensure scalability, session continuity, and mobility across heterogeneous multiaccess networks as demanded by Internet of Things (IoT). In this paper, it is proposed to exploit the Social IoT concept to address the above issues. More specifically, a scheme is introduced that browses the social graph of devices to find information about the locator of the intended destination. Results show that the proposed solution outperforms the alternative ILS approaches in terms of number of hops and latency incurred to accomplish the resolution procedure, at the cost of a slight increase in the storage demands to track social relationships.",
  "keyword": "internet of things"
 },
 {
  "No": 11,
  "judul": "Guest Editorial: Special Section on 5G Edge Computing-Enabled Internet of Medical Things",
  "abstrak": "The relationship between computing and healthcare has a long history, but adoption of telemedicine is gradual due to political resistance, lack of infrastructure development frameworks, and lack of resources. One of the most rapid technological advancements will be the Internet of Medical Things (IoMT), which is predicted to bring about the greatest technological delivery ever. Edge computing in conjunction with 5G speed is the solution to achieve the requirements of quality of service metrics metrics during the analysis of clinicaldata. Artificial intelligence with edge computing has made significant contributions to the smart healthcare system's network for ultra-reliable communication in the areas of less delay, widespread device connectivity, and enhanced speed of datatransmission. Since the edge-enabled IoMT-based system in the healthcare system offers a number of extraordinary potential, this Special Issue explores those areas of applicability. The aim of Special Issue is to cover the research difficulties associated with the implementation of edge computing-based IoMT systems in the healthcare system and suggests a framework for such a system.",
  "keyword": "internet of things"
 },
 {
  "No": 12,
  "judul": "From Cloud Down to Things: An Overview of Machine Learning in Internet of Things",
  "abstrak": "With the numerous Internet of Things (IoT) devices, the cloud-centric dataset processing fails to meet the requirement of all IoT applications. The limited computation and communication capacity of the cloud necessitate the edge computing, i.e., starting the IoT dataset processing at the edge and transforming the connected devices to intelligent devices. Machine learning (ML) the key means for information inference, should extend to the cloud-to-things continuum too. This paper reviews the role of ML in IoT from the cloud down to embedded devices. Different usages of ML for application dataset processing and management tasks are studied. The state-of-the-art usages of ML in IoT are categorized according to their application domain, input dataset type, exploited ML techniques, and where they belong in the cloud-to-things continuum. The challenges and research trends toward efficient ML on the IoT edge are discussed. Moreover, the publications on the “ML in IoT” are retrieved and analyzed systematically using ML classification techniques. Then, the growing topics and application domains are identified.",
  "keyword": "internet of things"
 },
 {
  "No": 13,
  "judul": "Bringing Deep Learning at the Edge of Information-Centric Internet of Things",
  "abstrak": "Various Internet solutions take their power processing and analysis from cloud computing services. Internet of Things (IoT) applications started discovering the benefits of computing, processing, and analysis on the device itself aiming to reduce latency for time-critical applications. However, on-device processing is not suitable for resource-constraints IoT devices. Edge computing (EC) came as an alternative solution that tends to move services and computation more closer to consumers, at the edge. In this letter, we study and discuss the applicability of merging deep learning (DL) models, i.e., convolutional neural network (CNN), recurrent neural network (RNN), and reinforcement learning (RL), with IoT and information-centric networking which is a promising future Internet architecture, combined all together with the EC concept. Therefore, a CNN model can be used in the IoT area to exploit reliably dataset from a complex environment. Moreover, RL and RNN have been recently integrated into IoT, which can be used to take the multi-modality of dataset in real-time applications into account.",
  "keyword": "internet of things"
 },
 {
  "No": 14,
  "judul": "Advancing NovaGenesis Architecture Towards Future Internet of Things",
  "abstrak": "Internet of Things (IoT) has been deeply challenging current Internet and emerging architectures, including 5G and future Internet. Many architectural limitations, such as weak security, datadistribution efficiency, provenance and traceability of sources, excessive human intervention, lack of interoperability, and service-awareness in devices configuration, have been exposed and called the society attention. This paper addresses these limitations by properly integrating five strategies: 1) efficient IoT dataexchanging, storage and processing via information-centric networking (ICN); 2) contract-based IoT services composition; 3) software-control\/management of IoT devices accordingly to the services requirements; 4) naming and name resolution of the physical and virtual entities, proving identifier\/locator splitting and contextualized self-organization; and 5) name-based routing and network caching. Considering the current state-of-the art, the main contributions of this paper can be summarized as follows: proposal of a novel service-defined architecture (SDA), in which device configurations are a reflex of the real service needs (given by established contracts); combination of ICN benefits with named-services; and perennial identification of IoT devices, services, and datausing self-verifiable naming. All integration work has been supported by a convergent architecture called NovaGenesis (NG), demonstrating its viability as an alternative for the current IoT architectures. A proof-of-concept prototype has been implemented in laboratory under real conditions. The experimental results indicate competitive performance in terms ofdata transfer, memory and CPU consumption. Embedded NG has smaller RAM and ROM requirements when compared to a similar RPL + 6LowPAN stack. Dataexchanging has been performed in few milliseconds in a local area network.",
  "keyword": "internet of things"
 },
 {
  "No": 15,
  "judul": "Toward Future Internet of Things Experimentation and Evaluation",
  "abstrak": "The Internet of Things (IoT) and its current Internet engineering task force and IEEE dual stack model have been facing limitations, requiring applications to overcome tough constraints by their own, narrowing the full potential of this concept. For instance, the IoT dual stack model does not entirely address or support perennial naming, immutable dataprovenance, dataaccountability, mobility support, and services’ self-organization. On the other hand, future Internet architecture (FIA) research emerges as a viable candidate to compensate these gaps through evolutionary and revolutionary efforts. Regardless of the FIA proposal model, this field lacks a suitable environment that enables the performance experimentation and evaluation of distinct protocols and architectures fairly and transparently. This article proposes a novel IoT experimentation and evaluation environment based on a low-cost and open-source solution. The proposed environment integrates technologies of containers, IoT nodes emulation, and network simulation to evaluate the current IoT dual stack and a promising FIA approach called eXpressive Internet architecture. Moreover, this work combines the mentioned disjointed technologies, exploiting Docker containers, Contiki open-source operational system for IoT, and Cooja the IoT network simulator. In this way, it addresses the objective of fostering an honest experimentation and evaluation environment for FIAs. Experimental results demonstrate the feasibility and effectiveness of this approach, broadening this field and sowing a path to work with other FIAs.",
  "keyword": "internet of things"
 },
 {
  "No": 16,
  "judul": "Zero-Forcing-Based Downlink Virtual MIMO–NOMA Communications in IoT Networks",
  "abstrak": "To support massive connectivity and boost spectral efficiency for Internet of Things (IoT), a downlink scheme combining virtual multiple-input-multiple-output (MIMO) and nonorthogonal multiple access (NOMA) is proposed. All the single-antenna IoT devices in each cluster cooperate with each other to establish a virtual MIMO entity, and multiple independent datastreams are requested by each cluster. NOMA is employed to superimpose all the requested datastreams, and each cluster leverages zero-forcing detection to demultiplex the input datastreams. Only statistical channel state information (CSI) is available at the base station to avoid the waste of the energy and bandwidth on frequent CSI estimations. The outage probability and goodput of the virtual MIMO-NOMA system are thoroughly investigated by considering the Kronecker model, which embraces both the transmit and receive correlations. Furthermore, the asymptotic results facilitate not only the exploration of physical insights but also the goodput maximization. In particular, the asymptotic outage expressions provide quantitative impacts of various system parameters and enable the investigation of diversity-multiplexing tradeoff (DMT). Moreover, power allocation coefficients and\/or transmission rates can be properly chosen to achieve the maximal goodput. By favor of the Karush-Kuhn-Tucker conditions, the goodput maximization problems can be solved in closed form, with which the joint power and rate selection is realized by using alternately iterating optimization. Besides, the optimization algorithms tend to allocate more power to clusters under unfavorable channel conditions and support clusters with a higher transmission rate under benign channel conditions.",
  "keyword": "internet of things"
 },
 {
  "No": 17,
  "judul": "Identification and Resolution for Industrial Internet: Architecture and Key Technology",
  "abstrak": "As a key component of Industrial Internet, identification and resolution has been considered as a promising technology to realize the interconnections among physical and virtual entities and to promote interoperability of digital objects, which improves the industrial efficiency and transparency of the supply chain. In this article, we focus on the identification and resolution design in the Industrial Internet. A general identification and resolution architecture is proposed to guide how to develop and improve technologies and schemes in terms of the joint requirements of service, role, function, implement, and security in Industrial Internet. Based on the proposed architecture, three key technologies are studied. Specifically, an identifiable digital object (IDO) model is constructed to enable the different Industrial Internet platforms to manage and interact datauniformly. Then, a hybrid structure-based identification and resolution system is designed, based on which identification registration and resolution procedures are proposed. Furthermore, a trustable system based on blockchain is deployed to guarantee the datacredibility. Finally, extensive practical experiments validate the effectiveness of the proposed technologies. The proposed architecture and technologies have been used in the practical construction and deployment of national top-level nodes and secondary-level nodes to sustain the identification and resolution services for the Industrial Internet in China.",
  "keyword": "internet of things"
 },
 {
  "No": 18,
  "judul": "6G Internet of Things: A Comprehensive Survey",
  "abstrak": "The sixth-generation (6G) wireless communication networks are envisioned to revolutionize customer services and applications via the Internet of Things (IoT) toward a future of fully intelligent and autonomous systems. In this article, we explore the emerging opportunities brought by 6G technologies in IoT networks and applications, by conducting a holistic survey on the convergence of 6G and IoT. We first shed light on some of the most fundamental 6G technologies that are expected to empower future IoT networks, including edge intelligence, reconfigurable intelligent surfaces, space–air–ground–underwater communications, Terahertz communications, massive ultrareliable and low-latency communications, and blockchain. Particularly, compared to the other related survey papers, we provide an in-depth discussion of the roles of 6G in a wide range of prospective IoT applications via five key domains, namely, healthcare IoTs, Vehicular IoTs and Autonomous Driving, Unmanned Aerial Vehicles, Satellite IoTs, and Industrial IoTs. Finally, we highlight interesting research challenges and point out potential directions to spur further research in this promising area.",
  "keyword": "internet of things"
 },
 {
  "No": 19,
  "judul": "Internet of Video Things: Next-Generation IoT With Visual Sensors",
  "abstrak": "The worldwide flourishing of the Internet of Things (IoT) in the past decade has enabled numerous new applications through the internetworking of a wide variety of devices and sensors. More recently, visual sensors have seen their considerable booming in IoT systems because they are capable of providing richer and more versatile information. Internetworking of large-scale visual sensors has been named Internet of Video Things (IoVT). IoVT has its own unique characteristics in terms of sensing, transmission, storage, and analysis, which are fundamentally different from the conventional IoT. These new characteristics of IoVT are expected to impose significant challenges to existing technical infrastructures. In this article, an overview of recent advances in various fronts of IoVT will be introduced and a broad range of technological and system challenges will be addressed. Several emerging IoVT applications will be discussed briefly to illustrate the potentials of IoVT in a broad range of practical scenarios.",
  "keyword": "internet of things"
 },
 {
  "No": 20,
  "judul": "Toward the Internet of Things for Physical Internet: Perspectives and Challenges",
  "abstrak": "The Physical Internet (PI, or π) paradigm has been developed to be a global logistics system that aims to move, handle, store, and transport logistics products in a sustainable and efficient way. To achieve the goal, the PI requires a high-level interconnectivity in the physical, informational, and operational aspects enabled by an interconnected network of intermodal hubs, collaborative protocols, and standardized, modular, and smart containers. In this context, PI is a key player poised to benefit from the Internet-of-Things (IoT) revolution since it potentially provides an end-to-end visibility of the PI objects, operations, and systems through ubiquitous information exchange. This article is to investigate opportunities of application of the IoT technology in the PI vision. In addition, an IoT ecosystem (π-IoT) encompassing key enabling IoT technologies, building blocks, and a service-oriented architecture (SoA) is proposed as a potential component for accelerating the implementation of PI. The major challenges regarding the deployment of IoT into the emerging logistics concept are also discussed intensively for further research.",
  "keyword": "internet of things"
 },
 {
  "No": 21,
  "judul": "Optimal Resource Allocation in Energy-Efficient Internet-of-Things Networks With Imperfect CSI",
  "abstrak": "Internet of Things (IoT) is an emerging networking paradigm that enhances smart device communications through Internet-enabled systems. Due to massive IoT devices connectivity with economic and greenhouse emission effects, the energy-efficiency poses critical concerns. Under imperfect channel state information (CSI), this article investigates joint optimization of user selection, power allocation, and the number of activated base station (BS) antennas of multiple IoT devices considering the transmit power and different Quality-of-Service (QoS) requirements in combinatorial mode to maximize energy-efficiency. The optimization problem formulated is a nonconvex mixed-integer nonlinear programming, which is NP-hard with no practical solution. The primal optimization problem is transformed into a tractable convex optimization problem and separated into inner and outer loop subproblems. This article proposes a joint energy-efficient iterative algorithm, which utilizes a successive convex approximation technique and the Lagrangian dual decomposition method to achieve near-optimal solutions with guaranteed convergence. The simulation results are provided to evaluate the proposed algorithm and its significant performance gain over the baseline algorithms in terms of energy-efficiency maximization.",
  "keyword": "internet of things"
 },
 {
  "No": 22,
  "judul": "Robust Resource Allocation for Lightweight Secure Transmission in Multicarrier NOMA-Assisted Full Duplex IoT Networks",
  "abstrak": "In this article, with the aim to enhance the secure transmission and improve the utilization of spectrum resources in Internet of Things (IoT), a multicarrier nonorthogonal multiple access (MC-NOMA)-assisted full duplex (FD) network is investigated, in which nonorthogonal multiple access (NOMA) is implemented in both uplink and downlink transmissions. The lightweight and low-power physical layer security (PLS) technology is employed to protect the information from eavesdropping. Taking the imperfect channel state information (CSI) into account, we formulate a problem to optimize the beamforming vector, artificial noise (AN), transmit power, and subcarrier assignment policy aiming to maximize the worst case sum secrecy rate under the Quality of Service (QoS) and power consumption constraints. Since the formulated problem is nonconvex and difficult to be solved, we decompose it into two joint optimization subproblems. The first is resource allocation with given subcarrier assignment, which is solved by using the block coordinate descent (BCD) approach. The second is subcarrier assignment solved by the matching theory. Our simulation shows that the proposed scheme is robust against the CSI imperfectness of the eavesdropping and self-interference channels, while providing significant sum secrecy rate improvement compared with the orthogonal multiple access (OMA), half duplex (HD) systems, and other benchmark schemes.",
  "keyword": "internet of things"
 },
 {
  "No": 23,
  "judul": "Intelligent Fault Identification for Industrial Internet of Things via Prototype-Guided Partial Domain Adaptation With Momentum Weight",
  "abstrak": "Partial domain adaptation (PDA) for fault identification has been widely researched to help construct self-monitoring systems in the era of the Industrial Internet of Things (IIoT). However, the existing PDA fault identification methods neglect the influence of uncertainty of the target domain on the identification performance. To solve this problem, this work developed a prototype-guided PDA method with momentum weight for fault diagnosis. Specifically, to reduce the risk of ruling out the outlier by the output of a classifier or a discriminator, a classwise selectively source weighting strategy that follows the number of the target pseudo labels is proposed. The target instances’ pseudo labels, which are obtained by calculating the distance between the target instance and the source prototypes, are irrelevant to the classifier and discriminator. Furthermore, the momentum algorithm, by which the historical weights information could be retained, is employed in the source weights calculation procedure to alleviate the fluctuation and more closely to the global optimal. Experiments demonstrated the effectiveness and superiority of the developed method.",
  "keyword": "internet of things"
 },
 {
  "No": 24,
  "judul": "From Micro to Macro IoT: Challenges and Solutions in the Integration of IEEE 802.15.4\/802.11 and Sub-GHz Technologies",
  "abstrak": "Research efforts in the field of Internet of Things (IoT) are providing solutions in building new types of “network of networks,” going beyond the technological barriers due to intrinsic limitations of the constrained devices typically used in this context. Thanks to the improvement in communication\/networking protocols and the hardware cost reduction, it is now possible to define new IoT architectures, combining the “micro” IoT paradigm, based on short-range radio technologies (e.g., IEEE 802.15.4 and IEEE 802.11), with the rising “macro” IoT paradigm, based on sub-GHz radio technologies. This allows the implementation of scalable network architectures, able to collect datacoming from constrained devices and process them in order to provide useful services and applications to final consumers. In this paper, we focus on practical integration between micro and macro IoT approaches, providing architectural and performance details for a set of experimental tests carried out in the campus of the University of Parma. We then discuss challenges and solutions of the proposed micro-macro integrated IoT systems.",
  "keyword": "internet of things"
 },
 {
  "No": 25,
  "judul": "Tornado: Enabling Blockchain in Heterogeneous Internet of Things Through a Space-Structured Approach",
  "abstrak": "With the widespread applications of the Internet of Things (IoT), e.g., smart city, business, healthcare, etc., the security ofdata and devices becomes a major concern. Although blockchain can effectively enhance the network security and achieve fault tolerance, the huge resource consumption and limited performance ofdata processing restrict its deployments in IoT scenarios. Observing the heterogeneity and resource constraints, we intend to make blockchain accommodate both wimpy and brawny IoT devices. In this article, we present Tornado, a high-performance blockchain system based on space-structured ledger and corresponding algorithms, to enable blockchain in IoT. Specifically, we first design a space-structured chain architecture with noveldata structures for promoting the network scalability. To address the huge heterogeneity of IoT, a novel consensus mechanism named collaborative-proof of work is developed. Moreover, we propose the space-structured greedy heaviest-observed subtree (S\n2\nGHOST) protocol for improving the resource efficiency of IoT devices. Additionally, a dynamic weight assignment mechanism in S\n2\nGHOST contributes to reflect the trustworthiness ofdata and devices. Extensive experiments demonstrate that Tornado can achieve a maximum throughput of 3464.76 transactions per second. The optimizations of propagation latency and resource efficiency are 68.14% and 30.56%, respectively.",
  "keyword": "internet of things"
 },
 {
  "No": 26,
  "judul": "Dynamic Connectivity Game for Adversarial Internet of Battlefield Things Systems",
  "abstrak": "In this paper, the problem of network connectivity is studied for an adversarial Internet of Battlefield Things (IoBT) system in which an attacker aims at disrupting the connectivity of the network by choosing to compromise one of the IoBT nodes at each time epoch. To counter such attacks, an IoBT defender attempts to reestablish the IoBT connectivity by either deploying new IoBT nodes or by changing the roles of existing nodes. This problem is formulated as a dynamic multistage Stackelberg connectivity game that extends classical connectivity games and that explicitly takes into account the characteristics and requirements of the IoBT network. In particular, the defender's payoff captures the IoBT latency as well as the sum of weights of disconnected nodes at each stage of the game. Due to the dependence of the attacker's and defender's actions at each stage of the game on the network state, the feedback Stackelberg solution [feedback Stackelberg equilibrium (FSE)] is used to solve the IoBT connectivity game. Then, sufficient conditions under which the IoBT system will remain connected, when the FSE solution is used, are determined analytically. Numerical results show that the expected number of disconnected sensors, when the FSE solution is used, decreases up to 46% compared to a baseline scenario in which a Stackelberg game with no feedback is used, and up to 43% compared to a baseline equal probability policy.",
  "keyword": "internet of things"
 },
 {
  "No": 27,
  "judul": "The Internet of Things in the Oil and Gas Industry: A Systematic Review",
  "abstrak": "The low oil price environment is driving the oil and gas (O&G) industry to become more innovative and deploy smart field technologies, to increase operational and asset efficiency, minimize health, safety, and environmental (HSE) risks, improve asset portfolio, reduce capital and operation costs, and maximize capital productivity. The Internet of Things (IoT) is at the forefront of this digital transformation, enabling seamless real-time dataset collection, processing, and analysis from a range of equipment, processes, and operations to achieve these objectives. There are various operations\/applications in the upstream, midstream, and downstream sectors (e.g., condition-based monitoring and location tracking) for which IoT-enabled solutions have a significant impact and offer a range of opportunities to increase socioeconomic benefits. However, there are several impediments (e.g., vulnerability to cyber attacks, lower technological readiness for deploying in zone-0 and zone-1 hazardous environments, unavailability of communication infrastructure, labor concerns, and maintenance and obsolescence) that slow the pace of adoption of IoT technologies for regular upstream, midstream, and downstream operations. This review article provides an overview and assessment of the role, impact, opportunities, challenges, and current status of IoT deployment in the O&G industry.",
  "keyword": "internet of things"
 },
 {
  "No": 28,
  "judul": "Collaborative Flow-Identification Mechanism for Software-Defined Internet of Things",
  "abstrak": "Due to the lack of unified standards in the Internet of Things (IoT), heterogeneity in terms of protocol and packet types exist. In such a case, accurate traffic identification using port-based and payload-based solutions are not suitable for flow processing in Software-Defined IoT (SD-IoT). In this article, we propose \niAcceSD\n, an intelligent Access Node (SD-Access) and SDN controller (SD-Controller) collaborated traffic identification mechanism for SD-IoT. Concerning the heterogeneous and unknown traffic flows in IoT, iAcceSD uses machine learning (ML)-based traffic identification mechanisms at the access node. The SD-Controller trains a lightweight ML module for a specific SD-Access node dealing with a similar set of traffic flows. By processing flows at the edge, network latency is reduced in the proposed scheme. An optimization model is developed to program the SD-Access nodes considering the heterogeneous and unknown traffic. Thorough performance analysis of iAcceSD shows improvement in latency by 34% and controller overheads by 23.4%, compared to the existing state of the art, with a simultaneous improvement in energy consumption and packet delivery ratio.",
  "keyword": "internet of things"
 },
 {
  "No": 29,
  "judul": "Toward Achieving a Balance Between the User Satisfaction and the Power Conservation in the Internet of Things",
  "abstrak": "In the recent era of digital revolution, life has become simple and more comfortable than it was before. Today, we are witnessing a revolution in the information technology domain where the Internet of Things is employed in various aspects of our life. The impact of this revolution is clearly seen in our homes where appliances are controlled and managed via voice commands, inspiration, hand signals, or by a command that results from analyzing the human behavior. Power conservation is one of the significant issues in smart homes and smart cities where buildings consume about 40% of the total energy. Another crucial issue is the user satisfaction. Achieving a balance between power conservation and the user satisfaction is a challenge. In this article, our contributions are: 1) a survey that sheds light on various techniques (the state of the art) used for reducing the power consumption based on monitoring the occupant's behavior; 2) a comparison between these techniques based on various factors elicited from the literature review; and 3) this study reveals the following gaps in the previous work: a) lack of integrity between the IoT systems; b) lack of auto measuring of the user satisfaction; and c) lack of achieving a balance between the user satisfaction and the power saving. As an attempt to close the above gaps, this article proposes a smart and integrated IoT framework for automeasuring of the user satisfaction and thus, achieves a balance between the power conservation and the user satisfaction. Besides, it suggests future research directions for researchers.",
  "keyword": "internet of things"
 },
 {
  "No": 30,
  "judul": "Internet of Things for Agricultural Applications: The State of the Art",
  "abstrak": "The advent of the Internet of Things (IoT) inspired various new and enhanced sets of applications in multiple domains including agriculture. The recent drive in the adoption of IoT technologies offers a major enhancement for the agricultural sectors in terms of efficiency and scalability. In this article, we investigate the specific issues and challenges associated with IoT, and review various IoT architectures, communication, middleware, and information processing technologies. We, then, discuss few IoT applications for agriculture-presenting various case studies to thoroughly analyze the solutions along with their design and implementation related parameters. Consequently, we provide a comprehensive review of the available simulation tools, datasets, and testbeds which provisions experimentation with IoT in agriculture. We enumerate open issues and challenges present in enabling IoT for agriculture. Finally, this article concludes while giving directions for future research.",
  "keyword": "internet of things"
 },
 {
  "No": 31,
  "judul": "Caching in Energy Harvesting Aided Internet of Things: A Game-Theoretic Approach",
  "abstrak": "The Internet of Things (IoT) sensing service enables users to monitor the ambient environment by fetching datafrom IoT sensors. The explosive growth of mobile users and IoT applications injects massive traffic to the IoT network and also speeds up the drainage of sensor batteries. Caching at the IoT gateway (GW), which stores the IoT dataand directly send them to the users, can avoid activating sensors too frequently, hence reducing the traffic in the IoT network as well as the energy consumption of sensors. To overcome the limited energy capacity of sensors, energy transmitters (ETs) are deployed to charge them. Practically, the GW and ETs may be owned by different operators, and the GW operator needs to incentivize ETs to provision the charging service. In this paper, we formulate a Stackelberg game in the cache-enabled energy harvesting aided IoT framework to improve the user quality of service. Caching strategies, incentive strategies, and ET transmission power strategies are jointly optimized to find the Stackelberg equilibrium by our proposed alternative direction approach. Simulation results elicit the benefits of our framework and demonstrate the performances of our proposed algorithm.",
  "keyword": "internet of things"
 },
 {
  "No": 32,
  "judul": "Internet of Things for Smart Railway: Feasibility and Applications",
  "abstrak": "The explosively growing demand of Internet of Things (IoT) has rendered broadscale advancements in the fields across sensors, radio access, network, and hardware\/software platforms for mass market applications. In spite of the recent advancements, limited coverage and battery for persistent connections of IoT devices still remains a critical impediment to practical service applications. In this paper, we introduces a cost-effective IoT solution consisting of device platform, gateway, IoT network, and platform server for smart railway infrastructure. Then, we evaluate and demonstrate the applicability through an in-depth case study related to IoT-based maintenance by implementing a proof of concept and performing experimental works. The IoT solution applied for the smart railway application makes it easy to grasp the condition information distributed over a wide railway area. To deduce the potential and feasibility, we propose the network architecture of IoT solution and evaluate the performance of the candidate radio access technologies for delivering IoT datasets in the aspects of power consumption and coverage by performing an intensive field test with system level implementations. Based on the observation of use cases in interdisciplinary approaches, we figure out the benefits that the IoT can bring.",
  "keyword": "internet of things"
 },
 {
  "No": 33,
  "judul": "Securing the Internet of Things in the Age of Machine Learning and Software-Defined Networking",
  "abstrak": "The Internet of Things (IoT) realizes a vision where billions of interconnected devices are deployed just about everywhere, from inside our bodies to the most remote areas of the globe. As the IoT will soon pervade every aspect of our lives and will be accessible from anywhere, addressing critical IoT security threats is now more important than ever. Traditional approaches where security is applied as an afterthought and as a “patch” against known attacks are insufficient. Indeed, next-generation IoT challenges will require a new secure-by-design vision, where threats are addressed proactively and IoT devices learn to dynamically adapt to different threats. To this end, machine learning (ML) and software-defined networking (SDN) will be key to provide both reconfigurability and intelligence to the IoT devices. In this paper, we first provide a taxonomy and survey the state of the art in IoT security research, and offer a roadmap of concrete research challenges related to the application of ML and SDN to address existing and next-generation IoT security threats.",
  "keyword": "internet of things"
 },
 {
  "No": 34,
  "judul": "Context-Aware Computing, Learning, and Big Data in Internet of Things: A Survey",
  "abstrak": "Internet of Things (IoT) has been growing rapidly due to recent advancements in communications and sensor technologies. Meanwhile, with this revolutionary transformation, researchers, implementers, deployers, and users are faced with many challenges. IoT is a complicated, crowded, and complex field; there are various types of devices, protocols, communication channels, architectures, middleware, and more. Standardization efforts are plenty, and this chaos will continue for quite some time. What is clear, on the other hand, is that IoT deployments are increasing with accelerating speed, and this trend will not stop in the near future. As the field grows in numbers and heterogeneity, “intelligence” becomes a focal point in IoT. Since data now becomes “big data,” understanding, learning, and reasoning with big data is paramount for the future success of IoT. One of the major problems in the path to intelligent IoT is understanding “context,” or making sense of the environment, situation, or status using data from sensors, and then acting accordingly in autonomous ways. This is called “context-aware computing,” and it now requires both sensing and, increasingly, learning, as IoT systems get more data and better learning from this big data. In this survey, we review the field, first, from a historical perspective, covering ubiquitous and pervasive computing, ambient intelligence, and wireless sensor networks, and then, move to context-aware computing studies. Finally, we review learning and big data studies related to IoT. We also identify the open issues and provide an insight for future study areas for IoT researchers.",
  "keyword": "internet of things"
 },
 {
  "No": 35,
  "judul": "A Pseudopacket Scheduling Algorithm for Protecting Source Location Privacy in the Internet of Things",
  "abstrak": "The massive growth in interconnected devices from a multiplicity of networks goes hand-in-hand with the emergence of the Internet of Things (IoT) paradigm. As a critical component of the IoT, sensor networks have become ubiquitous and widely used in various application domains. However, open-ended wireless communication brings severe threats to user privacy and security. Attackers from outside the network can trace back along thedata stream to capture the source node, which poses a significant threat to the privacy of the datasource. A feasible defense method is to interfere with the attacker’s tracking process through forged datastreams. However, the related traditional solutions generally have shortcomings in terms of balancing security and efficiency. Therefore, this article proposes a pseudopacket scheduling algorithm (PPSA), which aims at reasonably regulating the process of pseudopacket generation to interfere with the adversary’s tracking to the datasource. The algorithm comprises three phases. First, the sink node performs geographic information acquisition and neighbor node discovery with a flood-based method. Then, the sink node uses a self-adapting proxy selection method to construct backbone routes with both randomness and low latency to receive actual packets. Finally, the nodes on both sides of the backbone routes follow a pseudopacket scheduling strategy to interfere with the adversary’s tracking of the source locations. The experimental results showcase that our proposed scheme effectively controls the additional energy consumption and transmission delays within acceptable ranges while ensuring adequate location privacy.",
  "keyword": "internet of things"
 },
 {
  "No": 36,
  "judul": "Using Active Queue Management to Assist IoT Application Flows in Home Broadband Networks",
  "abstrak": "Internet of Things (IoT) applications such as telehealth, smart appliances, and smart energy are becoming more common within the home. However, they must compete for bandwidth with traditional applications such as video streaming, video conferencing, and bulk file transfers. Such competition can be detrimental to the IoT applications when home gateways use traditional first-in-first-out (FIFO) queue management. Simply increasing bandwidth between the home gateway and the Internet Service Provider (ISP), even when possible, provides no guarantee of bandwidth for IoT applications since many traditional applications will consume as much bandwidth as is available. In this paper, we explore whether active queue management (AQM), now being implemented in home gateways, can provide protection for IoT flows. We investigate the effect of different AQM algorithms deployed at the home gateway in scenarios with multiple concurrent application flows. We find that deploying multiqueue FlowQueue Controlled Delay (FQ-CoDel) or the hybrid FlowQueue Proportional Integral Controller Enhanced (FQ-PIE) at the home gateway can provide excellent capacity sharing, flow isolation, and good protection in terms of throughput and queuing delays for IoT flows and other applications, which cannot be achieved with traditional FIFO or other single-queue AQMs such as Proportional Integral Controller Enhanced (PIE).",
  "keyword": "internet of things"
 },
 {
  "No": 37,
  "judul": "Universal Forgery Attacks on Remote Authentication Schemes for Wireless Body Area Networks Based on Internet of Things",
  "abstrak": "Recently, in IEEE INTERNET OF THINGS JOURNAL (DOI: 10.1109\/JIOT.2018.2876133), Saeed et al. proposed a lightweight online\/offline certificateless signature scheme, L-OOCLS, and proposed a heterogeneous remote anonymous authentication protocol (HRAAP) based on L-OOCLS for remote wireless body area networks users to enjoy various healthcare services on Internet of Things applications. In this paper, we show that L-OOCLS is entirely broken: anyone can forge certificateless signatures on any messages for any identities from only publicly known information. Thus, the scheme is trivially insecure against the type I adversary who can replace user public keys and the type II adversary who knows the master secret key. Our result shows that their security proofs are also flawed.",
  "keyword": "internet of things"
 },
 {
  "No": 38,
  "judul": "IoT Cloud-Edge Reconfigurable Mixed-Signal Smart Meter Platform for Arc Fault Detection",
  "abstrak": "Smart meter monitors electricity consumption through modern metering devices connected to the Internet via Internet of Things (IoT) technology, which can provide intelligent and fast applications on-site such as arc fault protection based on nonintrusive monitoring load classification with fast safety responses for varying electric loads. Traditional three, four, and five layers of the IoT architecture cannot support such fast responses under loading variation. This article aims to propose six layers of IoT Cloud-Edge Infrastructure equipped with a reconfigurable mixed-signal controller as a smart meter platform for adaptation to different environments and, for example, deal with fast arc fault detection corresponding to different load characteristics. Such an architecture can reduce the processing time for safety, reduce bandwidth for communication, improve arc detection accuracy under different loading environments, and lower the overall computation cost.",
  "keyword": "internet of things"
 },
 {
  "No": 39,
  "judul": "Game Theory in Internet of Things: A Survey",
  "abstrak": "Internet of Things (IoT) devices are being used widely in the fields of smart city, smart grid, environmental monitoring, Internet of Vehicles and other fields that need large-scale sensingdata. However, the research on storage and computation power for IoT is still in its early stages. The game theory converts the interaction between two IoT devices into a game where the conflict is resolved by utilizing the game’s equilibrium conditions. Our goal with the game theory is to maximize the utility for every device in the IoT network. In this article, we review the recent game-theory-based solutions proposed in IoT networks. We summarize game theory concepts and categorize the common game models for ease of understanding for the reader. Later, we focus on analyzing solutions proposed in resource allocation, task scheduling, node selection, quality of service, and network security. Finally, we summarize research challenges and propose future research directions.",
  "keyword": "internet of things"
 },
 {
  "No": 40,
  "judul": "Decentralized Self-Enforcing Trust Management System for Social Internet of Things",
  "abstrak": "The Internet of Things (IoT) is the network of connected computing devices that have the ability to transfer datavalue between each other via the Internet without requiring human intervention. In such a connected environment, the social IoT (SIoT) has become an emerging trend where multiple IoT devices owned by users support communication within a social circle. Trust management in the SIoT network is imperative as trusting the information from compromised devices could lead to serious compromises within the network. It is important to have a mechanism where the devices and their users evaluate the trustworthiness of other devices and users before trusting the information sent by them. The privacy preservation, decentralization, and self-enforcing management without involving trusted third parties are the fundamental challenges in designing a trust management system for SIoT. To fulfill these challenges, this article presents a novel framework for computing and updating the trustworthiness of participants in the SIoT network in a self-enforcing manner without relying on any trusted third party. The privacy of the participants in the SIoT is protected by using homomorphic encryption in the decentralized setting. To achieve the properties of self-enforcement, the trust score of each device is automatically updated based on its previous trust score and the up-to-date tally of the votes by its peers in the network with zero-knowledge proofs (ZKPs) to enforce that every participant follows the protocol honestly. We evaluate the performance of the proposed scheme and present evaluation benchmarks by prototyping the main functionality of the system. The performance results show that the system has a linear increase in computation and communication overheads with more participants in the network. Furthermore, we prove the correctness, privacy, and security of the proposed system under a malicious adversarial model.",
  "keyword": "internet of things"
 },
 {
  "No": 41,
  "judul": "Recent Advances in Information-Centric Networking-Based Internet of Things (ICN-IoT)",
  "abstrak": "Information-centric networking (ICN) is being realized as a promising approach to accomplish the shortcomings of current Internet protocol-address-based networking. ICN models are based on naming the content to get rid of address-space scarcity, accessing the content via name-based-routing, and caching the content at intermediate nodes to provide reliable, efficient datasets delivery, and self-certifying contents to ensure better security. Obvious benefits of ICN in terms of fast and efficient datasets delivery and improved reliability raises ICN as highly promising networking model for Internet of Things (IoT) like environments. IoT aims to connect anyone and\/or anything at any time by any path on any place. From last decade, IoT attracts both industry and research communities. IoT is an emerging research field and still in its infancy. Thus, this paper presents the potential of ICN for IoT by providing state-of-the-art literature survey. We discuss briefly the feasibility of ICN features and their models (and architectures) in the context of IoT. Subsequently, we present a comprehensive survey on ICN-based caching, naming, security, and mobility approaches for IoT with appropriate classification. Furthermore, we present operating systems and simulation tools for ICN-IoT. Finally, we provide important research challenges and issues faced by ICN for IoT.",
  "keyword": "internet of things"
 },
 {
  "No": 42,
  "judul": "Blockchain Technology for Applications in Internet of Things—Mapping From System Design Perspective",
  "abstrak": "Internet of Things (IoT) refers to networks with billions of physical devices for collecting, sharing, and utilizing datain the virtual world. Most of IoT applications centralize security assurance in creating, authenticating, transferring, or delating system components. However, the centralization exposes its limitations to meet security needs of a rapidly growing number of things world-widely. How to scale up the applications with assured security becomes a critical challenge. Blockchain technology (BCT) is a promising solution to provide security and protect privacy in a large scale; especially, smart contracts offer opportunities to improve the reliability of IoT applications. Smart contracts establish trusts for both of dataand executed processes. Recently, many literature surveys and positioning articles have been published on the integration of BCT with IoT, but they are limited to superficial discussions of technical potentials, and very few of them have a thorough exploration of the challenges in developing BCT for IoT at technical levels. This paper uses the system design approach to scrutinize the state of the art of study on BCT-based applications and clarify critical research areas of enabling BCT for security assurance: 1) the relations of BCT and IoT are modeled and discussed; 2) the needs of eliminating threats in IoT-based applications are defined as functional requirements (FRs), existing works on enabling technologies of BCT are defined as the physical solutions (PSs); and 3) the mappings between FRs and PSs are established to identify the limitations and the critical areas for the applications of BCT in large-scale distributed environment.",
  "keyword": "internet of things"
 },
 {
  "No": 43,
  "judul": "Blockchain for Internet of Things: A Survey",
  "abstrak": "Internet of Things (IoT) is reshaping the incumbent industry to smart industry featured with datadriven decision-making. However, intrinsic features of IoT result in a number of challenges, such as decentralization, poor interoperability, privacy, and security vulnerabilities. Blockchain technology brings the opportunities in addressing the challenges of IoT. In this paper, we investigate the integration of blockchain technology with IoT. We name such synthesis of blockchain and IoT as blockchain of things (BCoT). This paper presents an in-depth survey of BCoT and discusses the insights of this new paradigm. In particular, we first briefly introduce IoT and discuss the challenges of IoT. Then, we give an overview of blockchain technology. We next concentrate on introducing the convergence of blockchain and IoT and presenting the proposal of BCoT architecture. We further discuss the issues about using blockchain for fifth generation beyond in IoT as well as industrial applications of BCoT. Finally, we outline the open research directions in this promising area.",
  "keyword": "internet of things"
 },
 {
  "No": 44,
  "judul": "Internet of Things: Device Capabilities, Architectures, Protocols, and Smart Applications in Healthcare Domain",
  "abstrak": "Nowadays, the Internet has spread to practically every country around the world and is having unprecedented effects on people’s lives. The Internet of Things (IoT) is getting more popular and has a high level of interest in both practitioners and academicians in the age of wireless communication due to its diverse applications. The IoT is a technology that enables everyday things to become savvier, everyday computation toward becoming intellectual, and everyday communication to become a little more insightful. In this article, the most common and popular IoT device capabilities, architectures, and protocols are demonstrated in brief to provide a clear overview of the IoT technology to the researchers in this area. The common IoT device capabilities, including hardware (Raspberry Pi, Arduino, and ESP8266) and software (operating systems (OSs), and built-in tools) platforms are described in detail. The widely used architectures that have recently evolved and used are the three-layer architecture, service-oriented architecture, and middleware-based architecture. The popular protocols for IoT are demonstrated which include constrained application protocol, message queue telemetry transport, extensible messaging and presence protocol, advanced message queuing protocol, datadistribution service, low power wireless personal area network, Bluetooth low energy, and ZigBee that are frequently utilized to develop smart IoT applications. Additionally, this research provides an in-depth overview of the potential healthcare applications based on IoT technologies in the context of addressing various healthcare concerns. Finally, this article summarizes state-of-the-art knowledge, highlights open issues and shortcomings, and provides recommendations for further studies which would be quite beneficial to anyone with a desire to work in this field and make breakthroughs to get expertise in this area.",
  "keyword": "internet of things"
 },
 {
  "No": 45,
  "judul": "On the Role of Hash-Based Signatures in Quantum-Safe Internet of Things: Current Solutions and Future Directions",
  "abstrak": "The Internet of Things (IoT) is gaining ground as a pervasive presence around us by enabling miniaturized “things” with computation and communication capabilities to collect, process, analyze, and interpret information. Consequently, trustworthydata act as fuel for applications that rely on thedata generated by these things, for critical decision-making processes, datadebugging, risk assessment, forensic analysis, and performance tuning. Currently, secure and reliabledata communication in IoT is based on public-key cryptosystems such as the elliptic curve cryptosystem (ECC). Nevertheless, the reliance on the security of de-facto cryptographic primitives is at risk of being broken by the impending quantum computers. Therefore, the transition from classical primitives to quantum-safe primitives is indispensable to ensure the overall security ofdata en route. In this article, we investigate applications of one of the postquantum signatures called hash-based signature (HBS) schemes for the security of IoT devices in the quantum era. We give a succinct overview of the evolution of HBS schemes with an emphasis on their construction parameters and associated strengths and weaknesses. Then, we outline the striking features of HBS schemes and their significance for IoT security in the quantum era. We also investigate the optimal selection of HBS in the IoT networks with respect to their performance-constrained requirements, resource-constrained nature, and design optimization objectives. In addition to ongoing standardization efforts, we also highlight current and future research and deployment challenges along with possible solutions. Finally, we outline the essential measures and recommendations that must be adopted by the IoT ecosystem while preparing for the quantum world.",
  "keyword": "internet of things"
 },
 {
  "No": 46,
  "judul": "From Pre-Quantum to Post-Quantum IoT Security: A Survey on Quantum-Resistant Cryptosystems for the Internet of Things",
  "abstrak": "Although quantum computing is still in its nascent age, its evolution threatens the most popular public-key encryption systems. Such systems are essential for today's Internet security due to their ability for solving the key distribution problem and for providing high security in insecure communications channels that allow for accessing websites or for exchanging e-mails, financial transactions, digitally signed documents, military communications or medical datasets. Cryptosystems like Rivest-Shamir-Adleman (RSA), elliptic curve cryptography (ECC) or Diffie-Hellman have spread worldwide and are part of diverse key Internet standards like Transport Layer Security (TLS), which are used both by traditional computers and Internet of Things (IoT) devices. It is especially difficult to provide high security to IoT devices, mainly because many of them rely on batteries and are resource constrained in terms of computational power and memory, which implies that specific energy-efficient and lightweight algorithms need to be designed and implemented for them. These restrictions become relevant challenges when implementing cryptosystems that involve intensive mathematical operations and demand substantial computational resources, which are often required in applications where dataprivacy has to be preserved for the long term, like IoT applications for defense, mission-critical scenarios or smart healthcare. Quantum computing threatens such a long-term IoT device security and researchers are currently developing solutions to mitigate such a threat. This article provides a survey on what can be called post-quantum IoT systems (IoT systems protected from the currently known quantum computing attacks): the main post-quantum cryptosystems and initiatives are reviewed, the most relevant IoT architectures and challenges are analyzed, and the expected future trends are indicated. Thus, this article is aimed at providing a wide view of post-quantum IoT security and give useful guidelines...",
  "keyword": "internet of things"
 },
 {
  "No": 47,
  "judul": "Scheduling Schemes for Age Optimization in IoT Systems With Limited Retransmission Times",
  "abstrak": "Age of Information (AoI) is a recently introduced metric to capture dataset freshness. In this article, we consider a multiuser single-destination Internet of Things (IoT) system with periodic state updating, and investigate the scheduling methods of minimizing the long-term average AoI with limited retransmission times. In view of the retransmission mode, the AoI optimization problems for retransmission with and without feedback are analyzed. For the retransmission without feedback, we formulate the expected decision loss (EDL) function of the AoI optimization problem with the finite retransmission times and propose a loss-greedy policy by minimizing the EDL at each step. For the retransmission with feedback, a potential optimal solution is to construct the AoI optimization problem as an infinite-horizon Markov decision process (MDP) and solve the corresponding Bellman optimal equations. However, this optimal solution is prone to suffer from the curse of dimensionality and is hard to implement. To address this issue, we decouple the infinite-horizon MDP into the finite-state MDP in each single frame, and then propose a low-complexity slot-based max-weight (SBMW) policy to minimize the long-term average AoI. Numerical results show that, compared with an ALOHA-like baseline policy, the proposed Loss-Greedy policy can achieve up to 48% reduction of the average AoI for the retransmission without feedback, while the SBMW policy can reduce the average AoI by 57% in the retransmission with feedback. The average performance gain of the proposed policies over the state-of-the-art policies is at least 10%.",
  "keyword": "internet of things"
 },
 {
  "No": 48,
  "judul": "Characterizing Heterogeneous Internet of Things Devices at Internet Scale Using Semantic Extraction",
  "abstrak": "Along with the rapid-growth number of Internet of Things (IoT) devices, significant security concerns are raised due to the hidden vulnerabilities among them. Illuminating the characteristics of online devices would shed a light on protecting these potential vulnerable devices. State-of-arts methodologies enumerate devices characteristics as keywords and rules and match them with IoT networkdata. However, the heterogeneous implementations of IoT devices introduce intricate characteristics features, which impede the large-scale identification. In this work, we close this gap and present a semantic extraction-based approach that can automatically and effectively characterize online devices. We leverage the observation that IoT devices can be identified by analyzing the semantic information of the network packets. Specifically, we first collect the networkdata of IoT devices and utilize a co-training algorithm to annotate thedata. We propose a residual dilate gated convolutional neural network (RDGCNN)-based encoder to extract semantic features from the annotateddata. Then, we put forward an entity relationship-based decoder to generate the characteristic triplet (type, brand, and model) of IoT devices by decoding extracted features. We have implemented the prototype of the system and conducted real-world experiments to evaluate the performance. Results show that our approach achieves 92.16% precision and 86.79% recall. In addition, we apply our proposed method to characterize 15 millions IoT devices on the Internet.",
  "keyword": "internet of things"
 },
 {
  "No": 49,
  "judul": "A Comprehensive Review on Secure Routing in Internet of Things: Mitigation Methods and Trust-Based Approaches",
  "abstrak": "Internet of Things (IoT) is a network of “things,” connected via Internet, to collect and exchange dataset. These “things” can be sensors, actuators, smartphones, wearables, computers, or any object that is interconnected to provide specific services. Similarly, wireless sensor network (WSN), as a part of IoT, forwards the gathered dataset after sensing any event. The scalability and heterogeneity of IoT offer limited protection and is prone to diverse attacks, including WSN-inherited attacks. Moreover, IPv6 routing protocol for low power and lossy networks (RPL), a de facto routing protocol for IoT networks, also suffers from certain vulnerabilities based on its features and functionalities. Researchers have proposed various mitigation mechanisms for secure networks and routing in IoT. Recently, trust-based approaches have gained tremendous interest from the research community to embed security in IoT networks and routing protocols. In the existing literature, several trust models have been introduced according to the security needs of the IoT system, such as SecTrust, DCTM-IoT, CTRUST, etc. In this research, security issues and requirements of IoT networks and RPL routing protocol are studied with respect to various attacks, such as Blackhole, Spoofing, Rank, etc. Additionally, various mitigation methods and significance of trust models in IoT for secure routing are analyzed. Further, trust metrics in IoT environments, including the open issues and research challenges, as well as the implication of trust as a security paradigm in IoT networks and routing protocols are discussed.",
  "keyword": "internet of things"
 },
 {
  "No": 50,
  "judul": "6LoWPAN Over Optical Wireless Communications for IPv6 Transport in Internet of Things Networks",
  "abstrak": "As an emerging technology for the Internet of Things (IoT) wireless connectivity, there have been a lot of research and standardization activities on Visible Light Communications (VLC) and Optical Wireless Communications (OWC) using Light Emitting Diode (LED) lights. In the meantime, the Internet Protocol version 6 (IPv6) over Low Power Wireless Personal Area Network (6LoWPAN) has been discussed to provide the IPv6-based IoT services in wireless networks. However, the study on IoT systems using 6LoWPAN over OWC networks has not been made so far. This letter proposes a new architectural model to effectively use 6LoWPAN between IoT gateway and IoT device in the OWC-based IoT networks. The proposed model is easy to implement and provides the performance enhancement in OWC-based IoT networks, compared to the general IPv6 model. From testbed experimentations, it is shown that the proposed model provides the delay gain up to 5% and the throughput gain up to 19.52%, compared to the conventional IPv6 transport model.",
  "keyword": "internet of things"
 },
 {
  "No": 51,
  "judul": "NSAC: A Novel Clustering Protocol in Cognitive Radio Sensor Networks for Internet of Things",
  "abstrak": "Clustering is an effective method to manage communications in cognitive radio sensor networks (CRSNs). This letter proposes a network stability-aware clustering (NSAC) protocol for CRSNs. Spectrum dynamics and energy consumption are for the first time simultaneously integrated into the protocol design of NSAC. Extensive simulations show that the proposed NSAC protocol obviously outperforms existing methods in the aspects of network stability and energy consumption.",
  "keyword": "internet of things"
 },
 {
  "No": 52,
  "judul": "Lifetime Maximization of an Internet of Things (IoT) Network Based on Graph Signal Processing",
  "abstrak": "The lifetime of an Internet of Things (IoT) system consisting of battery-powered devices can be increased by minimizing the number of transmissions per device while not excessively deteriorating the correctness of the overall IoT monitoring. We propose a graph signal processing based algorithm for partitioning the sensor nodes into disjoint sampling sets. The sets can be sampled on a round-robin basis and each one contains enough information to reconstruct the entire signal within an acceptable error bound. Simulations on different models of graphs, based on graph theory and on real-world applications, show that our proposal consistently outperforms state-of-the-art sampling schemes, with no additional computational burden.",
  "keyword": "internet of things"
 },
 {
  "No": 53,
  "judul": "An Energy-Efficient Dual-Field Elliptic Curve Cryptography Processor for Internet of Things Applications",
  "abstrak": "This brief presents an energy-efficient elliptic curve cryptography (ECC) processor for Internet of Things (IoT) security applications. The proposed processor supports dual-field computations, and employs various design techniques across the algorithm, architecture, and arithmetic circuit levels to minimize power and energy consumption. The proposed elliptic curve point multiplication (ECPM) algorithm employs signed binary representation (SBR) with the m-ary method to reduce both area and energy consumption, while avoiding attack from simple power analysis (SPA). In addition, the proposed hybrid modular arithmetic architecture effectively increases the hardware utilization to reduce both area and energy cost. Finally, the proposed processor uses an energy-efficient dataflow to further minimize memory overhead for group operations. The proposed ECC processor achieves 51.6% and 50.5% lower energy consumption for each GF(p) and GF(2\nm\n) ECPM operation, respectively, when compared to state-of-the-art ECC designs.",
  "keyword": "internet of things"
 },
 {
  "No": 54,
  "judul": "Fairness-Aware Federated Learning With Unreliable Links in Resource-Constrained Internet of Things",
  "abstrak": "In order to make full use of the network dataset and guarantee user privacy simultaneously, federated learning (FL) is proposed to enable distributed intelligence for local nodes without sharing dataset with each other. However, in practice, due to resource limitations, traditional FL suffers from node scheduling and parameter transmission failure, which not only affects the final performance but also further reduces the fairness of the participating nodes. This article addresses the challenge and proposes an FL method to enhance the performance of FL on the basis of guaranteeing the fairness of the local nodes in a resource-constrained Internet of Things (IoT) network. Specifically, an analytical model is first constructed to characterize the performance of FL with joint considerations of node fairness, unreliable parameter transmissions as well as resource limitations. Thereafter, a statistically reweighted aggregation (SRA) scheme is proposed for parameter aggregation and the corresponding model is proved to be unbiased to that based on ideal parameter transmissions. With the knowledge of time dependency of the global model, we further extend SRA and propose a reliable SRA (RSRA) scheme. Additionally, we prove RSRA is able to achieve higher stability performance than SRA in model training. Furthermore, the convergence bound of the proposed RSRA is derived analytically, based on which an adaptive local training scheme is proposed under a given resource budget. Finally, extensive experiments are carried out with a public dataset to validate the effectiveness of the proposed scheme with comparisons of other baseline schemes.",
  "keyword": "internet of things"
 },
 {
  "No": 55,
  "judul": "Design and Operation of a Lightweight Educational Testbed for Internet-of-Things Applications",
  "abstrak": "In this article, we present the design and operation process of the AssIuT-IoT testbed; an educational and remotely accessible testbed for Internet-of-Things (IoT) applications. The testbed adopts the experiment as a service (EaaS) model in which users are able to access the testbeds and reserve\/use the available resources remotely over the Internet. Also, the testbed is federated with other ones distributed among different locations to allow the sharing of resources, user accounts, and policies. The federated testbeds form an educational consortium that facilitates students' access to hardware resources available at different locations. AssIuT-IoT testbed consists of a control server and a set of IoT nodes (hardware resources) supported with different wireless communication capabilities. We present the design, operation, and performance details of the testbed in terms of the hardware and software components. We describe the steps needed to complete experiments using our testbed and we provide some examples. Moreover, we evaluate the testbed and the IoT network's performance. Finally, we present the IoT students' competition as one of the activities where the testbed has been used.",
  "keyword": "internet of things"
 },
 {
  "No": 56,
  "judul": "Privacy in Internet of Things: From Principles to Technologies",
  "abstrak": "Ubiquitous deployment of low-cost smart devices and widespread use of high-speed wireless networks have led to the rapid development of the Internet of Things (IoT). IoT embraces countless physical objects that have not been involved in the traditional Internet and enables their interaction and cooperation to provide a wide range of IoT applications. Many services in the IoT may require a comprehensive understanding and analysis of datacollected through a large number of physical devices that challenges both personal information privacy and the development of IoT. Information privacy in IoT is a broad and complex concept as its understanding and perception differ among individuals and its enforcement requires efforts from both legislation as well as technologies. In this paper, we review the state-of-the-art principles of privacy laws, the architectures for IoT and the representative privacy enhancing technologies (PETs). We analyze how legal principles can be supported through a careful implementation of PETs at various layers of a layered IoT architecture model to meet the privacy requirements of the individuals interacting with IoT systems. We demonstrate how privacy legislation maps to privacy principles which in turn drives the design of necessary PETs to be employed in the IoT architecture stack.",
  "keyword": "internet of things"
 },
 {
  "No": 57,
  "judul": "Internet of Things (IoT): A Review of Its Enabling Technologies in Healthcare Applications, Standards Protocols, Security, and Market Opportunities",
  "abstrak": "The Internet of Things (IoT) is a methodology or a system that encompasses real-world things to interact and communicate with each other with the assistance of networking technologies. This article describes surveys on advances in IoT-based healthcare methods and reviews the state-of-the-art technologies in detail. Moreover, this review classifies an existing IoT-based healthcare network and represents a summary of all perspective networks. IoT healthcare protocols are analyzed in this context and provide a broad discussion on it. It also initiates a comprehensive survey on IoT healthcare applications and services. Extensive insights into IoT healthcare security, its requirements, challenges, and privacy issues are visualized in IoT surrounding healthcare. In this review, we analyze security and privacy features consisting of dataprotection, network architecture, Quality of Services (QoS), app development, and continuous monitoring of healthcare that are facing difficulties in many IoT-based healthcare architectures. To mitigate the security problems, an IoT-based security architectural model has been proposed in this review. Furthermore, this review discloses the market opportunity that will enhance the IoT healthcare market development. To conduct the survey, we searched through established journal and conference databases using specific keywords to find scholarly works. We applied a filtering mechanism to collect only papers that were relevant to our research works. The selected papers were then examined carefully to understand their contributions\/research focus. Eventually, the paper reviews were analyzed to identify any existing research gaps and untouched areas of research and to discover possible features for sustainable IoT healthcare development.",
  "keyword": "internet of things"
 },
 {
  "No": 58,
  "judul": "TinyIKE: Lightweight IKEv2 for Internet of Things",
  "abstrak": "There is unanimous consensus that cyber security in the Internet of Things (IoT) is necessary. In cyber security, key establishment is one of the toughest problems. It is even more challenging in resource-constrained but Internet-connected IoT devices that use low-power wireless communication. A number of IoT communication protocols define cryptographic mechanisms for confidentiality and integrity services but do not specify key management. For example, IEEE 802.15.4, RPL, and object security all rely on external key management protocols. Due to the lack of automatic key management support, IoT devices either end up using preshared keys or no security at all. In this paper, we overcome these challenges and present TinyIKE, a lightweight adaptation of Internet Key Exchange version 2 (IKEv2) for the IoT. Using TinyIKE, we solve the key establishment problem for multiple IoT protocols using a single IKEv2-based solution. We implement TinyIKE for resource-constrained IoT devices that run the Contiki OS. The TinyIKE implementation supports full certificate-based IKEv2 that uses elliptic curve cryptography. In order to ensure the feasibility of TinyIKE in the IoT, we perform an extensive evaluation of TinyIKE using a setup consisting of real IoT hardware.",
  "keyword": "internet of things"
 },
 {
  "No": 59,
  "judul": "Energy-Efficient Space–Air–Ground Integrated Edge Computing for Internet of Remote Things: A Federated DRL Approach",
  "abstrak": "Space–air–ground integrated edge computing is expecting to provide pervasive computation services for Internet of Things (IoT), especially in remote areas. However, the offloading process of power-limited IoT devices is a challenge issue due to unreliable communications in an aerial environment. In this article, we propose an energy-efficient space–air–ground integrated edge computing network architecture, in which the IoT devices choose the most appropriate LEO satellites or unmanned aerial vehicles (UAVs) for task offloading according to their energy level, communication conditions and computing capabilities. In order to providing efficient task offloading and energy-saving policy under an uncertainty aerial environment, a constrained Markov decision process is employed to formulate the task offloading decision problem and a deep reinforcement learning (DRL)-based algorithm is devised to solve the proposed problem. An adaptive federated DRL-based offloading method is further proposed to find suboptimal offloading decisions by considering the privacy protection and communication failure in the proposed network. Numerical results confirm the effectiveness of the proposed schemes on energy saving and computation efficiency.",
  "keyword": "internet of things"
 },
 {
  "No": 60,
  "judul": "LightTrust: Lightweight Trust Management for Edge Devices in Industrial Internet of Things",
  "abstrak": "The phenomenal increase in the usage of Internet promotes the quality of trust in the scope of the Internet of Things (IoT). Trust is beneficial in the provision of an effective, reliable, scalable, and trustworthy environment to users of the IoT network, where they can share their private information with each other on a secure communication platform. For successful communications among the Internet users, trust is an important factor to provide them with private infrastructures and secure environments, where exchanging dataamong devices becomes more easy and trustworthy. Therefore, trust management is a backbone for the successful and secure transmission of dataamong various nodes in a large-scale IoT network. To overcome the security issues, latency, and risk of malicious activities, a lightweight approach is proposed for those nodes in Industrial IoT that cannot maintain security. LightTrust utilizes a centralized trust agent to generate and manage trust certificates that allow nodes to communicate for a specific time without performing trust computations. Trust agents also maintain a trust database to store the current trust degree for the aggregation\/propagation purposes. Trust between two nodes is developed by direct observations in terms of compatibility, cooperativeness, and delivery ratio, whereas recommendations are used to develop trust in the context of indirect observations, i.e., experience or previous knowledge. The comparative simulations of the proposed and existing approaches are also performed whereby the results illustrate that the proposed approach efficiently maintains resilience and robust environments.",
  "keyword": "internet of things"
 },
 {
  "No": 61,
  "judul": "A Provably Secure and Lightweight Anonymous User Authenticated Session Key Exchange Scheme for Internet of Things Deployment",
  "abstrak": "With the ever increasing adoption rate of Internet-enabled devices [also known as Internet of Things (IoT) devices] in applications such as smart home, smart city, smart grid, and healthcare applications, we need to ensure the security and privacyof data and communications among these IoT devices and the underlying infrastructure. For example, an adversary can easily tamper with the information transmitted over a public channel, in the sense of modification, deletion, and fabrication ofdata-in-transit anddata-in-storage. Time-critical IoT applications such as healthcare may demand the capability to support external parties (users) to securely access IoT dataand services in real-time. This necessitates the design of a secure user authentication mechanism, which should also allow the user to achieve security and functionality features such as anonymity and un-traceability. In this paper, we propose a new lightweight anonymous user authenticated session key agreement scheme in the IoT environment. The proposed scheme uses three-factor authentication, namely a user's smart card, password, and personal biometric information. The proposed scheme does not require the storing of user specific information at the gateway node. We then demonstrate the proposed scheme's security using the broadly accepted real-or-random (ROR) model, Burrows-Abadi-Needham (BAN) logic, and automated validation of Internet security protocols and applications (AVISPAs) software simulation tool, as well as presenting an informal security analysis to demonstrate its other features. In addition, through our simulations, we demonstrate that the proposed scheme outperforms existing related user authentication schemes, in terms of its security and functionality features, and computation costs.",
  "keyword": "internet of things"
 },
 {
  "No": 62,
  "judul": "A Reliable Energy Efficient Dynamic Spectrum Sensing for Cognitive Radio IoT Networks",
  "abstrak": "The Internet of Things (IoT) that allows connectivity of network devices embedded with sensors undergoes severe dataexchange interference as the unlicensed spectrum band becomes overcrowded. By applying cognitive radio (CR) capabilities to IoT, a novel cognitive radio IoT (CR-IoT) network arises as a promising solution to tackle the spectrum scarcity problem in conventional IoT network. CR is a form of wireless communication whereby a radio is dynamically programmed and configured to detect available spectrum channels. This enhances the spectrum utilization efficiency of radio frequency while avoiding interference and overcrowding to other users. Energy efficiency in CR-IoT network must be carefully formulated since the sensor nodes consume significant energy to support CR operations, such as in dynamic spectrum sensing and switching. In this paper, we study channel spectrum sensing to boost energy efficiency in clustered CR-IoT networks. We propose a two-way information exchange dynamic spectrum sensing algorithms to improve energy efficiency for datatransmission in licensed channels. In addition, the concern of the energy consumption in dynamic spectrum sensing and switching, we propose an energy efficient optimal transmit power allocation technique to enhance the dynamic spectrum sensing and datathroughput. Simulation results validate that the proposed dynamic spectrum sensing technique can significantly reduce the energy consumption in CR-IoT networks.",
  "keyword": "internet of things"
 },
 {
  "No": 63,
  "judul": "Hierarchical Naming Scheme in Named Networking for Internet of Things: A Review and Future Security Challenges",
  "abstrak": "The proliferation of connected devices in the Internet of Things (IoT) presents a connectivity challenge. The future internet will require a paradigm shift in which content is evaluated on the basis of “What” it is rather than “Where” it originated. ICN’s goal is to provide the benefits of name-based content addressing in order to facilitate scalable content distribution, security, mobility, and trust. NDN is a new internet architecture that evolved from Content-Centric Networking (CCN). NDN is viewed as a solution to the IoT’s challenges, as well as a way to transcend the IP paradigm. With IoT systems that had a number of challenging characteristics to satisfy, including heterogeneous devices, resource constraints, and energy efficiency. Due to the fact that NDN native features deliver via hierarchically structured names, it offer promising solutions for current research integrating NDN into IoT. The review discusses the significance of naming, its influence, and security factor. Additionally, research challenges in the areas of naming and security will be discussed. The primary objective of this review is to give a new facelift to a new integrating naming convention for NDN.",
  "keyword": "internet of things"
 },
 {
  "No": 64,
  "judul": "The Performance Evaluation of Blockchain-Based Security and Privacy Systems for the Internet of Things: A Tutorial",
  "abstrak": "This article presents research challenges and a tutorial on performance evaluation of blockchain-based security and privacy systems for the Internet of Things (IoT). We start by summarizing the existing surveys that deal with blockchain security for IoT networks. Then, we review the blockchain-based security and privacy systems for seventeen types of IoT applications, e.g., Industry 4.0, software-defined networking, edge computing, Internet of Drones, Internet of Cloud, Internet of Energy, Internet of Vehicles, etc. We also review various consensus algorithms and provide a comparison with respect to the nine properties, such as latency, throughput, computation, storage, and communication costs, scalability, attack model, advantage, disadvantage, etc. Moreover, we present the security analysis techniques and provide a classification into four categories, including Burrows, Abadi, and Needham (BAN) logic, game theory, theory analysis, and AVISPA tool. In addition, we analyze the performance metrics, blockchain testbeds, and cryptography libraries used in the performance evaluation of blockchain-based security and privacy systems for the IoT networks. Based on the current survey, we discuss the major steps to follow for building and evaluating blockchain-based security and privacy systems. Finally, we discuss and highlight open challenges and future research opportunities.",
  "keyword": "internet of things"
 },
 {
  "No": 65,
  "judul": "A Survey on Digital Forensics in Internet of Things",
  "abstrak": "Internet of Things (IoT) is increasingly permeating peoples' lives, gradually revolutionizing our way of life. Due to the tight connection between people and IoT, now civil and criminal investigations or internal probes must take IoT into account. From the forensic perspective, the IoT environment contains a rich set of artifacts that could benefit investigations, while the forensic investigation in IoT paradigm may have to alter to accommodate characteristics of IoT. Therefore, in this article, we analyze the impact of IoT on digital forensics and systematize the research efforts made by previous researchers from 2010 to 2018. We sketch the landscape of IoT forensics and examine the state of IoT forensics under a 3-D framework. The 3-D framework consists of a temporal dimension, a spatial dimension, and a technical dimension. The temporal dimension walks through the standard digital forensic process while the spatial dimension explores where to identify sources of evidence in IoT environment. These two dimensions attempt to provide principles and guidelines for standardizing digital investigations in the context of IoT. The technical dimension guides a way to the exploration of tools and techniques to ensure the enforcement of digital forensics in the ever-evolving IoT environment. Put together, we present a holistic overview of digital forensics in IoT. We also highlight open issues and outline promising suggestions to inspire future study.",
  "keyword": "internet of things"
 },
 {
  "No": 66,
  "judul": "MAC Protocols for IEEE 802.11ah-Based Internet of Things: A Survey",
  "abstrak": "The IEEE 802.11ah, also known as WiFi HaLow, is a scalable solution for medium-range communication in Internet of Things (IoT). While provisioning support for the IoT and machine-to-machine (M2M) communication, IEEE 802.11ah leverages various innovative medium access control (MAC) layer concepts, such as restricted access window (RAW), hierarchical association identification (AID), traffic indication map (TIM) segmentation, etc. This article presents a survey on various MAC protocols for IEEE 802.11ah. While discussing the essential features of IEEE 802.11ah, this survey points out various issues and limitations of such MAC protocols. Although there are some surveys available for MAC protocols of IEEE 802.11ah, they do not include a large number of schemes that have been recently proposed to solve different standardization and implementation-based issues. This article individually surveys issues and challenges in the different problem domains of the IEEE 802.11ah MAC protocol and analyzes the recently proposed solutions. Moreover, this article identifies various factors for further improvement of these protocols. Compared to other relevant surveys, this article emphasizes the issues and challenges to enable researchers to easily identify the problem domain.",
  "keyword": "internet of things"
 },
 {
  "No": 67,
  "judul": "Semantic Reasoning for Context-Aware Internet of Things Applications",
  "abstrak": "Acquiring knowledge from continuous and heterogeneousdata streams is a prerequisite for Internet of Things (IoT) applications. Semantic technologies provide comprehensive tools and applicable methods for representing, integrating, and acquiring knowledge. However, resource-constraints, dynamics, mobility, scalability, and real-time requirements introduce challenges for applying these methods in IoT environments. We study how to utilize semantic IoTdata for reasoning of actionable knowledge by applying state-of-the-art semantic technologies. For performing these studies, we have developed a semantic reasoning system operating in a realistic IoT environment. We evaluate the scalability of different reasoning approaches, including a single reasoner, distributed reasoners, mobile reasoners, and a hybrid of them. We evaluate latencies of reasoning introduced by different semantic dataformats. We verify the capabilities of promising semantic technologies for IoT applications through comparing the scalability and real-time response of different reasoning approaches with various semanticdata formats. Moreover, we evaluate differentdata aggregation strategies for integrating distributed IoTdata for reasoning processes.",
  "keyword": "internet of things"
 },
 {
  "No": 68,
  "judul": "Joint Dynamic Task Offloading and Resource Scheduling for WPT Enabled Space-Air-Ground Power Internet of Things",
  "abstrak": "The Space-Air-Ground Power Internet of Things (SAG-PIoT) can meet the communication needs of Power Internet of Things (PIoT) devices in localities with insufficient ground base station coverage. Considering that the battery capacity of PIoT devices is limited and difficult to be replaced, we use unmanned aerial vehicles (UAVs) to assist wireless power transmission (WPT) in the SAG-PIoT to ensure the network service quality and safe and stable operation of PIoT. In this paper, we put forward a novel WPT enabled SAG-PIoT architecture, based on which we investigate the dynamic task offloading and resource scheduling, and propose a joint online optimization algorithm for task assignment, local computing resource allocation, association control and UAV computing resource allocation to minimize the long-term time-averaged network operation cost. Since the coupling of task offloading and resource scheduling in the long-term stochastic optimization problem, it is decoupled into three subproblems to be solved separately using Lyapunov optimization. Moreover, we analyze the stability and sustainability of the proposed algorithm and reveal the tradeoff between the network stability and optimal network operation cost. Simulations show that the proposed algorithm has advantageous performances with respect of network stability, energy consumption and network operation cost compared to other benchmark algorithms.",
  "keyword": "internet of things"
 },
 {
  "No": 69,
  "judul": "Energy-Efficient Resource Allocation and Data Transmission of Cell-Free Internet of Things",
  "abstrak": "The Internet of Things (IoT) is a huge network consisting of various information-sensing devices combined with the Internet. The IoT aims to connect all things with the network to facilitate identification and management. The increasing number of things, connections, and volume of data is challenging the reliability and sustainability of the IoT. To push forward the development of the IoT, a cell-free IoT system is proposed based on a cell-free wireless communication network. Under the architecture of cell-free IoT, the cell concept for wireless communication in the IoT system has been diluted, and the reliability and robustness of the IoT system can be improved when compared with the cellular IoT system. Moreover, this article considers resource allocation and data transmission of cell-free IoT. An optimization model is designed for resource allocation to ensure quality data transmission. Furthermore, a novel heuristic algorithm is proposed for solving the optimization model. The heuristic algorithm is memetic of two machine learning methods. Extensive simulations are conducted to study the performance of the presented cell-free IoT. Energy efficiency is improved by the proposed algorithm compared with existing algorithms. The results show that cell-free IoT can achieve better reliability and sustainability.",
  "keyword": "internet of things"
 },
 {
  "No": 70,
  "judul": "Hybrid Channel Estimation for UPA-Assisted Millimeter-Wave Massive MIMO IoT Systems",
  "abstrak": "In this article, we present a hybrid channel estimation algorithm for uniform planar array (UPA)-assisted millimeter-wave (mmWave) massive multiple-input–multiple-output (MIMO) Internet of Things (IoT) systems by exploiting the benefits from both the compressed sensing (CS) and the sparse Bayesian learning (SBL). Compared with existing studies, the distribution characteristics and correlations between propagation paths in the elevation (e)- and azimuth (a)-angle domains are considered to enhance the estimation performance. Specifically, we first redefine the e-angles and the a-angles to simplify the system model. Then, a novel autoregressive (AR)-Gaussian channel prior is proposed to capture both the sparsity and the clustering properties of mmWave massive MIMO IoT channels. After that, we provide a channel approximation method to overcome the channel uncertainty by exploiting the structure of the AR-Gaussian channel prior. The hybrid beamforming (HBF) architecture with limited radio-frequency (RF) chains in mmWave IoT systems is also considered. Finally, we propose a hybrid channel estimation algorithm, which consists of two stages. Based on the different distribution characteristics in different angle domains, the CS-based channel estimation is performed for e-angles on stage one, while the SBL-based channel estimation is applied for a-angles on stage two. Numerical results reveal that compared with the existing CS- and SBL-only methods, the proposed hybrid channel estimation algorithm exhibits better performance in terms of computational complexity, sparsity robustness, and estimation accuracy.",
  "keyword": "internet of things"
 },
 {
  "No": 71,
  "judul": "Trust Management in Social Internet of Things: Architectures, Recent Advancements, and Future Challenges",
  "abstrak": "Social Internet of Things (SIoT) is an extension of the Internet of Things (IoT) that converges with social networking concepts to create social networks of interconnected smart objects. This convergence allows the enrichment of the two paradigms, resulting into new ecosystems. While IoT follows two interaction paradigms, human to human (H2H) and thing to thing (T2T), SIoT adds on human-to-thing (H2T) interactions. SIoT enables smart “social objects” that intelligently mimic the social behavior of human in the daily life. These social objects (SOs) are equipped with social functionalities capable of discovering other SOs in the surroundings and establishing social relationships. They crawl through the social network of objects for the sake of searching for services and information of interest. The notion of trust and trustworthiness in social communities formed in SIoT is still new and in an early stage of investigation. In this article, our contributions are threefold. First, we present the fundamentals of SIoT and trust concepts in SIoT, clarifying the similarities and differences between IoT and SIoT. Second, we categorize the trust management solutions proposed so far in the literature for SIoT over the last six years and provide a comprehensive review. We then perform a comparison of the state-of-the-art trust management schemes devised for SIoT by performing comparative analysis in terms of trust management process. Third, we identify and discuss the challenges and requirements in the emerging new wave of SIoT, and also highlight the challenges in developing trust and evaluating trustworthiness among the interacting SOs.",
  "keyword": "internet of things"
 },
 {
  "No": 72,
  "judul": "A Top-Down Survey on Optical Wireless Communications for the Internet of Things",
  "abstrak": "The Internet of Things (IoT) is a transformative technology marking the beginning of a new era where physical, biological, and digital worlds are integrated by connecting a plethora of uniquely identifiable smart objects. Although the Internet of Terrestrial Things (IoTT) has been at the center of our IoT perception, it has been recently extended to different environments, such as the Internet of Underwater Things (IoWT), the Internet of Biomedical Things (IoBT), and Internet of Underground Things (IoGT). Even though radio frequency (RF) based wireless networks are regarded as the default means of connectivity, they are not always the best option due to the limited spectrum, interference limitations caused by the ever-increasing number of devices, and severe propagation loss in transmission mediums other than air. As a remedy, optical wireless communication (OWC) technologies can complement, replace, or co-exist with audio and radio wave-based wireless systems to improve overall network performance. To this aim, this paper reveals the full potential of OWC-based IoT networks by providing a top-down survey of four main IoT domains: IoTT, IoWT, IoBT, and IoGT. Each domain is covered by a dedicated and self-contained section that starts with a comparative analysis, explains how OWC can be hybridized with existing wireless technologies, points out potential OWC applications fitting best the related IoT domain, and discusses open communication and networking research problems. More importantly, instead of presenting a visionary OWC-IoT framework, the survey discloses that OWC-IoT has become a reality by emphasizing ongoing proof-of-concept prototyping efforts and available commercial off-the-shelf (COTS) OWC-IoT products.",
  "keyword": "internet of things"
 },
 {
  "No": 73,
  "judul": "Comment on “Efficient and Secure Outsourcing Scheme for RSA Decryption in Internet of Things”",
  "abstrak": "Internet-of-Things (IoT) devices have grown in popularity over the past few years. The RSA public-key cryptographic primitive is time consuming for resource-constrained IoT. Recently, Zhang et al. proposed a two-party outsourcing protocol between a client and a server for RSA decryption in IoT. It relies on the Chinese remainder theorem as proposed by Quisquater and Couvreur in 1982 and is very efficient. We show that their protocol does not achieve the claimed security guarantees: 1) the (secret) decryption exponent, the plaintext, and the factorization of the RSA modulus are revealed to a passive adversary and 2) a malicious server can make the client accept an (invalid) value of its choice as the result of the delegated computation.",
  "keyword": "internet of things"
 },
 {
  "No": 74,
  "judul": "Anomaly Detection Based on Multidimensional Data Processing for Protecting Vital Devices in 6G-Enabled Massive IIoT",
  "abstrak": "As a result of the increasing deployment of Industrial-Internet-of-Things (IIoT) architectures, large volumes of multidimensional data are continuously generated. An important issue with these data is that higher dimensionality increases the degree of fragmentation. Furthermore, data sets collected by IIoT nodes often display outliers, which are usually caused by anomalous events or errors. These outliers contain considerable valuable information, which prevent the normal operation of the system. Thus, methodologies are able to quantify the obtained information to protect the high priority IIoT nodes, are crucial. This study aims at developing such a method driven by sixth-generation (6G) networks. The proposed algorithm uses a multidimensional data relationship diagram to characterize the spatiotemporal correlations among heterogeneous data. Then, an autoregressive exogenous model is used to eliminate the effects of noise on sensor data, and to help in detecting anomalies. Finally, the algorithm produces a Cumulative Coefficient of Value (CCoV), to identify high-value sensing devices and enable massive Internet of Things (IoT) with 6G-using the characteristic patterns hidden within the data. The experimental results demonstrate that the proposed method can effectively handle the effects of the ubiquitous interference noise in complex industrial environments. Moreover, the method yields effective anomaly detection and compensates for some of the shortcomings in traditional methods.",
  "keyword": "internet of things"
 },
 {
  "No": 75,
  "judul": "SDN-Based Dataset Transfer Security for Internet of Things",
  "abstrak": "The exponential growth of devices connected to the network has resulted in the development of new Internet of Things (IoT) applications and online services, which may have diverse and dynamic requirements on received quality. Although, the emerging software-defined networking (SDN) approach can be leveraged for the IoT environment, to dynamically achieve differentiated quality levels for different IoT tasks in very heterogeneous wireless networking scenarios, the open interfaces in SDN introduces new network attacks, which may make SDN-based IoT malfunctioned. The challenges lies in securely using SDN for IoT systems. To address this challenge, we design a SDN-based dataset transfer security model middlebox-guard (M-G). M-G aims at reducing network latency, and properly manage dataflow to ensure the network run safely. First, according to different security policies, middleboxes related to the defined secure policies, are placed at the most appropriate locations, using dataflow abstraction and a heuristic algorithm. Next, to avoid any middlebox becoming a hotspot, an offline integer linear program (ILP) pruning algorithm is proposed in M-G, to tackle switch volume constraints. In addition, an online linear program (LP) formulation is come up to handle load balance. Finally, secure mechanisms are proposed to handle different attacks. And network routing is solved flexibly, through dataflow management protocol, which are formulated via combining tunnels and tags. Experimental results demonstrate that this model can improve security performance and manage dataflow effectively in SDN-based IoT system.",
  "keyword": "internet of things"
 },
 {
  "No": 76,
  "judul": "Application-Oriented Traffic Modeling of WiFi-Based Internet of Things Gateways",
  "abstrak": "Many Internet of Things (IoT) devices generate relatively small-sized dataand have limited energy supply. These two factors limit their ability to connect directly to cloud servers through a wireless backbone network without imposing a burden on this network in providing efficient datatransfer. In this article, we consider an IoT network architecture where a number of different IoT devices send their datawirelessly to an IoT gateway (or a fog node) via a WiFi network. We focus on characterizing incoming traffic patterns to the gateway for three typical IoT applications with real-time and nonreal-time datatransfer requirements, such as video surveillance, smart city, and e-healthcare. Our study is based on generating real IoT traffic traces in a lab environment from various sensors and devices for the aforementioned applications and employing these traces to emulate a network of IoT nodes connected to a gateway via WiFi. In the conducted experiments, different homogenous and nonhomogeneous traffic patterns of the selected applications are examined for synchronized and unsynchronized datasources. Based on our empiricaldata, the experimental results reveal that the packet interarrival time distribution at the gateway is close to generalized Pareto distribution for homogeneous eHealth and smart city traffic, whereas the Weibull distribution is the nearest to model the empirical packet interarrival time for the rest of the examined traffic patterns. Moreover, we show that employing the experimental findings to analyze the delay performance of connecting the gateway to the cloud, given certain backbone network resources, leads to accurate results.",
  "keyword": "internet of things"
 },
 {
  "No": 77,
  "judul": "MIMO Design for Internet of Things: Joint Optimization of Spectral Efficiency and Error Probability in Finite Blocklength Regime",
  "abstrak": "In this article, we consider a multiple-input–multiple-output (MIMO) system serving Internet of Things (IoT) devices. To satisfy stringent requirements on the latency of IoT communications, the IoT devices communicate in the finite blocklength regime, wherein the achievable spectral efficiency (SE) has a backoff factor and decoding error probability is nonnegligible. Aiming to jointly optimize the sum SE and the maximum error probability, we first express the achievable SE as a function of the channel coefficients, precoders, and error probabilities. Subsequently, we formulate a problem with respect to precoders and error probabilities. A straightforward approach for the formulated problem, however, is challenging as it not only has multiple objectives but also is nonconvex. To resolve these issues, we first transform the problem as a single-objective optimization by using a weighted sum approach. Based on the reformulation, we propose an algorithm, in which error probabilities and precoders are determined by alternating two phases. Via simulations, we demonstrate that the proposed method offers significant gains compared to baseline methods, in terms of the achievable SE and the maximum error probability. In particular, we show that the communication latency is greatly reduced by using the proposed method.",
  "keyword": "internet of things"
 },
 {
  "No": 78,
  "judul": "User Selection for NOMA-Based MIMO With Physical-Layer Network Coding in Internet of Things Applications",
  "abstrak": "Nonorthogonal multiple access (NOMA)-based multiple-input-multiple-output (MIMO), which has the potential to provide both massive connectivity and high spectrum efficiency, is considered as one of the efficient techniques for sixth-generation (6G) wireless systems. In massive Internet of Things (IoT) networks, the user-set selection is crucial for enhancing the overall performance of NOMA-based systems when compared with orthogonal multiple access (OMA) techniques. In this article, we propose a user-set selection algorithm for IoT uplink transmission to improve the sum datarate of the NOMA-based MIMO systems. In order to exchange databetween the selected IoT pairs, we propose to employ wireless physical-layer network coding (PNC) to further improve the spectral efficiency and reduce the delay to fulfill the requirements of future IoT applications. Performance evaluations are provided based on both the sum datarate and bit error rate for the proposed NOMA-based MIMO with PNC in the considered massive IoT scenarios.",
  "keyword": "internet of things"
 },
 {
  "No": 79,
  "judul": "Massive Beam-Division Multiple Access for B5G Cellular Internet of Things",
  "abstrak": "In this article, we investigate the issue of massive access in a beyond fifth-generation (B5G) cellular Internet of Things (IoT) network. To reduce the overhead of channel state information acquisition and the complexity of transceiver design, an integrated framework of massive beam-division multiple access (BDMA) is proposed according to the characteristics of beamspace propagation. Then, we analyze the performance of the proposed massive BDMA scheme and derive a closed-form expression for the weighted sum rate in terms of channel conditions and system parameters. To improve the overall performance, we propose a massive access algorithm by jointly optimizing transmit power and receive vector. It is found that the optimal receive vector for each IoT device is the base beam corresponding to the arrival of angle of the IoT device's signal in the beamspace, which significantly simplifies the design of the receiver. Considering the constraint of radio-frequency (RF) chains in practical networks, we propose a clustering-based massive access algorithm, which allocates an RF chain for a cluster. Finally, extensive simulation results confirm that the proposed algorithms can provide small overhead and low complexity massive access schemes for B5G cellular IoT.",
  "keyword": "internet of things"
 },
 {
  "No": 80,
  "judul": "DSLN: Securing Internet of Things Through RF Fingerprint Recognition in Low-SNR Settings",
  "abstrak": "The explosive growth of Internet of Things (IoT) has mandated the security of dataaccess. Although authentication methods can enhance network security, their vulnerability to malicious attacks may be a barrier for the wide deployments in IoT scenarios. To address the security issue, we advocate the use of physical-layer security through radio-frequency (RF) fingerprint recognition. Observing that most RF fingerprint recognition methods show a degradation of performance under low signal-to-noise ratio (SNR) environments, we present a dynamic shrinkage learning network (DSLN) to enhance security for IoT applications, particularly in the setting of low SNR. We design a novel dynamic shrinkage threshold for improving the accuracy of recognition under low-SNR environments. Additionally, we design an identity shortcut for reducing the running time of RF fingerprint recognition. In comparison with convolutional neural network (CNN), recurrent neural network (RNN), and a hybrid CNN+RNN network (CRNN), our proposed DSLN yields accuracy improvements of up to 20%. Moreover, DSLN can reduce the running time by up to 60%, indicating its great potential to a real-time IoT system, e.g., an intelligent automotive system.",
  "keyword": "internet of things"
 },
 {
  "No": 81,
  "judul": "Recent Advances in Artificial Intelligence for Wireless Internet of Things and Cyber–Physical Systems: A Comprehensive Survey",
  "abstrak": "Advances in artificial intelligence (AI) and wireless technology are driving forward the large deployment of interconnected smart technologies that constitute cyber–physical systems (CPSs) and Internet of Things (IoT) for many commercial and military applications. CPS is characterized by communication, computing, and control engineering based on a large volume of dataoriginating from various devices, plants, sensors, etc. Wireless technologies have enabled the ease of networking and communications for both CPS and IoT, by providing massive and critical connectivity and control mechanisms. However, they are prone to challenges, such as low latency, throughput, and scheduling. Recent research trends focus on how to intelligently use datafrom CPS units to enhance wireless connectivity in CPS. AI tools, particularly AI systems and machine learning (ML) algorithms, have been widely applied in the literature to develop efficient schemes for wireless CPS\/IoT. This article presents a review on the role of AI in wireless networking for CPS and IoT. In particular, we focus on ML paradigms, such as transfer learning (TL), distributed learning, and federated learning, that have evolved as building blocks for the utilization of large datafor learning, adaptation, and predictions in CPS and IoT systems that leverage wireless networking. Furthermore, we also highlight challenges faced by current and future wireless networks pertaining to CPS\/IoT, which are yet to be addressed.",
  "keyword": "internet of things"
 },
 {
  "No": 82,
  "judul": "Missing Data Imputation With Bayesian Maximum Entropy for Internet of Things Applications",
  "abstrak": "Internet of Things (IoT) enables the seamless integration of sensors, actuators, and communication devices for real-time applications. IoT systems require good quality sensor data in order to make real-time decisions. However, values are often missing from the sensor data collected owing to faulty sensors, a loss of data during communication, interference, and measurement errors. Considering the spatiotemporal nature of IoT data and the uncertainty of the data collected by sensors, we propose a new framework with which to impute missing values utilizing Bayesian maximum entropy (BME) as a convenient means to estimate the missing data from IoT applications. Missing sensor measurements adversely affect the quality of data, and consequently the performance and outcomes of IoT systems. Our proposed framework incorporates BME in order to impute missing values in diverse IoT scenarios by making use of the combination of low- and high-precision sensors. Our approach can incorporate the measurement errors of low-precision sensors as interval quantities along with the high-precision sensor measurements, making it highly suitable for real-time IoT systems. Our framework is robust to variations in data, requires less execution time, and requires only a single input parameter, thus outperforming existing IoT data imputation methods. The experimental results obtained for three IoT data sets demonstrate the superiority of the BME framework as regards accuracy, running time, and robustness. The framework can additionally be extended to distributed IoT nodes for the online imputation of missing values.",
  "keyword": "internet of things"
 },
 {
  "No": 83,
  "judul": "In.IoT—A New Middleware for Internet of Things",
  "abstrak": "The evolution of Internet of Things (IoT) led to the construction of many IoT middleware, a software that plays a key role since it supports the communication among devices, users, and applications. Although various solutions and studies were proposed, they rarely address crucial privacy and security considerations, especially regarding the message queuing telemetry transport (MQTT) protocol. Moreover, in the majority of the solutions, integrating new devices is a time-consuming task performed manually that cannot be accomplished in a scenario with thousands, maybe millions of devices. In this sense, this article proposes a new IoT middleware, called In.IoT, a scalable, secure, and innovative middleware solution that addresses the middleware concerns identified in this article. In.IoT architectural recommendations and requirements are detailed and can be replicated by new and available solutions. It supports MQTT, CoAP, and HTTP as application-layer protocols. Its performance is evaluated in comparison with the most promising solutions available in the literature and the results obtained by the proposed solution are extremely promising. In.IoT is evaluated, demonstrated, validated, and it is ready and available for use.",
  "keyword": "internet of things"
 },
 {
  "No": 84,
  "judul": "Load-Balancing Algorithm for Multiple Gateways in Fog-Based Internet of Things",
  "abstrak": "This article investigates the performance of multicriteria-based load-balancing scheme among gateways in fog-assisted Internet of Things (IoT). We employ a queueing model of the IoT system to calculate the latency of datastreams from the IoT devices to the applications. However, a gateway node may easily become congested since all the traffic to IP networks are directed toward the gateway. A congested point can affect system performance and may cause reliability problems in the system. Thus, we employ multiple gateways to alleviate the performance degradation of the network, along with a multicriteria decision-making (MCDM)-based load-balancing policy among the gateways to achieve a global load fairness. The proposed model is evaluated in single-hop IPv6 over low-power wireless personal area networks (6LoWPANs). The evaluation results show the effectiveness of the proposed load-balancing model in providing fast and reliable responses to user queries.",
  "keyword": "internet of things"
 },
 {
  "No": 85,
  "judul": "Immune System for the Internet of Things Using Edge Technologies",
  "abstrak": "The Internet of Things (IoT) and edge computing are starting to go hand in hand. By providing cloud services close to end-users, edge paradigms enhance the functionality of IoT deployments, and facilitate the creation of novel services such as augmented systems. Furthermore, the very nature of these paradigms also enables the creation of a proactive defense architecture, an immune system, which allows authorized immune cells (e.g., virtual machines) to traverse edge nodes and analyze the security and consistency of the underlying IoT infrastructure. In this paper, we analyze the requirements for the development of an immune system for the IoT, and propose a security architecture that satisfies these requirements. We also describe how such a system can be instantiated in edge computing infrastructures using existing technologies. Finally, we explore the potential application of immune systems to other scenarios and purposes.",
  "keyword": "internet of things"
 },
 {
  "No": 86,
  "judul": "Sharded Blockchain for Collaborative Computing in the Internet of Things: Combined of Dynamic Clustering and Deep Reinforcement Learning Approach",
  "abstrak": "Immutability, decentralization, and linear promoted scalability make the sharded blockchain a promising solution, which can effectively address the trust issue in the large-scale Internet of Things (IoT). However, currently, the throughput of sharded blockchains is still limited when it comes to high proportion of cross-shard transactions (CSTs). On the other hand, the assemblage characteristic of the collaborative computing in IoT has not been received attention. Therefore, in this article, we present a clustering-based sharded blockchain strategy for collaborative computing in the IoT, where the sharding of the blockchain system is implemented in two steps: \nK-means\n-clustering-based user grouping and the assignment of consensus nodes. In this framework, how to reasonably group the IoT users while simultaneously guaranteeing the system performance is the key point. Specifically, we describe the datatransactions among IoT devices by datatransaction flow graph (DTFG) based on a dynamic stochastic block model. Then, formed as a Markov decision process (MDP), the optimization of the cluster number (shard number) and the adjustment of consensus parameters are jointly trained by deep reinforcement learning (DRL). Simulation results show that the proposed scheme improves the scalability of the sharded blockchain in the IoT application.",
  "keyword": "internet of things"
 },
 {
  "No": 87,
  "judul": "Joint Job Partitioning and Collaborative Computation Offloading for Internet of Things",
  "abstrak": "Advances in Internet of Things (IoT) bring massive intelligent applications, many of which are computation intensive and time sensitive. With limited resources of IoT devices, mobile computation offloading can be exploited to offload part of the applications to nearby devices that have more powerful computing resources, thereby speeding up the applications and reducing the energy consumption. In this paper, we consider application partitioning and collaborative computation offloading in IoT networks, in order to meet the completion deadline of the applications while minimizing the overall energy consumption. The problem is formulated as a binary integer linear programming problem, which is transformed into a weighted bipartite matching problem and then solved by the centralized Kuhn-Munkres algorithm. To fit the large-scale IoT scenarios, three distributed algorithms are then introduced from different perspectives. The first one is referred to as the noncooperative matching (NCM) algorithm, where each node makes offloading decision based on its own interest in minimizing energy consumption. Afterward, an asynchronous greedy matching (AGM) algorithm is developed by considering the mutual interest of the requestor and collaborator pairs in terms of their energy consumptions. Finally, a maximum differential energy matching (MDEM) algorithm is devised by relaxing the network stability requirement, which can further benefit the energy efficiency for all network nodes. Theoretical analysis and simulation results demonstrate that both the NCM and AGM algorithms guarantee the network stability and improve the energy saving compared with entirely local execution, while the MDEM algorithm can further achieve near-optimal energy consumption at the expense of higher implementation overheads.",
  "keyword": "internet of things"
 },
 {
  "No": 88,
  "judul": "Load-Balanced and QoS-Aware Software-Defined Internet of Things",
  "abstrak": "Internet of Things (IoT) offers a variety of solutions to control industrial environments. The new generation of IoT consists of millions of machines generating huge traffic volumes; this challenges the network in achieving the Quality-of-Service (QoS) and avoiding overload. Diverse classes of applications in IoT are subject to specific QoS treatments. In addition, traffic should be distributed among IoT servers based on their available capacity. In this article, we propose a novel framework based on software-defined networking (SDN) to fulfill the QoS requirements of various IoT services and to balance traffic between IoT servers simultaneously. At first, the problem is formulated as an integer linear programming (ILP) model that is NP-hard. Then, a predictive and proactive heuristic mechanism based on time-series analysis and fuzzy logic is proposed. Afterward, the proposed framework is implemented in a real testbed, which consists of the Open vSwitch, Floodlight controller, and Kaa servers. To evaluate the performance, various experiments are conducted under different scenarios. The results indicate the improved IoT QoS parameters, including throughput and delay, and illustrate the nonoccurrence of overload on IoT servers in heavy traffic. Furthermore, the results show improved performance compared to similar methods.",
  "keyword": "internet of things"
 },
 {
  "No": 89,
  "judul": "Distributed Learning for Low Latency Machine Type Communication in a Massive Internet of Things",
  "abstrak": "The Internet of Things (IoT) will encompass a massive number of machine type devices that must wirelessly transmit, in near real-time, a diverse set of messages sensed from their environment. Designing resource allocation schemes to support such coexistent, heterogeneous communication is hence a key IoT challenge. In particular, there is a need for self-organizing resource allocation solutions that can account for unique IoT features, such as massive scale and stringent resource constraints. In this paper, a novel finite memory multistate sequential learning framework is proposed to enable diverse IoT devices to share limited communication resources, while transmitting both delay-tolerant, periodic messages and urgent, critical messages. The proposed learning framework enables the IoT devices to learn the number of critical messages and to reallocate the communication resources for the periodic messages to be used for the critical messages. Furthermore, the proposed learning framework explicitly accounts for IoT device limitations in terms of memory and computational capabilities. The convergence of the proposed learning framework is proved, and the lowest expected delay that the IoT devices can achieve using this learning framework is derived. Furthermore, the effectiveness of the proposed learning algorithm in IoT networks with different delay targets, network densities, probabilities of detection, and memory sizes is analyzed in terms of the probability of a successful random access (RA) request and percentage of devices that learned correctly. Simulation results show that, for a delay threshold of 1.25 ms, the average achieved delay is 0.71 ms and the delay threshold is satisfied with probability 0.87. Moreover, for a massive network, a delay threshold of 2.5 ms is satisfied with probability 0.92. The results also show that the proposed learning algorithm is very effective in reducing the delay of urgent, critical messages by intelligently reallocating the commu...",
  "keyword": "internet of things"
 },
 {
  "No": 90,
  "judul": "An Application Development Framework for Internet-of-Things Service Orchestration",
  "abstrak": "Application development for the Internet of Things (IoT) poses immense challenges due to the lack of standard development frameworks, tools, and techniques to assist end users in dealing with the complexity of IoT systems during application development. These challenges invoke the use of model-driven development (MDD) along with the representational state transfer (REST) architecture to develop IoT applications, supporting model generation at different abstraction levels while generating software implementation artifacts for heterogeneous platforms and ensuring loose coupling in complex IoT systems. This article proposes an IoT application development framework, named IADev, which uses attribute-driven design and MDD to address the above-mentioned challenges. This framework is composed of two major steps, including iterative architecture development using attribute-driven design and generating models to guide the transformation using MDD. IADev uses attribute-driven design to transform the requirements into a solution architecture by considering the concerns of all involved stakeholders, and then, MDD metamodels are generated to hierarchically transform the design components into the software artifacts. We evaluate IADev for a smart vehicle scenario in an intelligent transportation system to generate an executable implementation code for a real-world system. The case study experiments proclaim that IADev achieves higher satisfaction of the participants for the IoT application development and service orchestration, as compared to conventional approaches. Finally, we propose an architecture that uses IADev with the Siemens IoT cloud platform for service orchestration in industrial IoT.",
  "keyword": "internet of things"
 },
 {
  "No": 91,
  "judul": "Toward 6G Internet of Things and the Convergence With RoF System",
  "abstrak": "The Internet of Things (IoT) has been a promising communication paradigm that involves sensors, microcontrollers, and transceivers for an efficient communication and computation system. The infrastructure and the applications shall enable and improve the intelligent management of our city service, workspace, and daily life. This article aims at the future 6G vision of IoT, and discusses the convergence with the Radio-over-Fiber (RoF) system. Comparing with the IoT services included in the 5G deployment, 6G IoT exploits high-density heterogeneous devices involving extremely high capacity, supporting much more robust system architecture and artificial intelligence (AI)-based smart algorithms. The RoF is one of the most promising enablers for the outstanding characters of flexibility and efficiency of 6G IoT systems. This article first introduces the IoT envolution roadmap from 5G toward 6G and the potency of optic fiber and RoF technologies. Then, we present the rapidly expanding RoF market and compatible technologies related to IoT-RoF convergence with the discussion on the current outstanding works in multiple dimensions. Finally, we investigate the challenges ahead for the future RoF supported 6G IoT system and the emerging technology solutions.",
  "keyword": "internet of things"
 },
 {
  "No": 92,
  "judul": "Data Allocation Mechanism for Internet-of-Things Systems With Blockchain",
  "abstrak": "The use of Internet of Things (IoT) has introduced genuine concerns regarding data security and its privacy when data are in collection, exchange, and use. Meanwhile, blockchain offers a distributed and encrypted ledger designed to allow the creation of immutable and tamper-proof records of data at different locations. While blockchain may enhance IoT with innate security, data integrity, and autonomous governance, IoT data management and its allocation in blockchain still remain an architectural concern. In this article, we propose a novel context-aware mechanism for on-chain data allocation in IoT-blockchain systems. Specifically, we design a data controller based on fuzzy logic to calculate the Rating of Allocation (RoA) value of each data request considering multiple context parameters, i.e., data, network, and quality and decide its on-chain allocation. Furthermore, we illustrate how the design and realization of the mechanism lead to refinements of two commonly used IoT-blockchain architectural styles (i.e., blockchain-based cloud and fog). To demonstrate the effectiveness of our approach, we instantiate the data allocation mechanism in the blockchain-based cloud and fog architectures and evaluate their performance using FogBus. We also compare the efficacy of our approach to the existing decision-making mechanisms through the deployment of a real-world healthcare application. The experimental results suggest that the realization of the data allocation mechanism improves network usage, latency, and blockchain storage and reduces energy consumption.",
  "keyword": "internet of things"
 },
 {
  "No": 93,
  "judul": "Preserving Balance Between Privacy and Data Integrity in Edge-Assisted Internet of Things",
  "abstrak": "Internet of Things (IoT) devices and the edge jointly broaden the IoT's sensing capability and the monitoring scope for various applications. Though accessing sensing data and making decisions through IoT smart devices turns out to be commonplace, it is challenging to guarantee user privacy and preserve the accuracy (integrity) of the collected data. The IoT smart devices frequently lose either IoT user's privacy or data integrity. This also makes it crucial to put a threshold on the cost of computation and load of the IoT devices, as gradually more IoT services demand access to the resources that devices offer. In this article, we propose BalancePIC, a scheme that attempts to preserve a balance in the three aspects (user privacy, data integrity in edge-assisted IoT devices, and the computational cost). It achieves the balance through a balanced truth discovery approach and a proposed enhanced technique for data privacy, which are used in IoT devices and edge server interactions. It authenticates the IoT user participation with privacy in the truth discovery process through a biometric-ECC-based authentication algorithm. The nature of the BalancePIC scheme is to straightforwardly provide the likelihood for a simple amendment on the cryptography technique and weight assignment. This lessens the overall computational cost for the IoT user devices but also restricts the communications between the user devices and the edge server, which is important for data integrity. We present an enhanced technique to preserve privacy by guarding the user from potential threats and suspicious data collection parties. To achieve this, BalancePIC takes steps to blur the original sensory data of the device by processing results in groups called zones. Simulation result analysis provides evidence for the balance preservation in the three aspects.",
  "keyword": "internet of things"
 },
 {
  "No": 94,
  "judul": "Reliability in Internet of Things: Current Status and Future Perspectives",
  "abstrak": "The Internet of Things (IoT) aims to transform the human society toward becoming intelligent, convenient, and efficient with potentially enormous economic and environmental benefits. Reliability is one of the main challenges that must be addressed to enable this revolutionized transformation. Based on the layered IoT architecture, this article first identifies reliability challenges posed by specific enabling technologies of each layer. This article then presents a systematic synthesis and review of IoT reliability-related literature. Reliability models and solutions at four layers (perception, communication, support, and application) are reflected and classified. Despite the rich body of works performed, the IoT reliability research is still in its early stage. Challenging research problems and opportunities are then discussed in relation to current underexplored behaviors and future new aspects of evolving IoT system complexity and dynamics.",
  "keyword": "internet of things"
 },
 {
  "No": 95,
  "judul": "Big Data Analytics for 6G-Enabled Massive Internet of Things",
  "abstrak": "The purposes are to enable large-scale Internet of Things (IoT) devices to analyze data more effectively and provide high-efficiency, low-energy, and wide-coverage technical services for terminals. The channel model and energy loss model analyze the devices' access performance, data transmission path delay, energy consumption in the IoT, and large-scale devices' access in the cellular narrowband IoT (NB-IoT) based on big data analysis technology are also discussed. The results show that in the access success rate analysis, the access success rate is the highest with an access time ( T) of 5 s and a preamble resource number ( K) of 25. The restriction factor is inversely proportional to the access success rate. In the node utilization analysis, different transmission node priorities result in different node utilization, and priority 2's node utilization is better than that of priority 1. Moreover, local data makes data analysis and transmission faster. The search time is prolonged, and the corresponding energy consumption is also higher without local data. In the energy consumption analysis, with the 6-generation (6G) technology, different interference thresholds lead to the different energy efficiency of data transmission. The larger the interference threshold, the higher the energy efficiency. Therefore, the 6G-based big data analysis technology can significantly improve large-scale IoT devices' access success rate and enable the system to meet the requirements of low energy consumption and high access success rate, significant for research on more devices' access data analysis.",
  "keyword": "internet of things"
 },
 {
  "No": 96,
  "judul": "UAV-Aided MIMO Communications for 5G Internet of Things",
  "abstrak": "The unmanned aerial vehicle (UAV) is a promising enabler of the Internet of Things (IoT) vision, due to its agile maneuverability. In this paper, we explore the potential gain of UAV-aided datacollection in a generalized IoT scenario. Particularly, a composite channel model, including both large-scale and small-scale fading is used to depict typical propagation environments. Moreover, rigorous energy constraints are considered to characterize IoT devices as practically as possible. A multiantenna UAV is employed, which can communicate with a cluster of single-antenna IoT devices to form a virtual MIMO link. We formulate a whole-trajectory-oriented optimization problem, where the transmission duration and the transmit power of all devices are jointly designed to maximize the datacollection efficiency for the whole flight. Different from previous studies, only the slowly varying large-scale channel state information is assumed available, to coincide with the fact that practically it is quite difficult to predictively acquire the random small-scale channel fading prior to the UAV flight. We propose an iterative scheme to overcome the nonconvexity of the formulated problem. The presented scheme can provide a significant performance gain over traditional schemes and converges quickly.",
  "keyword": "internet of things"
 },
 {
  "No": 97,
  "judul": "Internet of Things (IoT): Research, Simulators, and Testbeds",
  "abstrak": "The Internet of Things (IoT) vision is increasingly being realized to facilitate convenient and efficient human living. To conduct effective IoT research using the most appropriate tools and techniques, we discuss recent research trends in the IoT area along with current challenges faced by the IoT research community. Several existing and emerging IoT research areas such as lightweight energy-efficient protocol development, object cognition and intelligence, as well as the critical need for robust security and privacy mechanisms will continue to be significant fields of research for IoT. IoT research can be a challenging process spanning both virtual and physical domains through the use of simulators and testbeds to develop and validate the initial proof-of-concepts and subsequent prototypes. To support researchers in planning IoT research activities, we present a comparative analysis of existing simulation tools categorized based on the scope of coverage of the IoT architecture layers. We compare existing large-scale IoT testbeds that have been adopted by researchers for examining the physical IoT prototypes. Finally, we discuss several open challenges of current IoT simulators and testbeds that need to be addressed by the IoT research community to conduct large-scale, robust and effective IoT simulation, and prototype evaluations.",
  "keyword": "internet of things"
 },
 {
  "No": 98,
  "judul": "Internet of Things (IoT) Cybersecurity Research: A Review of Current Research Topics",
  "abstrak": "As an emerging technology, the Internet of Things (IoT) revolutionized the global network comprising of people, smart devices, intelligent objects, dataset, and information. The development of IoT is still in its infancy and many related issues need to be solved. IoT is a unified concept of embedding everything. IoT has a great chance to make the world a higher level of accessibility, integrity, availability, scalability, confidentiality, and interoperability. However, how to protect IoT is a challenging task. System security is the foundation for the development of IoT. This article systematically reviews IoT cybersecurity. The key considerations are the protection and integration of heterogeneous smart devices and information communication technologies (ICT). This review provides useful information and insights to researchers and practitioners who are interested in cybersecurity of IoT, including the current research of IoT cybersecurity, IoT cybersecurity architecture and taxonomy, key enabling countermeasures and strategies, major applications in industries, research trends and challenges.",
  "keyword": "internet of things"
 },
 {
  "No": 99,
  "judul": "Review of Internet of Things (IoT) in Electric Power and Energy Systems",
  "abstrak": "A transformation is underway in electric power and energy systems (EPESs) to provide clean distributed energy for sustainable global economic growth. Internet of Things (IoT) is at the forefront of this transformation imparting capabilities, such as real-time monitoring, situational awareness and intelligence, control, and cyber security to transform the existing EPES into intelligent cyber-enabled EPES, which is more efficient, secure, reliable, resilient, and sustainable. Additionally, digitizing the electric power ecosystem using IoT improves asset visibility, optimal management of distributed generation, eliminates energy wastage, and create savings. IoT has a significant impact on EPESs and offers several opportunities for growth and development. There are several challenges with the deployment of IoT for EPESs. Viable solutions need to be developed to overcome these challenges to ensure continued growth of IoT for EPESs. The advancements in computational intelligence capabilities can evolve an intelligent IoT system by emulating biological nervous systems with cognitive computation, streaming and distributed analytics including at the edge and device levels. This review paper provides an assessment of the role, impact and challenges of IoT in transforming EPESs.",
  "keyword": "internet of things"
 },
 {
  "No": 100,
  "judul": "Internet of Hybrid Energy Harvesting Things",
  "abstrak": "Internet of Things (IoT) is a perfect candidate to realize efficient observation and management for Smart City concept. This requires deployment of large number of wireless devices. However, replenishing batteries of thousands, maybe millions of devices may be hard or even impossible. In order to solve this problem, Internet of Energy Harvesting Things (IoEHT) is proposed. Although the first studies on IoEHT focused on energy harvesting (EH) as an auxiliary power provisioning method, now completely battery-free and self-sufficient systems are envisioned. Taking advantage of diverse sources that the concept of Smart City offers helps us to fully appreciate the capacity of EH. In this way, we address the primary shortcomings of IoEHT; availability, unreliability, and insufficiency by the Internet of Hybrid EH Things (IoHEHT). In this paper, we survey the various EH opportunities, propose an hybrid EH system, and discuss energy and datamanagement issues for battery-free operation. We mathematically prove advantages of hybrid EH compared to single source harvesting as well. We also point out to hardware requirements and present the open research directions for different network layers specific to IoHEHT for Smart City concept.",
  "keyword": "internet of things"
 },
 {
  "No": 101,
  "judul": "TriBoDeS: A Tri-Blockchain-Based Detection and Sharing Scheme for Dangerous Road Condition Information in Internet of Vehicles",
  "abstrak": "Bad weather or environmental factors, particularly in remote mountain areas, may result in unsafe driving conditions and consequently road traffic accidents. As the deployment of large-scale sensing nodes for reporting road conditions is too expensive, the crowdsourcing method or reporting by sensors in vehicles themselves will be easier to deploy and more practical. However, those participant sensing methods impose some difficulties, such as fake information, reporter misbehavior, and timeliness. Thus, we propose a tri-blockchain-based Internet of Vehicles system, called TriBoDeS, to facilitate real-time information detection and sharing. It is designed to guarantee concurrency and security to dynamically store, manage, and share information uploaded by vehicles with great efficiency. Such information will be announced on the blockchain under the autonomous identification of vehicles in low-trust conditions. In order to ensure the software’s security, TriBoDeS can monitor the software’s state, detect identified malicious activities, and respond accordingly. To ensure datasecurity, a role-based management mechanism is introduced to achieve fine-grained control over permissions, and confidence rules are established to guarantee the authenticity of thedata. To demonstrate the applicability of the proposed scheme, we evaluate its performance (e.g., computing and communication overheads) and security (e.g., resiliency against common attacks) over a consortium blockchain. The experimental results demonstrate that, under the conditions of a sufficient number of vehicles, the TriBoDeS system is capable of real-time information sharing while ensuring the security of user information. Compared to conventional single-chain systems, the TriBoDeS system achieves a 2.75-time improvement in efficiency.",
  "keyword": "internet security"
 },
 {
  "No": 102,
  "judul": "From Pre-Quantum to Post-Quantum IoT Security: A Survey on Quantum-Resistant Cryptosystems for the Internet of Things",
  "abstrak": "Although quantum computing is still in its nascent age, its evolution threatens the most popular public-key encryption systems. Such systems are essential for today's Internet security due to their ability for solving the key distribution problem and for providing high security in insecure communications channels that allow for accessing websites or for exchanging e-mails, financial transactions, digitally signed documents, military communications or medical datasets. Cryptosystems like Rivest-Shamir-Adleman (RSA), elliptic curve cryptography (ECC) or Diffie-Hellman have spread worldwide and are part of diverse key Internet standards like Transport Layer Security (TLS), which are used both by traditional computers and Internet of Things (IoT) devices. It is especially difficult to provide high security to IoT devices, mainly because many of them rely on batteries and are resource constrained in terms of computational power and memory, which implies that specific energy-efficient and lightweight algorithms need to be designed and implemented for them. These restrictions become relevant challenges when implementing cryptosystems that involve intensive mathematical operations and demand substantial computational resources, which are often required in applications where dataprivacy has to be preserved for the long term, like IoT applications for defense, mission-critical scenarios or smart healthcare. Quantum computing threatens such a long-term IoT device security and researchers are currently developing solutions to mitigate such a threat. This article provides a survey on what can be called post-quantum IoT systems (IoT systems protected from the currently known quantum computing attacks): the main post-quantum cryptosystems and initiatives are reviewed, the most relevant IoT architectures and challenges are analyzed, and the expected future trends are indicated. Thus, this article is aimed at providing a wide view of post-quantum IoT security and give useful guidelines...",
  "keyword": "internet security"
 },
 {
  "No": 103,
  "judul": "Certificate-Based Anonymous Device Access Control Scheme for IoT Environment",
  "abstrak": "As the “Internet communications infrastructure” develops to encircle smart devices, it is very much essential for designing suitable methods for secure communications with these smart devices, in the future Internet of Things (IoT) applications context. Due to wireless communication among the IoT smart devices and the gateway node (GWN), several security threats may arise in the IoT environment, including replay, man-in-the-middle, impersonation, malicious devices deployment, and physical devices capture attacks. In this article, to mitigate such security threats, we design a new certificate-based device access control scheme in IoT environment which is not only secure against mentioned attacks, but it also preserves anonymity property. A detailed security analysis using the widely accepted real-or-random (ROR) model-based formal security analysis, informal security analysis, and also formal security verification based on the broadly accepted automated validation of Internet security protocols and applications (AVISPAs) tool has been performed on the proposed scheme to show that it is secure against various known attacks. In addition, a comprehensive comparative analysis among the proposed scheme and other relevant schemes shows that a better tradeoff among the security and functionality attributes, communication, and computational costs is achieved for the proposed scheme as compared to other schemes.",
  "keyword": "internet security"
 },
 {
  "No": 104,
  "judul": "Security in the Internet of Things Application Layer: Requirements, Threats, and Solutions",
  "abstrak": "Communication systems and networks are evolving as an integral part of not only of our everyday life but also as a part of the industry, fundamental infrastructures, companies, etc. Current directions and concepts, such as the Internet of Things (IoT), promise the enhanced quality of life, greater business opportunities, cost-effective manufacturing, and efficient operation management through ubiquitous connectivity and deployment of smart physical objects. IoT networks can collect, preprocess, and transmit vast amounts of datasets. A considerable portion of this datasets is security- and privacy-critical datasets, which makes IoT networks a tempting option for attackers. Given that these networks deal with the actual aspects of our lives and fundamental infrastructures (e.g. smart grids), security in such networks is crucial. The large scale of these networks and their unique characteristics and complexity bring further vulnerabilities. In this study, we focus on the IoT application layer, security requirements, threats, and countermeasures in this layer, and some of the open issues and future research lines.",
  "keyword": "internet security"
 },
 {
  "No": 105,
  "judul": "Internet of Things: A Comprehensive Study of Security Issues and Defense Mechanisms",
  "abstrak": "The Internet of Things (IoT) is an evolving global trend in Web-based information architecture aiding in the exchange of services and goods over a network without necessitating human-to-human or human-to-computer interaction. It has the potential to revolutionize physical world interaction of individuals and the organizations. The application of IoT can be recognized significantly in many areas such as in healthcare, resource management, learning, knowledge processing, and many more. The practical realization of IoT is met with a plethora of security and privacy challenges that need to be tackled for IoT's successful deployment on a commercially viable large scale. This paper analyzes the security issues related to IoT networks through an analysis of the existing empirical researches to get an insight on the security requirements of the IoT networks. The findings of the study revealed that security threats are one of the biggest and ever-growing challenges for IoT, and it is essential to substantially mitigate them for the success of this platform.",
  "keyword": "internet security"
 },
 {
  "No": 106,
  "judul": "TinyIKE: Lightweight IKEv2 for Internet of Things",
  "abstrak": "There is unanimous consensus that cyber security in the Internet of Things (IoT) is necessary. In cyber security, key establishment is one of the toughest problems. It is even more challenging in resource-constrained but Internet-connected IoT devices that use low-power wireless communication. A number of IoT communication protocols define cryptographic mechanisms for confidentiality and integrity services but do not specify key management. For example, IEEE 802.15.4, RPL, and object security all rely on external key management protocols. Due to the lack of automatic key management support, IoT devices either end up using preshared keys or no security at all. In this paper, we overcome these challenges and present TinyIKE, a lightweight adaptation of Internet Key Exchange version 2 (IKEv2) for the IoT. Using TinyIKE, we solve the key establishment problem for multiple IoT protocols using a single IKEv2-based solution. We implement TinyIKE for resource-constrained IoT devices that run the Contiki OS. The TinyIKE implementation supports full certificate-based IKEv2 that uses elliptic curve cryptography. In order to ensure the feasibility of TinyIKE in the IoT, we perform an extensive evaluation of TinyIKE using a setup consisting of real IoT hardware.",
  "keyword": "internet security"
 },
 {
  "No": 107,
  "judul": "Interdependent Strategic Security Risk Management With Bounded Rationality in the Internet of Things",
  "abstrak": "With the increasing connectivity enabled by the Internet of Things (IoT), security becomes a critical concern, and users should invest to secure their IoT applications. Due to the massive devices in the IoT network, users cannot be aware of the security policies taken by all its connected neighbors. Instead, a user makes security decisions based on the cyber risks that he perceives by observing a selected number of nodes. To this end, we propose a model which incorporates the limited attention or bounded rationality nature of players in the IoT. Specifically, each individual builds a sparse cognitive network of nodes to respond to. Based on this simplified cognitive network representation, each user then determines his security management policy by minimizing his own real-world security cost. The bounded rational decision-makings of players and their cognitive network formations are interdependent and thus should be addressed in a holistic manner. We establish a games-in-games framework and propose a Gestalt Nash equilibrium (GNE) solution concept to characterize the decisions of agents and quantify their risk of bounded perception due to the limited attention. In addition, we design a proximal-based iterative algorithm to compute the GNE. With case studies of smart communities, the designed algorithm can successfully identify the critical users whose decisions need to be taken into account by the other users during the security management.",
  "keyword": "internet security"
 },
 {
  "No": 108,
  "judul": "A Survey on Emerging SDN and NFV Security Mechanisms for IoT Systems",
  "abstrak": "The explosive rise of Internet of Things (IoT) systems have notably increased the potential attack surfaces for cybercriminals. Accounting for the features and constraints of IoT devices, traditional security countermeasures can be inefficient in dynamic IoT environments. In this vein, the advantages introduced by software defined networking (SDN) and network function virtualization (NFV) have the potential to reshape the landscape of cybersecurity for IoT systems. To this aim, we provide a comprehensive analysis of security features introduced by NFV and SDN, describing the manifold strategies able to monitor, protect, and react to IoT security threats. We also present lessons learned in the adoption of SDN\/NFV-based protection approaches in IoT environments, comparing them with conventional security countermeasures. Finally, we deeply discuss the open challenges related to emerging SDN- and NFV-based security mechanisms, aiming to provide promising directives to conduct future research in this fervent area.",
  "keyword": "internet security"
 },
 {
  "No": 109,
  "judul": "IoT-Enabled Sensors in Automation Systems and Their Security Challenges",
  "abstrak": "Today, Internet of Things (IoT)-based sensor devices are ubiquitous. Being cost effective and easy to deploy, they are also considered for many applications outside their original domain, which was consumer electronics. Factory and process automation, smart buildings and homes, and, in general, Industry 4.0 are application fields in which the use of IoT technology is gaining popularity, often in addition to existing, classical communication architectures on the operational technology level. IoT devices, however, typically have a different philosophy for communication and dataexchange, which makes them easy to use but poses security challenges by bypassing established security architectures, such as the classical defense-in-depth concept defined, for instance, in the IEC 62443 standard. This letter highlights today's security needs and concepts in industrial environments. Furthermore, it looks at possible new attack surfaces opened by IoT-based applications and shows ways how to bridge the security gap.",
  "keyword": "internet security"
 },
 {
  "No": 110,
  "judul": "A Secure Platform Model Based on ARM Platform Security Architecture for IoT Devices",
  "abstrak": "The proliferation of Internet of Things (IoT) devices comes with many challenges among which security is one of the most serious issues. In order to address the security issue for low-end IoT devices, ARM recently proposed the platform security architecture (PSA), which provides execution isolation to safely manage and protect the computing resources of low-end IoT devices. However, developers implementing IoT services for PSA-based IoT devices need to follow complex development procedures and understand the PSA hardware, which dramatically increases the development time and cost of PSA-based IoT devices. This article analyzes vulnerabilities that may arise from general-purpose low-end IoT devices to derive the security requirements and essential security services for PSA-based IoT devices, and proposes a secure platform model based on the analysis results. The proposed secure platform model consists of System Security Services and Application Security Services based on the basic PSA model and essential trusted subsystems, and it is designed to be flexible and applicable to various types of PSA-based IoT devices. In addition, it provides secure platform services APIs to enable easy and fast development of IoT services. To evaluate the proposed secure platform model, two proof-of-concept implementations are provided by using both the basic PSA model with secure element (SE) and a reference device for ARM’s PSA. Finally, a case study shows that the development of IoT services can be done easily and quickly using the proposed security platform model.",
  "keyword": "internet security"
 },
 {
  "No": 111,
  "judul": "Internet of Things Security",
  "abstrak": "The emergence of the Internet of Things (IoT) with its sprawling set of technologies and use cases paves way for diversified and new service providers to develop a plethora of connected products and services for a go-ahead business and enrich lives of individuals. As the new service providers may be unaware of the threats their services face and the emerging categories of first time connected devices, IoT services, use cases and the network types comes along with a new threat landscape there is a huge possibility for even a Zero-day exploits. The provision of wide area connectivity to an ever-widening variety of IoT services will increase the whole ecosystem's exposure to fraud and attack. As the security issues are a significant inhibitor to the deployment of many new IoT services and attackers are showing ever greater interest in this area, this research article presents an overview of the threat vectors to the IoT ecosystem and the expected security features with Security-as-a-Network Service (SENSE) and other solutions that need to be in place to thwart the evolving security threats.",
  "keyword": "internet security"
 },
 {
  "No": 112,
  "judul": "Cooperative Jamming for Physical Layer Security Enhancement in Internet of Things",
  "abstrak": "Internet of Things (IoT) is becoming an emerging paradigm to achieve ubiquitous connectivity, via massive deployment of physical objects, such as sensors, controllers, and actuators. However, concerns on the IoT security are raised due to the wireless broadcasting nature and the energy constraint of the physical objects. In this paper, we study secure downlink transmission from a controller to an actuator, with the help of a cooperative jammer to fight against multiple passive and noncolluding eavesdroppers. In addition to artificial noise aided secrecy beamforming for secure transmission, cooperative jamming (CJ) is explored to further enhance physical layer security. In particular, we provide a secrecy enhancing transmit design to minimize the secrecy outage probability (SOP), subject to a minimum requirement on the secrecy rate. Based on a strict mathematical analysis, we further characterize the impacts of the main channel quality and the minimum secrecy rate on transmit designs. Numerical results confirm that our design can enhance both security (in terms of SOP) and power efficiency as compared with the approach without CJ.",
  "keyword": "internet security"
 },
 {
  "No": 113,
  "judul": "The Performance Evaluation of Blockchain-Based Security and Privacy Systems for the Internet of Things: A Tutorial",
  "abstrak": "This article presents research challenges and a tutorial on performance evaluation of blockchain-based security and privacy systems for the Internet of Things (IoT). We start by summarizing the existing surveys that deal with blockchain security for IoT networks. Then, we review the blockchain-based security and privacy systems for seventeen types of IoT applications, e.g., Industry 4.0, software-defined networking, edge computing, Internet of Drones, Internet of Cloud, Internet of Energy, Internet of Vehicles, etc. We also review various consensus algorithms and provide a comparison with respect to the nine properties, such as latency, throughput, computation, storage, and communication costs, scalability, attack model, advantage, disadvantage, etc. Moreover, we present the security analysis techniques and provide a classification into four categories, including Burrows, Abadi, and Needham (BAN) logic, game theory, theory analysis, and AVISPA tool. In addition, we analyze the performance metrics, blockchain testbeds, and cryptography libraries used in the performance evaluation of blockchain-based security and privacy systems for the IoT networks. Based on the current survey, we discuss the major steps to follow for building and evaluating blockchain-based security and privacy systems. Finally, we discuss and highlight open challenges and future research opportunities.",
  "keyword": "internet security"
 },
 {
  "No": 114,
  "judul": "Changing the Game: A Micro Moving Target IPv6 Defense for the Internet of Things",
  "abstrak": "This letter explores the uses of a micro moving target IPv6 defense (μMT6D). In this initial experiment we assess the power consumption overhead and detail the overall security benefits gained from use of this technique. Moving target defenses were seen as game changers in the security field and now we must change the game once more and look toward their application for IoT devices. With a need to provide privacy and a defense against targeted attacks on resource constrained devices that are a part of vital communication and control systems, μMT6D is a viable solution that we continue to develop and assess for future use.",
  "keyword": "internet security"
 },
 {
  "No": 115,
  "judul": "A Survey on Security and Privacy Issues in Edge-Computing-Assisted Internet of Things",
  "abstrak": "Internet of Things (IoT) is an innovative paradigm envisioned to provide massive applications that are now part of our daily lives. Millions of smart devices are deployed within complex networks to provide vibrant functionalities, including communications, monitoring, and controlling of critical infrastructures. However, this massive growth of IoT devices and the corresponding huge datatraffic generated at the edge of the network created additional burdens on the state-of-the-art centralized cloud computing paradigm due to the bandwidth and resource scarcity. Hence, edge computing (EC) is emerging as an innovative strategy that brings dataprocessing and storage near to the end users, leading to what is called the EC-assisted IoT. Although this paradigm provides unique features and enhanced Quality of Service (QoS), it also introduces huge risks in datasecurity and privacy aspects. This article conducts a comprehensive survey on security and privacy issues in the context of EC-assisted IoT. In particular, we first present an overview of EC-assisted IoT, including definitions, applications, architecture, advantages, and challenges. Second, we define security and privacy in the context of EC-assisted IoT. Then, we extensively discuss the major classifications of attacks in EC-assisted IoT and provide possible solutions and countermeasures along with the related research efforts. After that, we further classify some security and privacy issues as discussed in the literature based on security services and based on security objectives and functions. Finally, several open challenges and future research directions for secure EC-assisted IoT paradigm are also extensively provided.",
  "keyword": "internet security"
 },
 {
  "No": 116,
  "judul": "Blockchain-Based Secure and Lightweight Authentication for Internet of Things",
  "abstrak": "Over the past decade, the Internet of Things (IoT) is widely adopted in various domains, including education, commerce, government, and healthcare. There are also many IoT-based applications drawn significant attentions in recent years. With the increasing numbers of the connected devices in the IoT system, one of the challenging tasks is to ensure devices’ authenticity, which allows users to have a high confidence in the decision. In addition, due to the heterogeneity of the IoT system and the resource-constrained devices, how to efficiently manage such system and guarantee the security and privacy for devices is concerned. In this article, we proposed a new blockchain-based authentication scheme to meet the challenges. Our proposed framework combines the blockchain technique and the modular square root algorithm to achieve an effective authentication process. Besides, we demonstrate the security and utility of the proposed scheme by providing the security analysis and the detailed experiment.",
  "keyword": "internet security"
 },
 {
  "No": 117,
  "judul": "The Security and Privacy of Mobile-Edge Computing: An Artificial Intelligence Perspective",
  "abstrak": "Mobile-edge computing (MEC) is a new computing paradigm that enables cloud computing and information technology (IT) services to be delivered at the network’s edge. By shifting the load of cloud computing to individual local servers, MEC helps meet the requirements of ultralow latency, localized dataprocessing, and extends the potential of the Internet of Things (IoT) for end-users. However, the crosscutting nature of MEC and the multidisciplinary components necessary for its deployment have presented additional security and privacy concerns. Fortunately, artificial intelligence (AI) algorithms can cope with excessively unpredictable and complexdata, which offers a distinct advantage in dealing with sophisticated and developing adversaries in the security industry. Hence, in this article, we comprehensively provide a survey of security and privacy in MEC from the perspective of AI. On the one hand, we use European Telecommunications Standards Institute (ETSI) MEC reference architecture as our-based framework while merging the software-defined network (SDN) and network function virtualization (NFV) to better illustrate a serviceable platform of MEC. On the other hand, we focus on new security and privacy issues, as well as potential solutions from the viewpoints of AI. Finally, we comprehensively discuss the opportunities and challenges associated with applying AI to MEC security and privacy as possible future research directions.",
  "keyword": "internet security"
 },
 {
  "No": 118,
  "judul": "Secure Industrial Internet of Things Critical Infrastructure Node Design",
  "abstrak": "Integration of industrial Internet of Things (IIoT) into critical infrastructures (CIs) is aiming to improve efficiency in many crucial areas. The positive benefits of IIoT are increasing the demand for devices, but without proper security design foundations, CIs may be open to many different attack vectors. This lack of IIoT device security standards is a major concern. Due to the nature of these infrastructures and impact of compromises, CI device security is an utmost concern and must be focused on during all phases of development and manufacturing as opposed to the addition of security after functional design as seen with many current Internet of Things (IoT) devices. Many of these sensors and devices have limited resources, such as low power consumption, reduced memory storage, and reduced fixed-point processing capabilities. Traditional security protocols and solutions are often not practical on these restrained devices, and there are no current standards for characterizing device security. This paper presents a candidate security level-based architecture for low power IoT-CI devices implementing modular, low power, security primitives that are shown through simulation models and embedded software implementation to create a robustly layered defense-in-depth IoT architecture. This candidate architecture will help provide a foundation for future CI-IoT device security standardization.",
  "keyword": "internet security"
 },
 {
  "No": 119,
  "judul": "Security Testbed for Internet-of-Things Devices",
  "abstrak": "The Internet of Things (IoT) is a global ecosystem of information and communication technologies aimed at connecting any type of object (thing), at any time, and in any place, to each other and to the Internet. One of the major problems associated with the IoT is the heterogeneous nature of such deployments; this heterogeneity poses many challenges, particularly, in the areas of security and privacy. Specifically, security testing and analysis of IoT devices is considered a very complex task, as different security testing methodologies, including software and hardware security testing approaches, are needed. In this paper, we propose an innovative security testbed framework targeted at IoT devices. The security testbed is aimed at testing all types of IoT devices, with different software\/hardware configurations, by performing standard and advanced security testing. Advanced analysis processes based on machine learning algorithms are employed in the testbed in order to monitor the overall operation of the IoT device under test. The architectural design of the proposed security testbed along with a detailed description of the testbed implementation is discussed. The testbed operation is demonstrated on different IoT devices using several specific IoT testing scenarios. The results obtained demonstrate that the testbed is effective at detecting vulnerabilities and compromised IoT devices.",
  "keyword": "internet security"
 },
 {
  "No": 120,
  "judul": "A Software Defined Network-Based Security Assessment Framework for CloudIoT",
  "abstrak": "The integration of cloud and Internet of Things (IoT), named CloudIoT, has been considered as an enabler for many different applications. However, the suspicion about the security issue is one main concern that some organizations hesitate to adopt such technologies while some just ignore the security issue while integrating the CloudIoT into their business. Therefore, given the numerous choices of cloud-resource providers and IoT devices, how to evaluate their security level becomes an important issue to promote the adoption of CloudIoT as well as reduce the business security risks. To solve this problem, considering the importance of the business datasets in CloudIoT, we develop an end-to-end security assessment framework based on software defined network (SDN) to evaluate the security level for the given CloudIoT offering. Specially, in order to simplify the network controls and focus on the analysis about the datasets flow through CloudIoT, we develop a three-layer framework by integrating SDN and CloudIoT, which consists of 23 different indicators to describe its security features. Then, the interviews from industry and academic are carried out to understand the importance of these features for the overall security. Furthermore, given the relevant evidences from the CloudIoT offering, the Google Brillo and Microsoft Azure IoT Suite, our framework can effectively evaluate the security level which can help the consumers for their CloudIoT selection.",
  "keyword": "internet security"
 },
 {
  "No": 121,
  "judul": "A Survey on Standards for Interoperability and Security in the Internet of Things",
  "abstrak": "Recently, there has been an increase in studies relating to the Internet of Things (IoT) in various fields, such as smart cities, smart homes, smart factories, and healthcare. In an IoT environment, several entities, including users, devices, and information resources, are interconnected and interworked with services. Therefore, interoperability between different entities is essential to accomplish the goals of IoT systems. Further, security is another important aspect to achieve in an IoT environment to protect information resources and privacy when networking between different entities. Therefore, security and interoperability may be significant barriers in the implementation of IoT in the real world. Several studies have been conducted to investigate methods for accomplishing interoperability and security in IoT, but they address only specific problems. Hence, compatibility and generality must be considered to accomplish the goals of IoT systems. International standards provide general methods by listing protocols, rules, guidelines, and characteristics that are defined and approved by authorized organizations, helping develop and manage systems efficiently by applying these standards; interoperability and security are supported by adopting standards in development and management. Therefore, the adoption of international standards is required to overcome the barriers in IoT. Furthermore, international standard organizations are developing IoT-related standards that may provide a solution to interoperability and security. However, a study focusing on interoperability- and security-related standards has not yet been conducted. Therefore, in this paper, we focus on international standards related to interoperability and security for IoT environments. Moreover, we studied international standard organizations that have been developing standards for IoT. In this study, a systematic literature review is conducted, and international standards are analyzed. In addition, a...",
  "keyword": "internet security"
 },
 {
  "No": 122,
  "judul": "ANT-Centric IoT Security Reference Architecture—Security-by-Design for Satellite-Enabled Smart Cities",
  "abstrak": "Internet of Vehicles (IoV), a special form of Internet of Things (IoT), is an important enabler of intelligent transportation system, which is one of the most strategic applications in smart city initiatives. In order to achieve its intended functionalities, IoV requires anytime anywhere connectivity, which cannot be satisfied by traditional networking technologies. Space–air–ground-integrated network (SAGIN) is widely believed to be an ideal infrastructure for connecting IoV. In this article, we present an approach for understanding the security issues of complex IoT systems, and propose a security reference architecture for assessing security risks and addressing the security requirements. Specifically, we propose an activity-network-things (ANT)-centric security reference architecture, which is based on the three architectural perspectives in studying IoT systems, namely, device, Internet, and semantic. We discuss the limitations of existing IoT system architecture models, which are mainly evolved from the enterprise system architecture with some adaptation to the inherent features of IoT systems. Our approach can help manage the security risks by focusing on the critical activities performed in different microperimeters within an IoT system. The proposed architecture includes an organized process to understand the security requirements and select specific parameters for tailored security controls that are commensurate with organization-specific and application-specific security impacts of IoT. Our architecture is flexible enough to cater for any IoT application, and hence, can be easily applied to the case of SAGIN-enabled IoV.",
  "keyword": "internet security"
 },
 {
  "No": 123,
  "judul": "SDN-Based Dataset Transfer Security for Internet of Things",
  "abstrak": "The exponential growth of devices connected to the network has resulted in the development of new Internet of Things (IoT) applications and online services, which may have diverse and dynamic requirements on received quality. Although, the emerging software-defined networking (SDN) approach can be leveraged for the IoT environment, to dynamically achieve differentiated quality levels for different IoT tasks in very heterogeneous wireless networking scenarios, the open interfaces in SDN introduces new network attacks, which may make SDN-based IoT malfunctioned. The challenges lies in securely using SDN for IoT systems. To address this challenge, we design a SDN-based dataset transfer security model middlebox-guard (M-G). M-G aims at reducing network latency, and properly manage dataflow to ensure the network run safely. First, according to different security policies, middleboxes related to the defined secure policies, are placed at the most appropriate locations, using dataflow abstraction and a heuristic algorithm. Next, to avoid any middlebox becoming a hotspot, an offline integer linear program (ILP) pruning algorithm is proposed in M-G, to tackle switch volume constraints. In addition, an online linear program (LP) formulation is come up to handle load balance. Finally, secure mechanisms are proposed to handle different attacks. And network routing is solved flexibly, through dataflow management protocol, which are formulated via combining tunnels and tags. Experimental results demonstrate that this model can improve security performance and manage dataflow effectively in SDN-based IoT system.",
  "keyword": "internet security"
 },
 {
  "No": 124,
  "judul": "A Review of Security Standards and Frameworks for IoT-Based Smart Environments",
  "abstrak": "Assessing the security of IoT-based smart environments such as smart homes and smart cities is becoming fundamentally essential to implementing the correct control measures and effectively reducing security threats and risks brought about by deploying IoT-based smart technologies. The problem, however, is in finding security standards and assessment frameworks that best meets the security requirements as well as comprehensively assesses and exposes the security posture of IoT-based smart environments. To explore this gap, this paper presents a review of existing security standards and assessment frameworks which also includes several NIST special publications on security techniques highlighting their primary areas of focus to uncover those that can potentially address some of the security needs of IoT-based smart environments. Cumulatively a total of 80 ISO\/IEC security standards, 32 ETSI standards and 37 different conventional security assessment frameworks which included 7 NIST special publications on security techniques were reviewed. To present an all-inclusive and up-to-date state-of-the-art research, the review process considered both published security standards and assessment frameworks as well as those under development. The findings show that most of the conventional security standards and assessment frameworks do not directly address the security needs of IoT-based smart environments but have the potential to be adapted into IoT-based smart environments. With this insight into the state-of-the-art research on security standards and assessment frameworks, this study helps advance the IoT field by opening new research directions as well as opportunities for developing new security standards and assessment frameworks that will address future IoT-based smart environments security concerns. This paper also discusses open problems and challenges related to IoT-based smart environments security issues. As a new contribution, a taxonomy of challenges for IoT-base...",
  "keyword": "internet security"
 },
 {
  "No": 125,
  "judul": "An In-Depth Analysis of IoT Security Requirements, Challenges, and Their Countermeasures via Software-Defined Security",
  "abstrak": "Internet of Things (IoT) is transforming everyone's life by providing features, such as controlling and monitoring of the connected smart objects. IoT applications range over a broad spectrum of services including smart cities, homes, cars, manufacturing, e-healthcare, smart control system, transportation, wearables, farming, and much more. The adoption of these devices is growing exponentially, that has resulted in generation of a substantial amount of datasets for processing and analyzing. Thus, besides bringing ease to the human lives, these devices are susceptible to different threats and security challenges, which do not only worry the users for adopting it in sensitive environments, such as e-health, smart home, etc., but also pose hazards for the advancement of IoT in coming days. This article thoroughly reviews the threats, security requirements, challenges, and the attack vectors pertinent to IoT networks. Based on the gap analysis, a novel paradigm that combines a network-based deployment of IoT architecture through software-defined networking (SDN) is proposed. This article presents an overview of the SDN along with a thorough discussion on SDN-based IoT deployment models, i.e., centralized and decentralized. We further elaborated SDN-based IoT security solutions to present a comprehensive overview of the software-defined security (SDSec) technology. Furthermore, based on the literature, core issues are highlighted that are the main hurdles in unifying all IoT stakeholders on one platform and few findings that emphases on a network-based security solution for IoT paradigm. Finally, some future research directions of SDN-based IoT security technologies are discussed.",
  "keyword": "internet security"
 },
 {
  "No": 126,
  "judul": "Cross-Domain Security and Interoperability in Internet of Things",
  "abstrak": "The Internet of Things advancements has enabled smart city scenarios worldwide. A tight nit communication, on the one hand, is required for device management and monitoring, but on the other hand, it has raised risks in information exchange in cross-domain scenarios. Keeping in view the above issues, this article considered well-known cross-domain access control protocols, i.e., Shibboleth, xDAuth, and OAuth to ensure user’s security. And, the existing work focused specifically on their implementations without verifying if they are effective for the security scenarios. We aim to verify their claim of fulfilling the claimed security requirements or not. We did a Cyber Attack Analysis to highlight the possible security attacks in smart city scenarios. Then, the protocols are verified against those issues to critically analyze if they are the best fit for the said scenario. The Z3-solver and Satisfiability Modulo Theories Library have been in use for protocol verification purposes. Results reveal that the protocol models are functioning correctly in terms of confidentiality, availability, and integrity. Verifying these protocols in terms of other security properties is part of our future work.",
  "keyword": "internet security"
 },
 {
  "No": 127,
  "judul": "A Provably Secure and Lightweight Anonymous User Authenticated Session Key Exchange Scheme for Internet of Things Deployment",
  "abstrak": "With the ever increasing adoption rate of Internet-enabled devices [also known as Internet of Things (IoT) devices] in applications such as smart home, smart city, smart grid, and healthcare applications, we need to ensure the security and privacy ofdata and communications among these IoT devices and the underlying infrastructure. For example, an adversary can easily tamper with the information transmitted over a public channel, in the sense of modification, deletion, and fabrication ofdata-in-transit anddata-in-storage. Time-critical IoT applications such as healthcare may demand the capability to support external parties (users) to securely access IoT dataand services in real-time. This necessitates the design of a secure user authentication mechanism, which should also allow the user to achieve security and functionality features such as anonymity and un-traceability. In this paper, we propose a new lightweight anonymous user authenticated session key agreement scheme in the IoT environment. The proposed scheme uses three-factor authentication, namely a user's smart card, password, and personal biometric information. The proposed scheme does not require the storing of user specific information at the gateway node. We then demonstrate the proposed scheme's security using the broadly accepted real-or-random (ROR) model, Burrows-Abadi-Needham (BAN) logic, and automated validation of Internet security protocols and applications (AVISPAs) software simulation tool, as well as presenting an informal security analysis to demonstrate its other features. In addition, through our simulations, we demonstrate that the proposed scheme outperforms existing related user authentication schemes, in terms of its security and functionality features, and computation costs.",
  "keyword": "internet security"
 },
 {
  "No": 128,
  "judul": "Decoupling Security From Applications in CoAP-Based IoT Devices",
  "abstrak": "The complex and ever-changing Internet of Things (IoT) domain could benefit from standardization and a higher degree of autonomy between different layers: standard approaches defining the relationship between security communication software functionalities, hardware, and applications will allow a more efficient, flexible, and secure communication. To this end, techniques in which the security of IoT devices is decoupled from the applications they run can provide significant benefits and enable the development of new standardization strategies. This article presents a study of the benefits provided by IoTsafe, a security decoupling approach when used in combination with the constrained application protocol (CoAP). Whereas previous work relied on HTTP\/HTTP2 protocols, the present article is focused on the analysis of the feasibility of IoTsafe in more constrained devices in channels with high interference levels. The benefits of this technique are illustrated by means of a battery of tests to evaluate the impact of this scheme. The results show no performance penalty (taking CoAP with security as a baseline) in lossless channels, even when an overhead increment of 38% is borne. Furthermore, in lossier channels, a transfer time reduction of 36% is achieved, a figure that increases significantly if traffic compression is enabled.",
  "keyword": "internet security"
 },
 {
  "No": 129,
  "judul": "The Internet of Things for Health Care: A Comprehensive Survey",
  "abstrak": "The Internet of Things (IoT) makes smart objects the ultimate building blocks in the development of cyber-physical smart pervasive frameworks. The IoT has a variety of application domains, including health care. The IoT revolution is redesigning modern health care with promising technological, economic, and social prospects. This paper surveys advances in IoT-based health care technologies and reviews the state-of-the-art network architectures\/platforms, applications, and industrial trends in IoT-based health care solutions. In addition, this paper analyzes distinct IoT security and privacy features, including security requirements, threat models, and attack taxonomies from the health care perspective. Further, this paper proposes an intelligent collaborative security model to minimize security risk; discusses how different innovations such as bigdata, ambient intelligence, and wearables can be leveraged in a health care context; addresses various IoT and eHealth policies and regulations across the world to determine how they can facilitate economies and societies in terms of sustainable development; and provides some avenues for future research on IoT-based health care based on a set of open issues and challenges.",
  "keyword": "internet security"
 },
 {
  "No": 130,
  "judul": "Deep Learning in Security of Internet of Things",
  "abstrak": "Internet-of-Things (IoT) technology is increasingly prominent in the current stage of social development. All walks of life have begun to implement the IoT integration technology, so as to strive to promote industrial modernization, intelligence, and digitalization. In this case, how to link high-risk network activities with entities has become the primary issue for promoting industrial development. However, at this stage, the security issues in the development of the IoT technology have contradictions that are difficult to resolve. According to this situation, how to make system defense intelligent and replace manual monitoring has become the future of the development of security architecture. This article combines existing security research to explore the possibility of deep learning (DL) in upgrading the IoT security architecture, discusses how the IoT can identify and respond to cyber attacks, and how to encrypt edge datatransmission. Moreover, this article discusses security research in application fields, such as Industrial IoT, Internet of Vehicles, smart grid, smart home, and smart medical. Then, we summarized the areas that can be improved in future technological development, including sharing computing power through the edge network processing unit (NPU) central device and closely combining the environmental simulation model with the actual environment, as well as malicious code detection, intrusion detection, production safety, vulnerability detection, fault diagnosis, and blockchain technology.",
  "keyword": "internet security"
 },
 {
  "No": 131,
  "judul": "A Network Security Situation Prediction for Consumer in the Internet of Things Using Variational Mode Decomposition (VMD) and Fused CNN-BiLSTM-Attention",
  "abstrak": "Consumer in e-commerce platforms relies heavily on Internet of Things (IoT) devices, which bring forth numerous security threats. As an emerging proactive defense technology, IoT network security situation prediction has the capability to forecast the overall future network security conditions. However, the original network security situation sequences exhibit nonlinear and unstable characteristics, which diminish the direct predictive accuracy. In this paper, we propose a prediction model based on decomposition-fusion. Specifically, we propose a novel approach to compute situation values by integrating three key factors: IoT attack factors, IoT attack probabilities, and IoT threat factors. Then, we decompose the original sequence into more stable subsequences using Variational Mode Decomposition (VMD), and construct a Convolutional Neural Network (CNN)-Bidirectional Long Short-Term Memory (BiLSTM)-Attention architecture to predict these subsequences. Finally, we utilize BiLSTM to fuse the results from each subsequence calculation, generating the ultimate prediction. Experimental results underscore the significant advantages of this method in terms of stability and forecasting precision, with a fitting degree of 0.99. This method provides a more comprehensive security defense system for e-commerce platforms and IoT applications, thereby enhancing the overall security of consumer. Furthermore, it presents a novel solution for the field of network security.",
  "keyword": "internet security"
 },
 {
  "No": 132,
  "judul": "Securing the Internet of Things in the Age of Machine Learning and Software-Defined Networking",
  "abstrak": "The Internet of Things (IoT) realizes a vision where billions of interconnected devices are deployed just about everywhere, from inside our bodies to the most remote areas of the globe. As the IoT will soon pervade every aspect of our lives and will be accessible from anywhere, addressing critical IoT security threats is now more important than ever. Traditional approaches where security is applied as an afterthought and as a “patch” against known attacks are insufficient. Indeed, next-generation IoT challenges will require a new secure-by-design vision, where threats are addressed proactively and IoT devices learn to dynamically adapt to different threats. To this end, machine learning (ML) and software-defined networking (SDN) will be key to provide both reconfigurability and intelligence to the IoT devices. In this paper, we first provide a taxonomy and survey the state of the art in IoT security research, and offer a roadmap of concrete research challenges related to the application of ML and SDN to address existing and next-generation IoT security threats.",
  "keyword": "internet security"
 },
 {
  "No": 133,
  "judul": "An Architecture for IoT-Enabled Smart Transportation Security System: A Geospatial Approach",
  "abstrak": "Internet of Things (IoT) in urban transportation systems have been ubiquitously embedded into a variety of devices and transport entities. The IoT-enabled smart transportation system (STS) has thus gained growing tractions amongst scholars and practitioners. However, several IoT challenges in relation to cyber-physical security are exposed due to the heterogeneity, complexity and decentralization of the IoT network. There also exist geospatial security concerns with respect to the embeddings of 5G networks into public infrastructures that are interconnected with the transport system via IoT. To address these concerns, this article aims to apply geospatial modelling approach to propose a smart transportation security systems (STSSs). It is modelled and simulated by undertaking an experimental study in the city of Beijing, China. The simulation outcome of the proposed architecture is expected to offer a strategic guide for strategic security management of urban smart transportation.",
  "keyword": "internet security"
 },
 {
  "No": 134,
  "judul": "ISA Evaluation Framework for Security of Internet of Health Things System Using AHP-TOPSIS Methods",
  "abstrak": "Security has become a vital factor for any Internet of things network but it is of paramount importance for Internet of Health Things (IoHT). IoHT also known as Internet of Medical Things (IoMT) is integration of IoT and healthcare environment, where fragile datarelated to the patients is transmitted from IoT devices to server. During this transmission, if, any eavesdropping or intrusion occurs then it will not only lead to the serious mutilation of entire network but this datawill be handled maliciously for wrong doings as well. Therefore, a proper security is indispensable for IoHT based equipments due to exposure to different attacks. Security of IoHT has been the burning issue in last couple of years. In this regard different security models, surveys, frameworks have been presented. In this article, a proposed Identified Security Attributes (ISA) framework is presented to evaluate the security features of IoHT based device in healthcare environment. The proposed framework uses hybrid MCDM methods such as Analytical Hierarchical Process (AHP) and Technique for Order Preference by Similarity to Ideal Solution (TOPSIS). This framework works in two phase: in first phase the weights of attributes are derived by using AHP method and in second phase security assessment of alternatives is performed based upon security criteria by using TOPSIS method. The outcomes of proposed security assessment framework demonstrate that the reliable and secure alternative among alternatives is selected in IoMT system. This approach can be used as a guideline for future use in IoMT systems or any other IoT based domain. To the best of our knowledge, it is novel approach to address the security assessment of IoT and these MCDM methods have never been used before for assessment and decision making in IoHT system for security.",
  "keyword": "internet security"
 },
 {
  "No": 135,
  "judul": "Embedding Blockchain Technology Into IoT for Security: A Survey",
  "abstrak": "In recent years, the Internet of Things (IoT) has made great progress. The interconnection between IoT and the Internet enables real-time information processing and transaction implementation through heterogeneous intelligent devices. But the security, the privacy, and the reliability of IoT are key challenges that limit its development. The features of the blockchain, such as decentralization, consensus mechanism, dataencryption, and smart contracts, are suitable for building distributed IoT systems to prevent potential attacks and to reduce transaction costs. As a decentralized and transparent database platform, blockchain has the potential to raise the performance of IoT security to a higher level. This article systematically analyzes state of the art of IoT security based on the blockchain, paying special attention to the security features, issues, technologies, approaches, and related scenarios in blockchain-embedded IoT. The integration and interoperation of blockchain and IoT is an important and foreseeable development in the computational communication system.",
  "keyword": "internet security"
 },
 {
  "No": 136,
  "judul": "Internet of Things (IoT): A Review of Its Enabling Technologies in Healthcare Applications, Standards Protocols, Security, and Market Opportunities",
  "abstrak": "The Internet of Things (IoT) is a methodology or a system that encompasses real-world things to interact and communicate with each other with the assistance of networking technologies. This article describes surveys on advances in IoT-based healthcare methods and reviews the state-of-the-art technologies in detail. Moreover, this review classifies an existing IoT-based healthcare network and represents a summary of all perspective networks. IoT healthcare protocols are analyzed in this context and provide a broad discussion on it. It also initiates a comprehensive survey on IoT healthcare applications and services. Extensive insights into IoT healthcare security, its requirements, challenges, and privacy issues are visualized in IoT surrounding healthcare. In this review, we analyze security and privacy features consisting of dataprotection, network architecture, Quality of Services (QoS), app development, and continuous monitoring of healthcare that are facing difficulties in many IoT-based healthcare architectures. To mitigate the security problems, an IoT-based security architectural model has been proposed in this review. Furthermore, this review discloses the market opportunity that will enhance the IoT healthcare market development. To conduct the survey, we searched through established journal and conference databases using specific keywords to find scholarly works. We applied a filtering mechanism to collect only papers that were relevant to our research works. The selected papers were then examined carefully to understand their contributions\/research focus. Eventually, the paper reviews were analyzed to identify any existing research gaps and untouched areas of research and to discover possible features for sustainable IoT healthcare development.",
  "keyword": "internet security"
 },
 {
  "No": 137,
  "judul": "Identification and Resolution for Industrial Internet: Architecture and Key Technology",
  "abstrak": "As a key component of Industrial Internet, identification and resolution has been considered as a promising technology to realize the interconnections among physical and virtual entities and to promote interoperability of digital objects, which improves the industrial efficiency and transparency of the supply chain. In this article, we focus on the identification and resolution design in the Industrial Internet. A general identification and resolution architecture is proposed to guide how to develop and improve technologies and schemes in terms of the joint requirements of service, role, function, implement, and security in Industrial Internet. Based on the proposed architecture, three key technologies are studied. Specifically, an identifiable digital object (IDO) model is constructed to enable the different Industrial Internet platforms to manage and interact datauniformly. Then, a hybrid structure-based identification and resolution system is designed, based on which identification registration and resolution procedures are proposed. Furthermore, a trustable system based on blockchain is deployed to guarantee the datacredibility. Finally, extensive practical experiments validate the effectiveness of the proposed technologies. The proposed architecture and technologies have been used in the practical construction and deployment of national top-level nodes and secondary-level nodes to sustain the identification and resolution services for the Industrial Internet in China.",
  "keyword": "internet security"
 },
 {
  "No": 138,
  "judul": "A Network-Aware Internet-Wide Scan for Security Maximization of IPv6-Enabled WLAN IoT Devices",
  "abstrak": "Despite unprecedented advancements, wireless local area network (WLAN) technologies for the Internet of Things (IoT), such as IEEE 802.11ah (i.e., WiFi-HaLow), are prone to serious security threats, owing to their constrained computational and memory resources, which limit the use of heavyweight intrusion protection and security protocols. To address this problem, security administrators (sec-admins) must perform regular and comprehensive vulnerability assessments of IoT devices. An Internet-wide port scan (IWPS) is the initial step. However, the medium access control mechanism of IEEE 802.11ah, designed specifically for heterogeneous IoT traffic and low-power operations, can degrade network performance in the case of traditional port-scan traffic. Moreover, Internet-security (IPSec) protocol support is mandatory for IPv6-enabled IoT devices to ensure dataconfidentiality, integrity, and availability. Although the objective of a port scan is to improve IoT security, the resultant network performance can adversely affect IPSec services. Therefore, in this study, we optimize the IWPS to maximize the IoT security over IEEE 802.11ah WLAN. To this end, we propose novel mathematical models to evaluate IoT security based on port-scan network performance and IPsec services, which derives an optimal scan rate for sec-admins. The effectiveness of the proposed framework is verified by comprehensive numerical analysis, which shows that our approach minimizes the risk to IoT devices while probing them at an optimal scan rate.",
  "keyword": "internet security"
 },
 {
  "No": 139,
  "judul": "Security Analysis of IoT Devices by Using Mobile Computing: A Systematic Literature Review",
  "abstrak": "Internet of Things (IoT) devices are operating in various domains like healthcare environment, smart cities, smart homes, transportation, and smart grid system. These devices transmit a bulk of datathrough various sensors, actuators, transceivers, or other wearable devices. Datain the IoT environment is susceptible to many threats, attacks, and risks. Therefore, a robust security mechanism is indispensable to cope with attacks, vulnerabilities, security, and privacy challenges related to IoT. In this research, a systematic literature review has been conducted to analyze the security of IoT devices and to provide the countermeasures in response to security problems and challenges by using mobile computing. A comprehensive and in-depth security analysis of IoT devices has been made in light of mobile computing, which is a novel approach. Mobile computing's technological infrastructures such as smartphones, services, policies, strategies, and applications are employed to tackle and mitigate these potential security threats. In this paper, the security challenges and problems of IoT devices are identified by a systematic literature review. Then, mobile computing has been used to address these challenges by providing potential security measures and solutions. Hardware and software-based solutions furnished by mobile computing towards the IoT security challenges have been elaborated. To the best of our knowledge, this is the first attempt to analyze the security issues and challenges of IoT in light of mobile computing and it will open a gateway towards future research.",
  "keyword": "internet security"
 },
 {
  "No": 140,
  "judul": "Demystifying IoT Security: An Exhaustive Survey on IoT Vulnerabilities and a First Empirical Look on Internet-Scale IoT Exploitations",
  "abstrak": "The security issue impacting the Internet-of-Things (IoT) paradigm has recently attracted significant attention from the research community. To this end, several surveys were put forward addressing various IoT-centric topics, including intrusion detection systems, threat modeling, and emerging technologies. In contrast, in this paper, we exclusively focus on the ever-evolving IoT vulnerabilities. In this context, we initially provide a comprehensive classification of state-of-the-art surveys, which address various dimensions of the IoT paradigm. This aims at facilitating IoT research endeavors by amalgamating, comparing, and contrasting dispersed research contributions. Subsequently, we provide a unique taxonomy, which sheds the light on IoT vulnerabilities, their attack vectors, impacts on numerous security objectives, attacks which exploit such vulnerabilities, corresponding remediation methodologies and currently offered operational cyber security capabilities to infer and monitor such weaknesses. This aims at providing the reader with a multidimensional research perspective related to IoT vulnerabilities, including their technical details and consequences, which is postulated to be leveraged for remediation objectives. Additionally, motivated by the lack of empirical (and malicious) dataset related to the IoT paradigm, this paper also presents a first look on Internet-scale IoT exploitations by drawing upon more than 1.2 GB of macroscopic, passive measurements' dataset. This aims at practically highlighting the severity of the IoT problem, while providing operational situational awareness capabilities, which undoubtedly would aid in the mitigation task, at large. Insightful findings, inferences and outcomes in addition to open challenges and research problems are also disclosed in this paper, which we hope would pave the way for future research endeavors addressing theoretical and empirical aspects related to the imperative topic of IoT security.",
  "keyword": "internet security"
 },
 {
  "No": 141,
  "judul": "Ensuring the Security and Performance of IoT Communication by Improving Encryption and Decryption With the Lightweight Cipher uBlock",
  "abstrak": "Internet of Things (IoT) devices expect a secure communication method or protocol with communication performance that matches the network bandwidth in a public network. With the advancement of network bandwidths, such as through wireless technology 5G, encryption and decryption throughput can be the performance bottleneck of IoT communication security, as IoT devices have limited computing power. As a result, it is a challenge for IoT devices to obtain the required level of security and communication performance. In this article, we improve the encryption and decryption performance with a lightweight cipher algorithm called uBlock for IoT devices. By using sequential circuits, the encryption and decryption throughput can reach the 1 Gb\/s level with 90 nm technology in our simulation environment. We can expect a more than two fold improvement in the performance with better nanotechnology from hardware vendors. Our proposed solution can achieve the required level of security and communication performance with a little power consumption.",
  "keyword": "internet security"
 },
 {
  "No": 142,
  "judul": "The Effect of IoT New Features on Security and Privacy: New Threats, Existing Solutions, and Challenges Yet to Be Solved",
  "abstrak": "Internet of Things (IoT) is an increasingly popular technology that enables physical devices, vehicles, home appliances, etc., to communicate and even inter operate with one another. It has been widely used in industrial production and social applications including smart home, healthcare, and industrial automation. While bringing unprecedented convenience, accessibility, and efficiency, IoT has caused acute security and privacy threats in recent years. There are increasing research works to ease these threats, but many problems remain open. To better understand the essential reasons of new IoT threats and the challenges in current research, this survey first proposes the concept of “IoT features.” Then, we discuss the security and privacy effects of eight IoT features including the threats they cause, existing solutions to threats and research challenges yet to be solved. To help researchers follow the up-to-date works in this field, this paper finally illustrates the developing trend of IoT security research and reveals how IoT features affect existing security research by investigating most existing research works related to IoT security from 2013 to 2017.",
  "keyword": "internet security"
 },
 {
  "No": 143,
  "judul": "Physical-Layer Security in the Internet of Things: Sensing and Communication Confidentiality Under Resource Constraints",
  "abstrak": "The Internet of Things (IoT) will feature pervasive sensing and control capabilities via a massive deployment of machine-type communication (MTC) devices. The limited hardware, low-complexity, and severe energy constraints of MTC devices present unique communication and security challenges. As a result, robust physical-layer security methods that can supplement or even replace lightweight cryptographic protocols are appealing solutions. In this paper, we present an overview of low-complexity physical-layer security schemes that are suitable for the IoT. A local IoT deployment is modeled as a composition of multiple sensor and datasubnetworks, with uplink communications from sensors to controllers, and downlink communications from controllers to actuators. The state of the art in physical-layer security for sensor networks is reviewed, followed by an overview of communication network security techniques. We then pinpoint the most energy-efficient and low-complexity security techniques that are best suited for IoT sensing applications. This is followed by a discussion of candidate low-complexity schemes for communication security, such as on-off switching and space-time block codes. The paper concludes by discussing open research issues and avenues for further work, especially the need for a theoretically well-founded and holistic approach for incorporating complexity constraints in physical-layer security designs.",
  "keyword": "internet security"
 },
 {
  "No": 144,
  "judul": "A Security Awareness and Protection System for 5G Smart Healthcare Based on Zero-Trust Architecture",
  "abstrak": "The key features of 5G network (i.e., high bandwidth, low latency, and high concurrency) along with the capability of supporting big dataplatforms with high mobility make it valuable in coping with emerging medical needs, such as COVID-19 and future healthcare challenges. However, enforcing the security aspect of a 5G-based smart healthcare system that hosts critical dataand services is becoming more urgent and critical. Passive security mechanisms (e.g., dataset encryption and isolation) used in legacy medical platforms cannot provide sufficient protection for a healthcare system that is deployed in a distributed manner and fail to meet the need for dataset\/service sharing across “cloud-edge-terminal” in the 5G era. In this article, we propose a security awareness and protection system that leverages zero-trust architecture for a 5G-based smart medical platform. Driven by the four key dimensions of 5G smart healthcare including “subject” (i.e., users, terminals, and applications), “object” (i.e., dataset, platforms, and services), “behavior,” and “environment,” our system constructs trustable dynamic access control models and achieves real-time network security situational awareness, continuous identity authentication, analysis of access behavior, and fine-grained access control. The proposed security system is implemented and tested thoroughly at industrial-grade, which proves that it satisfies the needs of active defense and end-to-end security enforcement ofdata, users, and services involved in a 5G-based smart medical system.",
  "keyword": "internet security"
 },
 {
  "No": 145,
  "judul": "Security, Privacy and Trust for Smart Mobile- Internet of Things (M-IoT): A Survey",
  "abstrak": "With an enormous range of applications, the Internet of Things (IoT) has magnetized industries and academicians from everywhere. IoT facilitates operations through ubiquitous connectivity by providing Internet access to all the devices with computing capabilities. With the evolution of wireless infrastructure, the focus from simple IoT has been shifted to smart, connected and mobile IoT (M-IoT) devices and platforms, which can enable low-complexity, low-cost and efficient computing through sensors, machines, and even crowdsourcing. All these devices can be grouped under a common term of M-IoT. Even though the positive impact on applications has been tremendous, security, privacy and trust are still the major concerns for such networks and insufficient enforcement of these requirements introduces non-negligible threats to M-IoT devices and platforms. Thus, it is important to understand the range of solutions which are available for providing a secure, privacy-compliant, and trustworthy mechanism for M-IoT. There is no direct survey available, which focuses on security, privacy, trust, secure protocols, physical layer security and handover protections in M-IoT. This paper covers such requisites and presents comparisons of state-the-art solutions for IoT which are applicable to security, privacy, and trust in smart and connected M-IoT networks. Apart from these, various challenges, applications, advantages, technologies, standards, open issues, and roadmap for security, privacy and trust are also discussed in this paper.",
  "keyword": "internet security"
 },
 {
  "No": 146,
  "judul": "Amassing the Security: An ECC-Based Authentication Scheme for Internet of Drones",
  "abstrak": "The continuous innovation and progression in hardware, software and communication technologies helped the expansion and accelerated growth in Internet of Things based drone networks (IoD), for the devices, applications and people to communicate and share dataset. IoD can enhance comfort in many applications including, daily life, commercial, and military\/rescue operations in smart cities. However, this growth in infrastructure smartness is also subject to new security threats and the countermeasures require new customized solutions for IoD. Many schemes to secure IoD environments are proposed recently; however, some of those were proved as insecure and some degrades the efficiency. In this article, using elliptic curve cryptography, we proposed a new authentication scheme to secure the communication between a user and a drone flying in some specific flying zone. The security of the proposed scheme is solicited using formal Random oracle method along with a brief discussion on security aspects provided by proposed scheme. Finally, the comparisons with some related and latest schemes is illustrated.",
  "keyword": "internet security"
 },
 {
  "No": 147,
  "judul": "Hierarchical Naming Scheme in Named Networking for Internet of Things: A Review and Future Security Challenges",
  "abstrak": "The proliferation of connected devices in the Internet of Things (IoT) presents a connectivity challenge. The future internet will require a paradigm shift in which content is evaluated on the basis of “What” it is rather than “Where” it originated. ICN’s goal is to provide the benefits of name-based content addressing in order to facilitate scalable content distribution, security, mobility, and trust. NDN is a new internet architecture that evolved from Content-Centric Networking (CCN). NDN is viewed as a solution to the IoT’s challenges, as well as a way to transcend the IP paradigm. With IoT systems that had a number of challenging characteristics to satisfy, including heterogeneous devices, resource constraints, and energy efficiency. Due to the fact that NDN native features deliver via hierarchically structured names, it offer promising solutions for current research integrating NDN into IoT. The review discusses the significance of naming, its influence, and security factor. Additionally, research challenges in the areas of naming and security will be discussed. The primary objective of this review is to give a new facelift to a new integrating naming convention for NDN.",
  "keyword": "internet security"
 },
 {
  "No": 148,
  "judul": "Evaluating Critical Security Issues of the IoT World: Present and Future Challenges",
  "abstrak": "Social Internet of Things (SIoT) is a new paradigm where Internet of Things (IoT) merges with social networks, allowing people and devices to interact, and facilitating information sharing. However, security and privacy issues are a great challenge for IoT but they are also enabling factors to create a “trust ecosystem.” In fact, the intrinsic vulnerabilities of IoT devices, with limited resources and heterogeneous technologies, together with the lack of specifically designed IoT standards, represent a fertile ground for the expansion of specific cyber threats. In this paper, we try to bring order on the IoT security panorama providing a taxonomic analysis from the perspective of the three main key layers of the IoT system model: 1) perception; 2) transportation; and 3) application levels. As a result of the analysis, we will highlight the most critical issues with the aim of guiding future research directions.",
  "keyword": "internet security"
 },
 {
  "No": 149,
  "judul": "Differential Game Approach for Attack-Defense Strategy Analysis in Internet of Things Networks",
  "abstrak": "Internet of Things (IoT) is vulnerable to various cyber attacks due to the massive deployment of IoT devices and the openness of wireless environments. In this article, taking IoT devices as the network resources competed between an attacker and a defender, we study the modeling and analysis of network resource competition in an attack-defense game. The attacker and defender inject different competition strength in each IoT device as their strategies. As a result, the security state of each IoT device will change, which is captured by differential equations. To study the interaction between the attacker and defender and the evolution of the system security states, a zero-sum differential game is formulated by modeling the competition of IoT devices. To achieve the equilibrium of the formulated differential game, optimal control theory is employed to solve the optimization problems of players. Further, a Gauss–Seidel-like implicit finite-difference method is utilized to obtain the saddle point strategy. Finally, numerical results are provided to demonstrate the evolution of network resource competition between the attacker and defender. The results show that our formulated model can effectively and accurately characterize the evolution of the system security states with strategic interactions between the attacker and defender.",
  "keyword": "internet security"
 },
 {
  "No": 150,
  "judul": "A Survey of Security Challenges, Attacks Taxonomy and Advanced Countermeasures in the Internet of Things",
  "abstrak": "Internet of Things (IoT) facilitates the integration between objects and different sensors to provide communication among them without human intervention. However, the extensive demand for IoT and its various applications has continued to grow, coupled with the need to achieve foolproof security requirements. IoT produces a vast amount of dataunder several constraints such as low processor, power, and memory. These constraints, along with the invaluabledata produced by IoT devices, make IoT vulnerable to various security attacks. This paper presents an overview of IoT, its well-known system architecture, enabling technologies, and discusses security challenges and goals. Furthermore, we analyze security vulnerabilities and provide state-of-the-art security taxonomy. The taxonomy of the most relevant and current IoT security attacks is presented for application, network, and physical layers. While most other surveys studied one of the areas of security measures, this study considers and reports on the most advanced security countermeasures within the areas of autonomic, encryption, and learning-based approaches. Additionally, we uncover security challenges that may be met by the research community regarding security implementation in heterogeneous IoT environment. Finally, we provide different visions about possible security solutions and future research directions.",
  "keyword": "internet security"
 },
 {
  "No": 151,
  "judul": "AKM-IoV: Authenticated Key Management Protocol in Fog Computing-Based Internet of Vehicles Deployment",
  "abstrak": "Internet of Vehicles (IoV) is an intelligent application of Internet of Things (IoT) in smart transportation that takes intelligent commitments to the passengers to improve traffic safety and efficiency, and generate a more enjoyable driving and riding environment. Fog cloud-based IoV is another variant of mobile cloud computing where vehicular cloud and Internet can co-operate in more effective way in IoV. However, more increasing dependence on wireless communication, control, and computing technology makes IoV more dangerous to prospective attacks. For secure communication among vehicles, road-side units, fog and cloud servers, we design a secure authenticated key management protocol in fog computing-based IoV deployment, called AKM-IoV. In the designed AKM-IoV, after mutual authentication between communicating entities in IoV they establish session keys for secure communications. AKM-IoV is tested for its security analysis using the formal security analysis under the widely accepted real-or-random (ROR) model, informal, and formal security verification using the broadly accepted automated validation of Internet security protocols and applications (AVISPAs) tool. The practical demonstration of AKM-IoV is shown using the NS2 simulation. In addition, a detailed comparative study is conducted to show the efficiency and functionality and security features supported by AKM-IoV as compared to other existing recent protocols.",
  "keyword": "internet security"
 },
 {
  "No": 152,
  "judul": "Beamforming Design for Physical Layer Security in a Two-Way Cognitive Radio IoT Network With SWIPT",
  "abstrak": "In this article, we study the secure beamforming design for a two-way cognitive radio (CR) Internet of Things (IoT) network aided with the simultaneous wireless information and power transfer (SWIPT). Located at the center of secondary network, the IoT controller helps to provide relay assistance and cooperative physical layer security (PLS) for two primary users (PUs) against an eavesdropper, while transmitting information and power to the other IoT devices (IoDs) with primary spectrum. To enhance the information security, we aim to maximize the secrecy sum rate (SSR) for PUs by jointly designing the beamforming matrix and vectors at the central controller. To efficiently solve the nonconvex problem, we first propose the branch-reduce-and-bound (BRB)-based algorithm to obtain an upper bound for the SSR and offer a feasible solution by Gaussian randomization, which demands two-level iteration and thus has high complexity. To strike a balance between the complexity and the performance, we then propose iterative algorithm based on constrained-convex-concave programming (CCCP) and a zero forcing (ZF)-based noniterative algorithm, the latter of which with lowest complexity is suitable for the central controller with limited-power supply. The simulation results are provided to demonstrate the effectiveness of our proposed optimization algorithms in comparison to the traditional schemes.",
  "keyword": "internet security"
 },
 {
  "No": 153,
  "judul": "Comments on “AKM-IoV: Authenticated Key Management Protocol in Fog Computing-Based Internet of Vehicles Deployment”",
  "abstrak": "Internet of Vehicles (IoV) has become an intelligent application of Internet of Things (IoT) in smart transportation. IoV takes intelligent commitments to the passengers for improving the efficiency and safety of traffic. It also creates an enjoyable riding atmosphere. Another variation of mobile cloud computing is fog cloud-based IoV, where Internet and vehicular cloud can co-operate in an effective way in IoV. Moreover, IoV is becoming dangerous to various attacks due to the increasing dependency of wireless communication and computing technologies. Recently, Wazid \net al.\n proposed “AKM-IoV: Authenticated Key Management Protocol in Fog Computing-Based IoV Deployment” to make the communication secure between vehicles, fog servers, roadside units (RSUs), and cloud servers. In this comment, we cryptanalyzed the protocol of Wazid \net al.\n and found it vulnerable to vehicle impersonation, fog server impersonation, RSU impersonation, and cloud server impersonation attacks.",
  "keyword": "internet security"
 },
 {
  "No": 154,
  "judul": "ACKE: Asymmetric Computing Key Exchange Protocol for IoT Environments",
  "abstrak": "Most of the cryptographic protocols currently in use are not appropriate for Internet of Things (IoT) environments because of their huge computing overhead, especially for terminal-embedded devices with resource constrained. Moreover, the computing resources in IoT environments are frequently asymmetric, that is to say, the computing power of the terminal devices is always weak and the server side is relatively stronger. In order to guarantee the security in the scenario, we present the asymmetric computing cryptosystem. Take the key exchange protocol as an example, we show how to construct ACKE, an asymmetric computing key exchange protocol, by employing the Diffie–Hellman key exchange protocol and the Subset Product problem (NP-complete) in this article. The underlying idea this construction is to significantly decrease the computational complexity of one party, and allow for a suitable rise in the computational complexity of another party. Our proposed protocol is implemented on an IoT simulation platform composed of a notebook PC of Intel i5-5200U 2.2 GHz\/8G and a smart watch of MTK6062 1.2 GHz\/512M. The experimental results show that this work will assist in making the Diffie–Hellman type protocol suitable for practical applications in IoT environments.",
  "keyword": "internet security"
 },
 {
  "No": 155,
  "judul": "An Improved Lightweight PUF–PKI Digital Certificate Authentication Scheme for the Internet of Things",
  "abstrak": "Prosanta and Biplab presented a lightweight two-factor authentication scheme for the Internet of Things (IoT) devices based on the physical unclonable function (PUF). Their presented scheme was based on the fuzzy extractor and analyzed various security reasonings, such as mutual authentication, session key agreement, privacy and protection against impersonation, message tampering, and replay attacks. In this article, we present sufficient security analysis to demonstrate that the scheme has various security and privacy issues in its setup and authentication phases. We propose a highly secure and robust authentication protocol based on a public key infrastructure (PKI) digital certificate based on two certificate authorities (CAs) for cloud IoT systems. The proposed authentication method is verified and validated using the Tamarin prover and supported with a detailed security and performance analysis discussion. The scheme security and privacy attributes are compared with other IoT authentication schemes. The analysis has proved that the proposed authentication scheme is more secure and highly reliable as compared to the Prosanta and Biplab authentication scheme.",
  "keyword": "internet security"
 },
 {
  "No": 156,
  "judul": "Enhancing Information Security via Physical Layer Approaches in Heterogeneous IoT With Multiple Access Mobile Edge Computing in Smart City",
  "abstrak": "Heterogeneous Internet of Things (IoT) and multi-access mobile edge computing (MA-MEC) are believed as supporting technologies for building a smart city. The advancement and flourish of IoT are facilitating the entry of human society into the Internet of Everything era, which lay the foundation of the smart city. To address the conflict between computation capability and low-cost mobile devices in IoT, the MA-MEC is available for supporting the resource-limited and computation-sensitive services and applications by computation offloading and distributed content delivery\/caching. However, deploying cloud computing capability within the radio access network may face serious security threats, which stem from not only the existing technologies and networks but also the MA-MEC-based IoT itself. Therefore, in this paper, the solutions to address the security threats are investigated from physical layer perspectives, since physical layer security technologies have the advantages of achieving perfect secrecy, low-computational complexity, and resource consumption, and good adaptation for channel changes. Specifically, we investigate the secure wiretap coding, resource allocation, signal processing, and multi-node cooperation, along with physical layer key generation and authentication, to cope with the emerging security challenges. Finally, the paper is concluded with some possible future research directions.",
  "keyword": "internet security"
 },
 {
  "No": 157,
  "judul": "A Survey of Machine and Deep Learning Methods for Internet of Things (IoT) Security",
  "abstrak": "The Internet of Things (IoT) integrates billions of smart devices that can communicate with one another with minimal human intervention. IoT is one of the fastest developing fields in the history of computing, with an estimated 50 billion devices by the end of 2020. However, the crosscutting nature of IoT systems and the multidisciplinary components involved in the deployment of such systems have introduced new security challenges. Implementing security measures, such as encryption, authentication, access control, network and application security for IoT devices and their inherent vulnerabilities is ineffective. Therefore, existing security methods should be enhanced to effectively secure the IoT ecosystem. Machine learning and deep learning (ML\/DL) have advanced considerably over the last few years, and machine intelligence has transitioned from laboratory novelty to practical machinery in several important applications. Consequently, ML\/DL methods are important in transforming the security of IoT systems from merely facilitating secure communication between devices to security-based intelligence systems. The goal of this work is to provide a comprehensive survey of ML methods and recent advances in DL methods that can be used to develop enhanced security methods for IoT systems. IoT security threats that are related to inherent or newly introduced threats are presented, and various potential IoT system attack surfaces and the possible threats related to each surface are discussed. We then thoroughly review ML\/DL methods for IoT security and present the opportunities, advantages and shortcomings of each method. We discuss the opportunities and challenges involved in applying ML\/DL to IoT security. These opportunities and challenges can serve as potential future research directions.",
  "keyword": "internet security"
 },
 {
  "No": 158,
  "judul": "ECC-Based Authenticated Key Agreement Protocol for Industrial Control System",
  "abstrak": "Nowadays, Industrial Internet of Things (IIoT) technology has made a great progress and the industrial control systems (ICSs) have been used extensively, which has brought more and more serious information security threats to the ICS at the same time. The authenticated key agreement (AKA) protocol is a common method to ensure the communication security. This work proposes a lightweight AKA protocol based on the elliptic curve cryptography (ECC) algorithm to adapt to the resource-constrained environment. We only employ hash operation, XOR operation, and ECC algorithm to encrypt the datain the authentication and key agreement phase, and avoid involving the register center while proceeding the key agreement, to give consideration to both performance and security. The security analyses indicate that our protocol can meet nine critical security requirements, more than all of the existing protocols, and the performance analysis carried out indicates that our protocol has less computational and communication overheads in contrast to other corelative protocols.",
  "keyword": "internet security"
 },
 {
  "No": 159,
  "judul": "IoT: Internet of Threats? A Survey of Practical Security Vulnerabilities in Real IoT Devices",
  "abstrak": "The Internet of Things (IoT) is rapidly spreading, reaching a multitude of different domains, including personal health care, environmental monitoring, home automation, smart mobility, and Industry 4.0. As a consequence, more and more IoT devices are being deployed in a variety of public and private environments, progressively becoming common objects of everyday life. It is hence apparent that, in such a scenario, cybersecurity becomes critical to avoid threats like leakage of sensible information, denial of service (DoS) attacks, unauthorized network access, and so on. Unfortunately, many low-end IoT commercial products do not usually support strong security mechanisms, and can hence be target of-or even means for-a number of security attacks. The aim of this article is to provide a broad overview of the security risks in the IoT sector and to discuss some possible counteractions. To this end, after a general introduction to security in the IoT domain, we discuss the specific security mechanisms adopted by the most popular IoT communication protocols. Then, we report and analyze some of the attacks against real IoT devices reported in the literature, in order to point out the current security weaknesses of commercial IoT solutions and remark the importance of considering security as an integral part in the design of IoT systems. We conclude this article with a reasoned comparison of the considered IoT technologies with respect to a set of qualifying security attributes, namely integrity, anonymity, confidentiality, privacy, access control, authentication, authorization, resilience, self organization.",
  "keyword": "internet security"
 },
 {
  "No": 160,
  "judul": "Data Security in Healthcare Industrial Internet of Things With Blockchain",
  "abstrak": "The industrial Internet of Things (IIoT) has gained more attention because of the self-governing nature of system configuration with interoperable application connectivity. In the E-healthcare environment, it cooperates with medical sensors to capture, examine, analyze, preserve, and document day-to-day transactions of patients in real-time. The integration of E-healthcare and IIoT provides an analytical platform to handle a large amount of data with a low cost of cloud-enabling scalable storage. Throughout the transformation, patients’ personal information is at risk while exchanging records over centralized server-based systems. Thus, the rate of node connectivity, failure of parallel data sharing, and deliverance-related issues increase. In this article, we present solutions in three folds. First, the article proposes a novel and secure architecture for E-healthcare data security using a blockchain-distributed ledger technology named BHIIoT. Second, the transformation in the lifecycle of medical wireless sensor networks (WSNs) for data management and optimization with a distributed layered hierarchy is developed, which enhances the network resources and increases trust in the blockchain-enabled peer-to-peer (P2P) environment. Third, the proposed BHIIoT uses the NuCypher threshold re-encryption mechanism for data encryption and protects shared resources in the form of blocks preserved in a blockchain immutable storage. For instance, chain codes are deployed to automate authentication, logging, index information deliverance, and trace transactions to resist illegal activities in the E-healthcare distributed application. The customized lightweight blockchain multi-proof-of-work (PoW) and multi-proof-of-stake (PoS) are designed with a digital signature for improving the consumption of resources and reducing the load of storage while the transaction process of E-healthcare IIoT is scheduled.",
  "keyword": "internet security"
 },
 {
  "No": 161,
  "judul": "EU Cybersecurity Act and IoT Certification: Landscape, Perspective and a Proposed Template Scheme",
  "abstrak": "The vulnerabilities in deployed IoT devices are a threat to critical infrastructure and user privacy. There is ample ongoing research and efforts to produce devices that are secure-by-design. However, these efforts are still far from translation into actual deployments. To address this, worldwide efforts towards IoT device and software certification have accelerated as a potential solution, including UK’s IoT assurance program, EU Cybersecurity Act and the US executive order 14028. In EU, the Cybersecurity Act was launched in 2019 which initiated the European cybersecurity certification framework for Internet and Communications Technology (ICT). The heterogeneity of the IoT landscape with devices ranging from industrial to consumer, makes it challenging to incorporate IoT devices in the certification framework or introduce a European cybersecurity certification scheme solely for IoT. This paper analyses the cybersecurity certification prospects for IoT devices and also places article 54 of the EU Cybersecurity Act in an international perspective. We conducted a comparative study of existing IoT certification schemes to identify potential gaps and extract requirements of a candidate IoT device security certification scheme. We also propose an approach that can be used as a template to instantiate an EU cybersecurity certification scheme for IoT devices. In the proposed template, we identify IoT-critical elements from the article 54 of the Cybersecurity Act. We also evaluate the proposed template using the ENISA qualification system for cybersecurity certification schemes and show its qualification on all criteria.",
  "keyword": "internet security"
 },
 {
  "No": 162,
  "judul": "Improving Security Architecture of Internet of Medical Things: A Systematic Literature Review",
  "abstrak": "Ever since its emergence, the concept internet of things (IoT) has been applied in many fields. In the area of medical sciences, a new concept “Internet of Medical Things” (IoMT) has been explored. IoMT establishes a connection between humans & machines and serves both of them. It has been expected that, by 2025, services of IoMT would reach the entire world. IoMT has covered a wide scope pertaining to health but unfortunately been facing many security challenges. Healthcare systems consist of sensitive and significantdata, which is unorganized and noisy and needs additional power to be calculated for effective analysis & workable results. This dataset is worked upon for the purpose of making critical decisions. Therefore, it has become the main target of Cyber Criminals. The need of robust security and privacy (S&P) is gradually increasing as more and more devices are getting connected to the IoMT. The S&P of the IoMT has now become a great challenge, considering the utmost significance and vulnerability of the dataset in the healthcare industry. Lack of sufficient S&P in IoMT devices keeps the patient’ privacy at high stake. This research is intended to propose a Security Model to cope with these Security threats, attacks, issues and challenges. The proposed model has been developed by thoroughly investigating all the major security models through a detailed systematic literature review. The SLR has been conducted to explore all the security threats, security attacks, security issues and security challenges. Extensive meta-analysis has been performed for each of the defined category in order to prioritize these risks. After analyzing these risks, a comprehensive security model has been proposed. The interface has been developed in Python which is well structured, user friendly and easy to implement. The developed module not only identify and prioritize the risks but also automatically control different level of threats. The developed system also contain user intimation...",
  "keyword": "internet security"
 },
 {
  "No": 163,
  "judul": "A Novel Architectural Framework on IoT Ecosystem, Security Aspects and Mechanisms: A Comprehensive Survey",
  "abstrak": "For the past few years, the Internet of Things (IoT) technology continues to not only gain popularity and importance, but also witnesses the true realization of everything being smart. With the advent of the concept of smart everything, IoT has emerged as an area of great potential and incredible growth. An IoT ecosystem centers around innovation perspective which is considered as its fundamental core. Accordingly, IoT enabling technologies such as hardware and software platforms as well as standards become the core of the IoT ecosystem. However, any large-scale technological integration such as the IoT development poses the challenge to ensure secure datatransmission. Perhaps, the ubiquitous and the resource-constrained nature of IoT devices and the sensitive and privatedata being generated by IoT systems make them highly vulnerable to physical and cyber threats. In this paper, we re-define an IoT ecosystem from the core technologies view point. We propose a modified three layer IoT architecture by dividing the perception layer into elementary blocks based on their attributed functions. Enabling technologies, attacks and security countermeasures are classified under each layer of the proposed architecture. Additionally, to give the readers a broader perspective of the research area, we discuss the role of various state-of-the-art emerging technologies in the IoT security. We present the security aspects of the most prominent standards and other recently developed technologies for IoT which might have the potential to form the yet undefined IoT architecture. Among the technologies presented in this article, we give a special interest to one recent technology in IoT domain. This technology is named IQRF that stands for Intelligent Connectivity using Radio Frequency. It is an emerging technology for wireless packet-oriented communication that operates in sub-GHz ISM band (868 MHz) and which is intended for general use where wireless connectivity is needed, either in...",
  "keyword": "internet security"
 },
 {
  "No": 164,
  "judul": "General Theory of Security and a Study Case in Internet of Things",
  "abstrak": "This paper studies the problems of “security meridian-collateral” and “security confrontation” systematically and refreshes the traditional concept of security. On the one hand, based on the model of “meridians” in Chinese traditional medicine for the first time, this paper proves the following results strictly by the probability method. There is a complete “meridian-collateral diagram” in Internet of things and any finite system, so that any “sickness” of the system can be cured effectively. On the other hand, this paper studies the network attack and defense from the perspective of the information theory. Through mathematical modeling, based on the famous Shannon's coding theorem, the research on the ability problem of attacker and defender is transformed into the research on the channels of attacker and defender. From the perspective of the channel capacity in information theory for the first time, this paper gives the reachable theoretical limitation of the attack ability of hacker and the defend ability of honker precisely.",
  "keyword": "internet security"
 },
 {
  "No": 165,
  "judul": "ESPQuery: An Enhanced Secure Scheme for Privacy-Preserving Query Based on Untrusted Devices in the Internet of Things",
  "abstrak": "The development of the Internet of Things (IoT) has brought various IoT services, which facilitate and enrich human life. All these services are at risk of privacy leakage. The privacy-preserving issue of IoT query, which is a typical service, has attracted much attention. A superior candidate for solving the above issue is classical cryptographic schemes based on the computational difficulty. With the advent of quantum computation, the security of such schemes may be broken by the strong ability of some advanced quantum algorithms. How to design a secure scheme for privacy-preserving query under the threat of quantum computation is crucial. To this end, we construct a general architecture of privacy-preserving query in IoT scenarios. Besides trust or honesty-but-curious models, we present a scheme for privacy-preserving query which is also valid in the scenario of untrusted devices. We provide a detailed security analysis. The result shows our scheme achieves private preservation of both service providers and clients.",
  "keyword": "internet security"
 },
 {
  "No": 166,
  "judul": "Twenty Security Considerations for Cloud-Supported Internet of Things",
  "abstrak": "To realize the broad vision of pervasive computing, underpinned by the “Internet of Things” (IoT), it is essential to break down application and technology-based silos and support broad connectivity and datasharing; the cloud being a natural enabler. Work in IoT tends toward the subsystem, often focusing on particular technical concerns or application domains, before offloading datato the cloud. As such, there has been little regard given to the security, privacy, and personal safety risks that arise beyond these subsystems; i.e., from the wide-scale, cross-platform openness that cloud services bring to IoT. In this paper, we focus on security considerations for IoT from the perspectives of cloud tenants, end-users, and cloud providers, in the context of wide-scale IoT proliferation, working across the range of IoT technologies (be they things or entire IoT subsystems). Our contribution is to analyze the current state of cloud-supported IoT to make explicit the security considerations that require further work.",
  "keyword": "internet security"
 },
 {
  "No": 167,
  "judul": "Immune System for the Internet of Things Using Edge Technologies",
  "abstrak": "The Internet of Things (IoT) and edge computing are starting to go hand in hand. By providing cloud services close to end-users, edge paradigms enhance the functionality of IoT deployments, and facilitate the creation of novel services such as augmented systems. Furthermore, the very nature of these paradigms also enables the creation of a proactive defense architecture, an immune system, which allows authorized immune cells (e.g., virtual machines) to traverse edge nodes and analyze the security and consistency of the underlying IoT infrastructure. In this paper, we analyze the requirements for the development of an immune system for the IoT, and propose a security architecture that satisfies these requirements. We also describe how such a system can be instantiated in edge computing infrastructures using existing technologies. Finally, we explore the potential application of immune systems to other scenarios and purposes.",
  "keyword": "internet security"
 },
 {
  "No": 168,
  "judul": "Anatomy of Threats to the Internet of Things",
  "abstrak": "The world is resorting to the Internet of Things (IoT) for ease of control and monitoring of smart devices. The ubiquitous use of IoT ranges from industrial control systems (ICS) to e-Health, e-Commerce, smart cities, supply chain management, smart cars, cyber physical systems (CPS), and a lot more. Such reliance on IoT is resulting in a significant amount of dataset to be generated, collected, processed, and analyzed. The big dataset analytics is no doubt beneficial for business development. However, at the same time, numerous threats to the availability and privacy of the user dataset, message, and device integrity, the vulnerability of IoT devices to malware attacks and the risk of physical compromise of devices pose a significant danger to the sustenance of IoT. This paper thus endeavors to highlight most of the known threats at various layers of the IoT architecture with a focus on the anatomy of malware attacks. We present a detailed attack methodology adopted by some of the most successful malware attacks on IoT, including ICS and CPS. We also deduce an attack strategy of a distributed denial of service attack through IoT botnet followed by requisite security measures. In the end, we propose a composite guideline for the development of an IoT security framework based on industry best practices and also highlight lessons learned, pitfalls and some open research challenges.",
  "keyword": "internet security"
 },
 {
  "No": 169,
  "judul": "A Survey on Key Agreement and Authentication Protocol for Internet of Things Application",
  "abstrak": "The Internet of Things (IoT) represents a dynamic infrastructure, leveraging sensing and network communication technology to establish ubiquitous connectivity among people, machines, and objects. Due to its end devices’ limited computing resources and storage space, it is not feasible to merely transpose traditional internet security technologies directly to IoT endpoints. Maintaining security while concurrently ensuring performance is a particularly challenging endeavor. This paper provides a review of key agreements and authentication protocols pivotal to the security of IoT. First, this survey discusses the applications that need authentication and key agreement to strengthen their security and current research on these application fields. Subsequently, this paper engages in an in-depth exploration of the phase involved in the scheme of authentication and key agreement, including an examination of the cryptographic techniques employed within these processes. This survey also thoroughly studies the scheme’s security services, potential attacks, formal analysis and informal analysis to ensure resilience against such threats. This study aims to provide a profound understanding of the recent research on authentication and key agreement in IoT applications. It strives to contribute towards strengthening security systems for IoT applications, ensuring their sustainability in the face of evolving threats.",
  "keyword": "internet security"
 },
 {
  "No": 170,
  "judul": "Security for the Internet of Things: A Survey of Existing Protocols and Open Research Issues",
  "abstrak": "The Internet of Things (IoT) introduces a vision of a future Internet where users, computing systems, and everyday objects possessing sensing and actuating capabilities cooperate with unprecedented convenience and economical benefits. As with the current Internet architecture, IP-based communication protocols will play a key role in enabling the ubiquitous connectivity of devices in the context of IoT applications. Such communication technologies are being developed in line with the constraints of the sensing platforms likely to be employed by IoT applications, forming a communications stack able to provide the required power-efficiency, reliability, and Internet connectivity. As security will be a fundamental enabling factor of most IoT applications, mechanisms must also be designed to protect communications enabled by such technologies. This survey analyzes existing protocols and mechanisms to secure communications in the IoT, as well as open research issues. We analyze how existing approaches ensure fundamental security requirements and protect communications on the IoT, together with the open challenges and strategies for future research work in the area. This is, as far as our knowledge goes, the first survey with such goals.",
  "keyword": "internet security"
 },
 {
  "No": 171,
  "judul": "Privacy and Security in Internet of Things and Wearable Devices",
  "abstrak": "Enter the nascent era of Internet of Things (IoT) and wearable devices, where small embedded devices loaded with sensors collect information from its surroundings, process it, and relay it to remote locations for further analysis. Albeit looking harmless, these nascent technologies raise security and privacy concerns. We pose the question of the possibility and effects of compromising such devices. Concentrating on the design flow of IoT and wearable devices, we discuss some common design practices and their implications on security and privacy. Two representatives from each category, the Google Nest Thermostat and the Nike+ Fuelband, are selected as examples on how current industry practices of security as an afterthought or an add-on affect the resulting device and the potential consequences to the user's security and privacy. We then discuss design flow enhancements, through which security mechanisms can efficiently be added into a device, vastly differing from traditional practices.",
  "keyword": "internet security"
 },
 {
  "No": 172,
  "judul": "A Review on Security Issues and Solutions of the Internet of Drones",
  "abstrak": "The Internet of Drones (IoD) has attracted increasing attention in recent years because of its portability and automation, and is being deployed in a wide range of fields (e.g., military, rescue and entertainment). Nevertheless, as a result of the inherently open nature of radio transmission paths in the IoD, datacollected, generated or handled by drones is plagued by many security concerns. Since security and privacy are among the foremost challenges for the IoD, in this paper we conduct a comprehensive review on security issues and solutions for IoD security, discussing IoD-related security requirements and identifying the latest advancement in IoD security research. This review analyzes a host of important security technologies with emphases on authentication techniques and blockchain-powered schemes. Based on a detailed analysis, we present the challenges faced by current methodologies and recommend future IoD security research directions. This review shows that appropriate security measures are needed to address IoD security issues, and that newly designed security solutions should particularly consider the balance between the level of security and cost efficiency.",
  "keyword": "internet security"
 },
 {
  "No": 173,
  "judul": "MoTH: Mobile Terminal Handover Security Protocol for HUB Switching Based on 5G and Beyond (5GB) P2MP Backhaul Environment",
  "abstrak": "With the evolution of wireless technologies, 5G and Beyond (5GB) communication is paving a way for efficient, ultrareliable, low-latent, and high converging services for the Internet of Things (IoT). Along with efficient communication, the security of messages is one of the concerns that must be maintained throughout the operations. Backhaul forms an essential part of 5GB with an ability to enhance the coverage and quality of service for IoT. However, conventional wired backhaul connection would cost operators thousands of dollars in the construction of 5GB infrastructure considering the ultradense nature of IoT. As a result, wireless backhaul is quickly becoming a feasible alternative to address 5GB’s direction toward network densification without affecting its other provisions. Wireless backhaul is expected to increase the landscape, covering from islands to mountains, which were difficult to access in the existing network generation. Moreover, it can effectively respond to the situation where thedata traffic tremendously increased. Despite such provisioning, the wireless backhaul poses relatively various security threats and vulnerabilities due to the characteristics of wireless technologies. Several studies have been conducted to address the security problems; however, existing protocols do not support dynamic security policy and key management in a decentralized structure as well as secure handover in a specific scenario where Terminals (TMs) are moving. Motivated by this, we proposed the Mobile Terminal Handover (MoTH) security protocol to provide secure handover of mobile terminals between hubs. To solve the problem of existing protocols, a new entity called BMF is introduced to support distributed and dynamic security policy and key management in each serving network of the 5GB backhaul environment. The proposed protocol satisfies security requirements, including authentication and key management, confidentiality, integrity, and perfect forward secrecy. Add...",
  "keyword": "internet security"
 },
 {
  "No": 174,
  "judul": "Physical-Layer Detection and Security of Printed Chipless RFID Tag for Internet of Things Applications",
  "abstrak": "This article has proposed detection and physical-layer security provision for printed sensory tag systems for Internet of Things (IoT) applications. The printed sensory tags can be a very cost-effective way to speed up the proliferation of the intelligent world of IoT. The printed radio-frequency identification (RFID) of a sensory tag is chipless with the fully printable feature, Nonline-of-Sight (NLoS) reading, low cost, and robustness to the environment. The detection and adoption of security features for such tags in a robust environment are still challenging. This article initially presents a robust technology for detecting tags using both the amplitude and phase information of the frequency signature. After successfully identifying tag IDs, the article presents novel physical-layer security using a deep learning model to prevent the cloning of tags. Our experiment shows that the proposed system can detect and identify the unique physical attributes of the tag and isolate the clone tag from the genuine tag. It is believed that such real-time and precise detection and security features bring this technology closer to commercialization for IoT applications.",
  "keyword": "internet security"
 },
 {
  "No": 175,
  "judul": "RAV: Relay Aided Vectorized Secure Transmission in Physical Layer Security for Internet of Things Under Active Attacks",
  "abstrak": "Internet of Things (IoT) security becomes of great importance, as IoT is the foundation for many emerging services. To safeguard IoT security, cryptosystems at upper layer relying on sophisticated key management alone can face many challenges due to the massive deployment of resource constrained machine-type communication (MTC) devices. Physical layer (PHY) security can complement and enhance IoT security, by exploiting the characteristics of the bottom layer. In PHY security, channel state information (CSI) estimated through reverse pilot training is essential for the sender to select appropriate beamforming\/precoder, which however is also vulnerable to adversaries. An adversary can actively launch pilot contamination attacks to affect the channel estimation and improve its signal reception quality. In this paper, we propose a relay-aided vectorized (RAV) secure transmission scheme, to safeguard the downlink communication in IoT networks under potential pilot contamination attacks. The proposed scheme does not distinguish the pilot sequences sent from an adversary and the receiver; and the sender utilizes what it receives to estimate the CSI for beamforming\/precoder design. Then, a set ofdata symbols are presuperposed using a random complex matrix to form signal vectors to send. Through cooperation with a relay, the signal vectors can be recovered by the intended receiver whereas the adversary or the relay cannot, as proved through security analysis. The simulation results also demonstrate that the bit error rate (BER) of the adversary is 0.5 regardless of its channel quality, indicating perfect secrecy is achieved.",
  "keyword": "internet security"
 },
 {
  "No": 176,
  "judul": "A Survey on Sensor-Based Threats and Attacks to Smart Devices and Applications",
  "abstrak": "Modern electronic devices have become “smart” as well as omnipresent in our day-to-day lives. From small household devices to large industrial machines, smart devices have become very popular in every possible application domain. Smart devices in our homes, offices, buildings, and cities can connect with other devices as well as with the physical world around them. This increasing popularity has also placed smart devices as the center of attention among attackers. Already, several types of malicious activities exist that attempt to compromise the security and privacy of smart devices. One interesting and noteworthy emerging threat vector is the attacks that abuse the use of sensors on smart devices. Smart devices are vulnerable to sensor-based threats and attacks due to the lack of proper security mechanisms available to control the use of sensors by installed apps. By exploiting the sensors (e.g., accelerometer, gyroscope, microphone, light sensor, etc.) on a smart device, attackers can extract information from the device, transfer malware to a device, or trigger a malicious activity to compromise the device. In this paper, we explore various threats and attacks abusing sensors of smart devices for malicious purposes. Specifically, we present a detailed survey about existing sensor-based threats and attacks to smart devices and countermeasures that have been developed to secure smart devices from sensor-based threats. Furthermore, we discuss security and privacy issues of smart devices in the context of sensor-based threats and attacks and conclude with future research directions.",
  "keyword": "internet security"
 },
 {
  "No": 177,
  "judul": "Design of Secure User Authenticated Key Management Protocol for Generic IoT Networks",
  "abstrak": "In recent years, the research in generic Internet of Things (IoT) attracts a lot of practical applications including smart home, smart city, smart grid, industrial Internet, connected healthcare, smart retail, smart supply chain and smart farming. The hierarchical IoT network (HIoTN) is a special kind of the generic IoT network, which is composed of the different nodes, such as the gateway node, cluster head nodes, and sensing nodes organized in a hierarchy. In HIoTN, there is a need, where a user can directly access the real-time datafrom the sensing nodes for a particular application in generic IoT networking environment. This paper emphasizes on the design of a new secure lightweight three-factor remote user authentication scheme for HIoTNs, called the user authenticated key management protocol (UAKMP). The three factors used in UAKMP are the user smart card, password, and personal biometrics. The security of the scheme is thoroughly analyzed under the formal security in the widely accepted real-or-random model, the informal security as well as the formal security verification using the widely accepted automated validation of Internet security protocols and applications tool. UAKMP offers several functionality features including offline sensing node registration, freely password and biometric update facility, user anonymity, and sensing node anonymity compared to other related existing schemes. In addition, UAKMP is also comparable in computation and communication costs as compared to other existing schemes.",
  "keyword": "internet security"
 },
 {
  "No": 178,
  "judul": "A Novel Image Steganography Method for Industrial Internet of Things Security",
  "abstrak": "The rapid development of the Industrial Internet of Things (IIoT) and artificial intelligence (AI) brings new security threats by exposing secret and privatedata. Thus, information security has become a major concern in the communication environment of IIoT and AI, where security and privacy must be ensured for the messages between a sender and the intended recipient. In this article, we propose a method called Harris hawks optimization-integer wavelet transform (HHO-IWT) for covert communication and securedata in the IIoT environment based on digital image steganography. The method embeds secretdata in the cover images using a metaheuristic optimization algorithm called HHO to efficiently select image pixels that can be used to hide bits of secretdata within integer wavelet transforms. The HHO-based pixel selection operation uses an objective function evaluation depending on the following two phases: exploitation and exploration. The objective function is employed to determine an optimal encoding vector to transform secretdata into an encoded form generated by the HHO algorithm. Several experiments are conducted to validate the performance of the proposed method with respect to visual quality, payload capacity, and security against attacks. The obtained results reveal that the HHO-IWT method achieves higher levels of security than the state-of-the-art methods and that it resists various forms of steganalysis. Thus, utilizing this approach can keep unauthorized individuals away from the transmitted information and solve some security challenges in the IIoT.",
  "keyword": "internet security"
 },
 {
  "No": 179,
  "judul": "Toward Secure and Provable Authentication for Internet of Things: Realizing Industry 4.0",
  "abstrak": "The Internet of Things (IoT) has many applications, including Industry 4.0. There are a number of challenges when deploying IoT devices in the Industry 4.0 setting, partly due to the low-cost IoT devices\/nodes with limited capacity to run\/support security solutions. Hence, there is a need for a lightweight and efficient security solution to protect the environment. Thus, in this article, we present a robust, lightweight, and provably secure authentication and key agreement protocol specifically for the IoT environment based on a hierarchical approach. The proposed protocol relies on lightweight operations, such as elliptic curve cryptography, physically unclonable functions, hash functions, concatenation, and XOR operations. We then evaluate the security of the designed protocol, including the widely used automated validation of Internet security protocols and applications (AVISPA), and demonstrate that it supports mutual authentication between IoT nodes and server, and is resilient against a number of common security attacks [denial of service (DoS), replay, spoofing, etc.]. The computational and communication overhead analysis shows that the proposed protocol is comparatively less expensive than three other recently published, competing protocols.",
  "keyword": "internet security"
 },
 {
  "No": 180,
  "judul": "Security Requirements and Solutions for IoT Gateways: A Comprehensive Study",
  "abstrak": "The need for improving the security level of Internet-of-Things (IoT) systems is growing. Users may refrain from using such systems if they realize that security measures are not in place. In this context, one of the most important IoT components has not received the necessary attention by the community: the gateway. IoT gateways play a central role in an IoT system as they solve heterogeneity issues. However, if compromised, gateways can be a source of security threats, and these threats become more relevant due to the gateway's central role. Gateways connect IoT devices and cloud services, and successful attacks on this component may exploit this fact. Using a well-known security requirements (SRs) engineering approach, this article evaluates and prioritizes SRs specifically for IoT gateways. The prioritization highlights a set of SRs that must be observed when evaluating and improving the gateway security level. Also, current solutions are detailed to support the enforcement of such SRs. Finally, a set of open challenges are discussed to highlight current research gaps that must be addressed to support SRs.",
  "keyword": "internet security"
 },
 {
  "No": 181,
  "judul": "Advances in IoT Security: Vulnerabilities, Enabled Criminal Services, Attacks, and Countermeasures",
  "abstrak": "Although the Internet of Things (IoT) incorporates millions of heterogeneous devices to provide advanced intelligent services and has greatly impacted our lives over time, it has a huge blind spot since its design favors connectivity over security. Myriad efforts have been made to secure it, but it is still one of the most lucrative and often an easy target for attackers. IoT devices remain at higher risk of attack due to their intrinsic properties which include but are not limited to extreme heterogeneity, mostly plug-and-play nature, computational limitations, improper patch management, unnecessary open ports, default or no security credentials, and extensive use of reusable open-source software. To address these security concerns we need to thoroughly understand IoT devices’ vulnerabilities, associated attacks, and how criminal services can abuse these devices. In this article, we present recent advances in IoT security vulnerabilities, criminal services by empirically identifying major vulnerable IoT devices and cyber attacks exploiting them by cyber criminals. Additionally, we present mapping of vulnerabilities, criminal services, attacks, and potential solutions against such vulnerabilities and attacks. We have also presented different approaches in a tabular form for side-by-side comparison.",
  "keyword": "internet security"
 },
 {
  "No": 182,
  "judul": "Exploiting Workflow Languages and Semantics for Validation of Security Policies in IoT Composite Services",
  "abstrak": "Internet of Things (IoT) ecosystems are recently experiencing a significant growth in complexity. Most IoT applications in domains like healthcare, industry, automotive, and smart energy are composed of several interconnected subsystems that produce, collect, process, and exchange a huge amount ofdata, and that offer composite services to the end users based on thesedata. This scenario is exacerbated by the dynamism of the IoT device layer, which may be subject to structural or technological changes over time, to cope for example with the need for new sensing\/actuation capabilities requirements or with technical issues. Due to the inherent sensitive nature of thedata that is typically processed by IoT applications, security represents one of the primary issues to address. It is worth noting that each subsystem integrated within a composite IoT application may have different requirements and enforce different local security policies, and the policies that result globally enforced at the system level may not comply with the existing global requirements. In general, the analysis and validation of security properties in a composite IoT system represents a very complex task, made even more complex by the introduction of new laws and regulations during system life. To cope with the above issues, in this article, we propose a methodology that leverages both workflow languages and semantics in order to enable the validation of the security features offered by a composite IoT system, with the goal of verifying whether they match with global end-user policies and even with national and international laws and rules.",
  "keyword": "internet security"
 },
 {
  "No": 183,
  "judul": "Security Management Architecture for NFV\/SDN-Aware IoT Systems",
  "abstrak": "The Internet of Things (IoT) brings a multidisciplinary revolution in several application areas. However, security and privacy concerns are undermining a reliable and resilient broad-scale deployment of IoT-enabled critical infrastructures (IoT-CIs). To fill this gap, this paper proposes a comprehensive architectural design that captures the main security and privacy challenges related to cyber-physical systems and IoT-CIs. The architecture is devised to empower IoT systems and networks to make autonomous security decisions through the usage of novel technologies such as software defined networking and network function virtualization, as well as endowing them with intelligent and dynamic security reaction capabilities by relying on monitoring methodologies and cyber-situational tools. The architecture has been successfully implemented and evaluated in the scope of ANASTACIA H2020 EU research project.",
  "keyword": "internet security"
 },
 {
  "No": 184,
  "judul": "LightTrust: Lightweight Trust Management for Edge Devices in Industrial Internet of Things",
  "abstrak": "The phenomenal increase in the usage of Internet promotes the quality of trust in the scope of the Internet of Things (IoT). Trust is beneficial in the provision of an effective, reliable, scalable, and trustworthy environment to users of the IoT network, where they can share their private information with each other on a secure communication platform. For successful communications among the Internet users, trust is an important factor to provide them with private infrastructures and secure environments, where exchanging dataamong devices becomes more easy and trustworthy. Therefore, trust management is a backbone for the successful and secure transmission of dataamong various nodes in a large-scale IoT network. To overcome the security issues, latency, and risk of malicious activities, a lightweight approach is proposed for those nodes in Industrial IoT that cannot maintain security. LightTrust utilizes a centralized trust agent to generate and manage trust certificates that allow nodes to communicate for a specific time without performing trust computations. Trust agents also maintain a trust database to store the current trust degree for the aggregation\/propagation purposes. Trust between two nodes is developed by direct observations in terms of compatibility, cooperativeness, and delivery ratio, whereas recommendations are used to develop trust in the context of indirect observations, i.e., experience or previous knowledge. The comparative simulations of the proposed and existing approaches are also performed whereby the results illustrate that the proposed approach efficiently maintains resilience and robust environments.",
  "keyword": "internet security"
 },
 {
  "No": 185,
  "judul": "Attitudes and Perceptions of IoT Security in Critical Societal Services",
  "abstrak": "A quiet revolution that impacts several sectors, ranging over transport, home automation, energy, industrial control, and health services is undergoing with addition of new networked devices leading to enhanced services. In this paper, we aim to identify information security requirements that are common over several (vertical) sectors, and in particular, ones that impact critical societal services, namely, the energy, water, and health management systems. We present the results of an interview-based study where actors in these sectors were asked about their perceptions and attitudes on the security of Internet of Things (IoT). We set these perceptions and attitudes in context through a literature review of IoT security, and relate to current challenges in this area. This paper demonstrates that despite an overall optimistic view on IoT in critical societal services, there is a lack of consensus on risks related to IoT security.",
  "keyword": "internet security"
 },
 {
  "No": 186,
  "judul": "Characterizing DNS Behaviors of Internet of Things in Edge Networks",
  "abstrak": "The recent spate of cyber attacks and security threats toward Internet-of-Things (IoT) systems in smart cities, smart homes, and industry 4.0 calls for effective techniques to understand if, when, who, what IoT systems are exploited and compromised by Internet attackers. Toward this end, this article attempts to study DNS behavioral patterns of IoT systems in edge networks as a first step of characterizing their communication patterns and their interactions with IoT users, cloud servers, and other IoT or non-IoT devices in the same edge networks. Specifically, we analyze the temporal-spatial patterns of DNS behaviors of a variety of IoT systems in two dozens of edge networks and develop a simple yet effective Bloom filter mechanism for detecting anomalous traffic patterns based on unusual DNS queries and answers. To the best of our knowledge, this article is the first effort to systematically measure and monitor IoT network traffic from a DNS perspective for providing the security of heterogeneous IoT systems and ensuring IoT user privacy.",
  "keyword": "internet security"
 },
 {
  "No": 187,
  "judul": "Efficient Policy-Hiding and Large Universe Attribute-Based Encryption With Public Traceability for Internet of Medical Things",
  "abstrak": "Modern day medical systems are closely integrated and interconnected with other systems, such as those comprising Internet-of-Medical Things (IoMT) devices that facilitate remote healthcare services, say during pandemics (e.g., COVID-19). Attribute-based encryption (ABE) is a promising cryptographic primitive to support fine-grained access control in the ciphertext environment; in other words, ABE can potentially be used to ensure dataconfidentiality and user privacy in the IoMT ecosystem. In this article, we propose an efficient partially-policy-hidden and large universe ABE scheme with public traceability to construct a practical IoMT system (hereafter referred to as PTIoMT). The system is designed to achieve the following features: 1) the access policy is partially hidden: only nonsensitive attribute labels\/names are displayed, while sensitive attribute values are hidden in the encrypted electronic health records (EHRs); 2) the number of the attributes is independent of the public parameters and, thus, can be arbitrarily large; 3) any user who discloses the decryption key can be efficiently tracked; and 4) fewer bilinear pairing operations are required during the decryption process. The security analysis and performance evaluation demonstrate the security and efficiency of PTIoMT.",
  "keyword": "internet security"
 },
 {
  "No": 188,
  "judul": "A Security Analysis Method for Industrial Internet of Things",
  "abstrak": "The industrial Internet of Things (IIoT) provide an opportunity for industries to build large interconnected systems that utilize various technologies, such as personal computers, wireless devices, and sensor devices, and bring together the cyber and the physical worlds. Such systems provide us with huge advantages but they also introduce major security challenges at both the design and runtime stages. The literature argues for the need to introduce security-by-design methods, which enable security analysis and mitigation of security threats. This paper proposes a novel security-by-design method for IIoT environments across two different levels, design\/modeling, and runtime\/simulation. Our method supports the analysis of security requirements and identification of attack paths and their integration for the mitigation of potential vulnerabilities. We demonstrate its applicability through a real case study on a critical environment from the maritime sector, which demonstrates that our method helps to identify security mechanisms to mitigate attacks on critical assets.",
  "keyword": "internet security"
 },
 {
  "No": 189,
  "judul": "Challenges and Opportunities in Securing the Industrial Internet of Things",
  "abstrak": "Given the tremendous success of the Internet of Things in interconnecting consumer devices, we observe a natural trend to likewise interconnect devices in industrial settings, referred to as industrial Internet of Things or Industry 4.0. While this coupling of industrial components provides many benefits, it also introduces serious security challenges. Although sharing many similarities with the consumer Internet of Things, securing the industrial Internet of Things introduces its own challenges but also opportunities, mainly resulting from a longer lifetime of components and a larger scale of networks. In this article, we identify the unique security goals and challenges of the industrial Internet of Things, which, unlike consumer deployments, mainly follow from safety and productivity requirements. To address these security goals and challenges, we provide a comprehensive survey of research efforts to secure the industrial Internet of Things, discuss their applicability, and analyze their security benefits.",
  "keyword": "internet security"
 },
 {
  "No": 190,
  "judul": "Content Object Security in the Internet of Things: Challenges, Prospects, and Emerging Solutions",
  "abstrak": "Content objects are confined dataset elements that carry meaningful information. Massive amounts of content objects are published and exchanged every day on the Internet. The emerging Internet of Things (IoT) augments the network edge with reading sensors and controlling actuators that comprise machine-to-machine communication using small dataset objects. IoT content objects are often messages that fit into single IPv6 datagram. These IoT messages frequently traverse protocol translators at gateways, which break end-to-end transport and security of Internet protocols. To preserve content security from end to end via gateways and proxies, the IETF recently developed Object Security for Constrained RESTful Environments (OSCORE), which extends the Constrained Application Protocol (CoAP) with content object security features commonly known from Information Centric Networking (ICN). This paper revisits the current IoT protocol architectures and presents a comparative analysis of protocol stacks that protect request-response transactions. We discuss features and limitations of the different protocols and analyze emerging functional extensions. We measure the protocol performances of CoAP over Datagram Transport Layer Security (DTLS), OSCORE, and the information-centric NDN protocol on a large-scale IoT testbed in single- and multi-hop scenarios. Our findings indicate that (a) OSCORE improves on CoAP over DTLS in error-prone wireless regimes due to omitting the overhead of maintaining security sessions at endpoints, (b) NDN attains superior robustness and reliability due to its intrinsic network caches and hop-wise retransmissions, and (c) OSCORE\/CoAP offers room for improvement and optimization in multiple directions.",
  "keyword": "internet security"
 },
 {
  "No": 191,
  "judul": "Securing the Internet of Things: A Standardization Perspective",
  "abstrak": "The Internet of Things (IoT) is the next wave of innovation that promises to improve and optimize our daily life based on intelligent sensors and smart objects working together. Through Internet Protocol (IP) connectivity, devices can now be connected to the Internet, thus allowing them to be read, controlled, and managed at any time and at any place. Security is an important aspect for IoT deployments. However, proprietary security solutions do not help in formulating a coherent security vision to enable IoT devices to securely communicate with each other in an interoperable manner. This paper gives an overview of the efforts in the Internet Engineering Task Force (IETF) to standardize security solutions for the IoT ecosystem. We first provide an in-depth review of the communication security solutions for IoT, specifically the standard security protocols to be used in conjunction with the Constrained Application Protocol (CoAP), an application protocol specifically tailored to the needs of adapting to the constraints of IoT devices. Since Datagram Transport Layer Security (DTLS) has been chosen as the channel security underneath CoAP, this paper also discusses the latest standardization efforts to adapt and enhance the DTLS for IoT applications. This includes the use of 1) raw public key in DTLS; 2) extending DTLS record Layer to protect group (multicast) communication; and 3) profiling DTLS for reducing the size and complexity of implementations on embedded devices. We also provide an extensive review of compression schemes that are being proposed in IETF to mitigate message fragmentation issues in DTLS.",
  "keyword": "internet security"
 },
 {
  "No": 192,
  "judul": "A Survey on Security and Privacy Issues in Internet-of-Things",
  "abstrak": "Internet-of-Things (IoT) are everywhere in our daily life. They are used in our homes, in hospitals, deployed outside to control and report the changes in environment, prevent fires, and many more beneficial functionality. However, all those benefits can come of huge risks of privacy loss and security issues. To secure the IoT devices, many research works have been conducted to countermeasure those problems and find a better way to eliminate those risks, or at least minimize their effects on the user's privacy and security requirements. The survey consists of four segments. The first segment will explore the most relevant limitations of IoT devices and their solutions. The second one will present the classification of IoT attacks. The next segment will focus on the mechanisms and architectures for authentication and access control. The last segment will analyze the security issues in different layers.",
  "keyword": "internet security"
 },
 {
  "No": 193,
  "judul": "Consumer, Commercial, and Industrial IoT (In)Security: Attack Taxonomy and Case Studies",
  "abstrak": "Internet of Things (IoT) devices are becoming ubiquitous in our lives, with applications spanning from the \nconsumer\n domain to \ncommercial\n and \nindustrial\n systems. The steep growth and vast adoption of IoT devices reinforce the importance of sound and robust cybersecurity practices during the device development life cycles. IoT-related vulnerabilities, if successfully exploited can affect, not only the device itself but also the application field in which the IoT device operates. Evidently, identifying and addressing every single vulnerability are an arduous, if not impossible, task. Attack taxonomies can assist in classifying attacks and their corresponding vulnerabilities. Security countermeasures and best practices can then be leveraged to mitigate threats and vulnerabilities before they emerge into catastrophic attacks and ensure overall secure IoT operation. Therefore, in this article, we provide an attack taxonomy, which takes into consideration the different layers of the IoT stack, i.e., device, infrastructure, communication, and service, and each layer’s designated characteristics, which can be exploited by adversaries. Furthermore, using nine real-world cybersecurity incidents that had targeted IoT devices deployed in the consumer, commercial, and industrial sectors, we describe the IoT-related vulnerabilities, exploitation procedures, attacks, impacts, and potential mitigation mechanisms and protection strategies. These (and many other) incidents highlight the underlying security concerns of IoT systems and demonstrate the potential attack impacts of such connected ecosystems, while the proposed taxonomy provides a systematic procedure to categorize attacks based on the affected layer and corresponding impact.",
  "keyword": "internet security"
 },
 {
  "No": 194,
  "judul": "Analysis on Security and Privacy Guidelines: RFID-Based IoT Applications",
  "abstrak": "The Internet of Things (IoT) comprises many technologies, among them is Radio Frequency Identification (RFID), which can be used to track single or multiple objects. This technology has been widely used in healthcare, supply chain, logistics, and asset tracking. However, such applications require a high level of security and privacy and are unfortunately vulnerable to various attacks and threats that need to be addressed in order for RFID-based IoT applications to reach their full potential. To this end, we propose a set of security and privacy guidelines for RFID, supported by modelling guidelines, mitigations, and the attack vectors cohesively. We compare to the state of the art and point out their shortcomings on known guidelines and reason to address these in our model. The overall methodology is as follows: (i) identify the security and privacy guideline features, (ii) highlight the security goals for RFID-based IoT applications, (iii) analyze the features in relation to RFID industrial standards, and relate them to security goals, (iv) summarize attacks and threats against RFID applications and correlate them with violated security goals, (v) derive a set of security and privacy guidelines for RFID applications in accordance with security and privacy by design frameworks. We also describe our derived guidelines in connection with the involved stakeholders, and (vi) outline the existing mitigation strategies to implement our proposed guidelines. Finally, we describe the main limitations of our work that should be investigated in the future and identify the multiple challenges that concern current security strategies.",
  "keyword": "internet security"
 },
 {
  "No": 195,
  "judul": "Arm PSA-Certified IoT Chip Security: A Case Study",
  "abstrak": "With the large scale adoption of Internet of Things (IoT) applications in people's lives and industrial manufacturing processes, IoT security has become an important problem today. IoT security significantly relies on the security of the underlying hardware chip, which often contains critical information, such as encryption key. To understand existing IoT chip security, this study analyzes the security of an IoT security chip that has obtained an Arm Platform Security Architecture (PSA) Level 2 certification. Our analysis shows that the chip leaks part of the encryption key and presents a considerable security risk. Specifically, we use commodity equipment to collect electromagnetic traces of the chip. Using a statistical T-test, we find that the target chip has physical leakage during the AES encryption process. We further use correlation analysis to locate the detailed encryption interval in the collected electromagnetic trace for the Advanced Encryption Standard (AES) encryption operation. On the basis of the intermediate value correlation analysis, we recover half of the 16-byte AES encryption key. We repeat the process for three different tests; in all the tests, we obtain the same result, and we recover around 8 bytes of the 16-byte AES encryption key. Therefore, experimental results indicate that despite the Arm PSA Level 2 certification, the target security chip still suffers from physical leakage. Upper layer application developers should impose strong security mechanisms in addition to those of the chip itself to ensure IoT application security.",
  "keyword": "internet security"
 },
 {
  "No": 196,
  "judul": "On the Security–Reliability and Secrecy Throughput of Random Mobile User in Internet of Things",
  "abstrak": "Physical-layer security (PLS) in Internet of Things (IoT) has attracted great attentions recently. Although mobility is an intrinsic property of IoT networks, most of the existing works only investigate the secure transmission design for static users. To fill this gap, this article specifically investigates the secrecy throughput maximization problems for the mobile IoT user under two typical mobility models: 1) random waypoint model (RWP) and 2) random direction model (RD). The insights about how the mobility patterns, and security–reliability requirements affect the mobile user’s secrecy throughput are revealed. First, in order to establish the relationship between security and reliability of the mobile user, a general analytic framework is provided to derive the closed-form expressions of transmit secrecy outage probability (TSOP) for the mobile user. Second, two transmission schemes are proposed to maximize the secrecy throughput of the mobile user by ensuring a certain level of transmit probability (TP) and TSOP requirements. The numerical and simulation results verify the validity and effectiveness of the proposed schemes, and indicate that by adopting appropriate mobility pattern, the user’s secrecy throughput can be improved, and the constraint on its moving region can be largely reduced. Those properties are lightweight and feasible to enhance security for many mobile IoT scenarios.",
  "keyword": "internet security"
 },
 {
  "No": 197,
  "judul": "The Global State of Security in Industrial Control Systems: An Empirical Analysis of Vulnerabilities Around the World",
  "abstrak": "Operational Technology (OT) networks and devices, i.e., all components used in industrial environments, were not designed with security in mind. Efficiency and ease of use were the most important design characteristics. However, due to the digitization of industry, an increasing number of devices and industrial networks are opened up to public networks. This is beneficial for the administration and organization of the industrial environments. However, it also increases the attack surface, providing possible points of entry for an attacker. Originally, breaking into production networks meant to break an information technology (IT)-perimeter first, such as a public Website, and then to move laterally to industrial control systems (ICSs) to influence the production environment. However, many OT-devices are connected directly to the Internet, which drastically increases the threat of compromise, especially since OT-devices contain several vulnerabilities. In this work, the presence of OT-devices in the Internet is analyzed from an attacker’s perspective. Publicly available tools, such as the search engine \nShodan\n and vulnerability databases, are employed to find commonly used OT-devices and map vulnerabilities to them. These findings are grouped according to the country of origin, manufacturer, and number as well as severity of vulnerability. More than 13000 devices were found, almost all contained at least one vulnerability. European and Northern American countries are by far the most affected ones.",
  "keyword": "internet security"
 },
 {
  "No": 198,
  "judul": "A Comprehensive Review on Secure Routing in Internet of Things: Mitigation Methods and Trust-Based Approaches",
  "abstrak": "Internet of Things (IoT) is a network of “things,” connected via Internet, to collect and exchange dataset. These “things” can be sensors, actuators, smartphones, wearables, computers, or any object that is interconnected to provide specific services. Similarly, wireless sensor network (WSN), as a part of IoT, forwards the gathered dataset after sensing any event. The scalability and heterogeneity of IoT offer limited protection and is prone to diverse attacks, including WSN-inherited attacks. Moreover, IPv6 routing protocol for low power and lossy networks (RPL), a de facto routing protocol for IoT networks, also suffers from certain vulnerabilities based on its features and functionalities. Researchers have proposed various mitigation mechanisms for secure networks and routing in IoT. Recently, trust-based approaches have gained tremendous interest from the research community to embed security in IoT networks and routing protocols. In the existing literature, several trust models have been introduced according to the security needs of the IoT system, such as SecTrust, DCTM-IoT, CTRUST, etc. In this research, security issues and requirements of IoT networks and RPL routing protocol are studied with respect to various attacks, such as Blackhole, Spoofing, Rank, etc. Additionally, various mitigation methods and significance of trust models in IoT for secure routing are analyzed. Further, trust metrics in IoT environments, including the open issues and research challenges, as well as the implication of trust as a security paradigm in IoT networks and routing protocols are discussed.",
  "keyword": "internet security"
 },
 {
  "No": 199,
  "judul": "Ontology-Based Security Recommendation for the Internet of Medical Things",
  "abstrak": "Security and privacy are among the key barriers to adopting the Internet of Medical Things (IoMT) solutions. IoMT adopters have to adhere to security and privacy policies to ensure that patient dataremains confidential and secure. However, there is confusion among IoMT stakeholders as to what security measures they should expect from the IoMT manufacturers and whether these measures would comply with the adopter's security and compliance requirements. In this paper, we present a recommendation tool that models IoMT concepts and security issues in addition to successively recommending security measures. The presented tool utilizes semantically enriched ontology to model the IoMT components, security issues, and measures. The developed ontology is equipped with context-aware rules to enable reasoning in order to build a recommendation system that empowers users to make well-educated decisions. The recommendation tool classifies IoMT security threats faced by IoMT stakeholders and automatically recommends security controls that have to be enforced for each threat. We have experimented the proposed tool with respect to the completeness and effectiveness of its output (i.e., security issues and recommended security measures). The results show that the tool was effectively able to recommend necessary security measures.",
  "keyword": "internet security"
 },
 {
  "No": 200,
  "judul": "Ranking Security of IoT-Based Smart Home Consumer Devices",
  "abstrak": "Manufacturers of smart home consumer devices like home theatres, music players, voice-based assistants, smart lighting, and security cameras have widely adopted the Internet of Things (IoT). These devices pose a significant security risk to consumers because the devices are exposed to mobile applications and cloud-based services with known security vulnerabilities. Most current home consumer devices provide little or no information about the level of security they afford. Since most consumers are not tech-savvy, it is currently difficult for a consumer to make an informed decision about which consumer device model (e.g., smart television model) has the best security. Hence, consumers need an objective security ranking of each type (e.g., security cameras) of home consumer devices. This paper proposes a novel methodology to systematically build such security rankings for home consumer devices. The proposed methodology can be applied by utilizing datafrom any security assessment study. The paper discusses previous efforts in applying Analytic Hierarchy Process (AHP) to rank security risks in general. The paper also presents a systematic survey of security vulnerabilities of smart home consumer devices when viewed from an IoT lens. Using the proposed methodology, a case study, employing an AHP model for ranking commonly used home consumer devices including home theatres, security cameras, smart lighting, smart speakers, video surveillance, smart switches, home automation systems, home security systems, smart routers, wireless doorbell cameras, and home audio systems, was developed. Relative security rankings for each type of consumer device were derived from the AHP model. According to the AHP model, network security was the primary driver of smart home device security with a priority of 0.6893 while application security had the least priority of 0.0591. Critical Vulnerabilities were the most important for device security (priority=0.4397), Man-in-The-Middle attacks f...",
  "keyword": "internet security"
 },
 {
  "No": 201,
  "judul": "Evaluation of Fault Level of Sensitive Equipment Caused by Voltage Sag via Data Mining",
  "abstrak": "So far, there is no literature to evaluate the fault level of sensitive equipment (FLSE) caused by voltage sag from the perspective of the power grid. In practice, although the FLSE are dominated by the voltage sag of node, the voltage sag of the node is associated with whole power grid, including the voltage grade and location of the node, the distance between the concerned node and location of fault, and the weather and date at the time of fault, which are all named as voltage sag properties (VSP). In view of this gap, this paper evaluates the FLSE using the long-time monitoring data of VSP of some regional power grid by data mining method based on multidimensional matrix simplification and the improved gray target theory. The data mining process can be divided into three steps: 1) construct a database consisted of VSP and FLSE using long-time monitoring data, 2) mine the association rules between VSP and FLSE by multidimensional matrix simplification, and 3) match some actual scenario with the mined association rules by the improved gray target theory. Performance and effectiveness of the proposed method are verified through the simulation and field case.",
  "keyword": "data mining"
 },
 {
  "No": 202,
  "judul": "A Constrained Randomization Approach to Interactive Visual Data Exploration with Subjective Feedback",
  "abstrak": "Data visualization and iterative\/interactive data mining are growing rapidly in attention, both in research as well as in industry. However, while there are a plethora of advanced data mining methods and lots of works in the field of visualization, integrated methods that combine advanced visualization and\/or interaction with data mining techniques in a principled way are rare. We present a framework based on constrained randomization which lets users explore high-dimensional data via `subjectively informative' two-dimensional data visualizations. The user is presented with `interesting' projections, allowing users to express their observations using visual interactions that update a background model representing the user's belief state. This background model is then considered by a projection-finding algorithm employing data randomization to compute a new `interesting' projection. By providing users with information that contrasts with the background model, we maximize the chance that the user encounters striking new information present in the data. This process can be iterated until the user runs out of time or until the difference between the randomized and the real data is insignificant. We present two case studies, one controlled study on synthetic data and another on census data, using the proof-of-concept tool SIDE that demonstrates the presented framework.",
  "keyword": "data mining"
 },
 {
  "No": 203,
  "judul": "Efficient Tree Structures for High Utility Pattern Mining in Incremental Databases",
  "abstrak": "Recently, high utility pattern (HUP) mining is one of the most important research issues in data mining due to its ability to consider the nonbinary frequency values of items in transactions and different profit values for every item. On the other hand, incremental and interactive data mining provide the ability to use previous data structures and mining results in order to reduce unnecessary calculations when a database is updated, or when the minimum threshold is changed. In this paper, we propose three novel tree structures to efficiently perform incremental and interactive HUP mining. The first tree structure, Incremental HUP Lexicographic Tree ({\\rm IHUP}_{{\\rm {L}}}-Tree), is arranged according to an item's lexicographic order. It can capture the incremental data without any restructuring operation. The second tree structure is the IHUP Transaction Frequency Tree ({\\rm IHUP}_{{\\rm {TF}}}-Tree), which obtains a compact size by arranging items according to their transaction frequency (descending order). To reduce the mining time, the third tree, IHUP-Transaction-Weighted Utilization Tree ({\\rm IHUP}_{{\\rm {TWU}}}-Tree) is designed based on the TWU value of items in descending order. Extensive performance analyses show that our tree structures are very efficient and scalable for incremental and interactive HUP mining.",
  "keyword": "data mining"
 },
 {
  "No": 204,
  "judul": "Mining 5.0: Concept and Framework for Intelligent Mining Systems in CPSS",
  "abstrak": "This letter is part of the Intelligent Mining Development Forum and aims to summarize the discussions of Mining 5.0 from Intelligent Vehicle 5.0 project by IEEE TIV, which represents a shift from Cyber-Physical Systems (CPS) to Cyber-Physical-Social Systems (CPSS). This letter highlights the basic theories and summarizes the framework of Mining 5.0 with three worlds, three modes, and three technologies that promote values of ESG, ERS, and DEI in physical, mental, and artificial worlds. Mining 5.0 plays a pivotal role in achieving “6S” goals (Safety, Security, Sustainability, Sensitivity, Service, Smartness) for mining industries.",
  "keyword": "data mining"
 },
 {
  "No": 205,
  "judul": "Efficient Algorithms for Mining Top-K High Utility Itemsets",
  "abstrak": "High utility itemsets (HUIs) mining is an emerging topic in data mining, which refers to discovering all itemsets having a utility meeting a user-specified minimum utility threshold min_util. However, setting min_util appropriately is a difficult problem for users. Generally speaking, finding an appropriate minimum utility threshold by trial and error is a tedious process for users. If min_util is set too low, too many HUIs will be generated, which may cause the mining process to be very inefficient. On the other hand, if min_util is set too high, it is likely that no HUIs will be found. In this paper, we address the above issues by proposing a new framework for top-k high utility itemset mining, where k is the desired number of HUIs to be mined. Two types of efficient algorithms named TKU (mining Top-K Utility itemsets) and TKO (mining Top-K utility itemsets in One phase) are proposed for mining such itemsets without the need to set min_util. We provide a structural comparison of the two algorithms with discussions on their advantages and limitations. Empirical evaluations on both real and synthetic datasets show that the performance of the proposed algorithms is close to that of the optimal case of state-of-the-art utility mining algorithms.",
  "keyword": "data mining"
 },
 {
  "No": 206,
  "judul": "Remote Health Monitoring of Heart Failure With Data Mining via CART Method on HRV Features",
  "abstrak": "Disease management programs, which use no advanced information and computer technology, are as effective as telemedicine but more efficient because less costly. We proposed a platform to enhance effectiveness and efficiency of home monitoring using data mining for early detection of any worsening in patient's condition. These worsenings could require more complex and expensive care if not recognized. In this letter, we briefly describe the remote health monitoring platform we designed and realized, which supports heart failure (HF) severity assessment offering functions of data mining based on the classification and regression tree method. The system developed achieved accuracy and a precision of 96.39% and 100.00% in detecting HF and of 79.31% and 82.35% in distinguishing severe versus mild HF, respectively. These preliminary results were achieved on public databases of signals to improve their reproducibility. Clinical trials involving local patients are still running and will require longer experimentation.",
  "keyword": "data mining"
 },
 {
  "No": 207,
  "judul": "Educational Data Mining to Support Programming Learning Using Problem-Solving Data",
  "abstrak": "Computer programming has attracted a lot of attention in the development of information and communication technologies in the real world. Meeting the growing demand for highly skilled programmers in the ICT industry is one of the major challenges. In this point, online judge (OJ) systems enhance programming learning and practice opportunities in addition to classroom-based learning. Consequently, OJ systems have created a large number of problem-solving data (solution codes, logs, and scores) archives that can be valuable raw materials for programming education research. In this paper, we propose an educational data mining framework to support programming learning using unsupervised algorithms. The framework includes the following sequence of steps: (\n i \n) problem-solving data collection (logs and scores are collected from the OJ) and preprocessing; (\n ii \n) MK-means clustering algorithm is used for data clustering in Euclidean space; (\n iii \n) statistical features are extracted from each cluster; (\n iv \n) frequent pattern (FP)-growth algorithm is applied to each cluster to mine data patterns and association rules; (\n v \n) a set of suggestions are provided on the basis of the extracted features, data patterns, and rules. Different parameters are adjusted to achieve the best results for clustering and association rule mining algorithms. For the experiment, approximately 70,000 real-world problem-solving data from 537 students of a programming course (Algorithm and Data Structures) were used. In addition, synthetic data have leveraged for experiments to demonstrate the performance of MK-means algorithm. The experimental results show that the proposed framework effectively extracts useful features, patterns, and rules from problem-solving data. Moreover, these extracted features, patterns, and rules highlight the weaknesses and the scope of possible improvements in programming learning.",
  "keyword": "data mining"
 },
 {
  "No": 208,
  "judul": "Multilevel Process Mining for Financial Audits",
  "abstrak": "The relevance of business intelligence increases with the growing amount of recorded data. The research on business intelligence has led to a mature set of methods and tools that are used in many application areas, but they are almost absent in the auditing industry. Public accountants face the challenge to audit increasingly complex business processes that process huge amounts of transaction data. Process mining can be used as a business intelligence approach in the context of process audits to exploit this data. We introduce a process mining algorithm to improve such audits. Key requirements for this purpose are the reliability of the mining results, the integration of a data flow perspective and the ability to inspect data from the point of origin to the final output on the financial accounts. The presented algorithm integrates the control flow and data flow perspective. It operates on different abstraction levels to enable the auditor to follow the audit trail. The algorithm creates precise and fitting process models to prevent false negative and false positive audit results, accepts specific unlabeled event logs as input, and considers data relationships for inferring the control flow. It was evaluated by using extensive real world data.",
  "keyword": "data mining"
 },
 {
  "No": 209,
  "judul": "Four Decades of Data Mining in Network and Systems Management",
  "abstrak": "How has the interdisciplinary data mining field been practiced in Network and Systems Management (NSM)? In Science and Technology, there is a wide use of data mining in areas like bioinformatics, genetics, Web, and, more recently, astroinformatics. However, the application in NSM has been limited and inconsiderable. In this article, we provide an account of how data mining has been applied in managing networks and systems for the past four decades, presumably since its birth. We look into the field's applications in the key NSM activities-discovery, monitoring, analysis, reporting, and domain knowledge acquisition. In the end, we discuss our perspective on the issues that are considered critical for the effective application of data mining in the modern systems which are characterized by heterogeneity and high dynamism.",
  "keyword": "data mining"
 },
 {
  "No": 210,
  "judul": "Non-Query-Based Pattern Mining and Sentiment Analysis for Massive Microblogging Online Texts",
  "abstrak": "Pattern mining has been widely studied in the last decade given its great interest for research and its numerous applications in the real world. In this paper the definition of query and non-query based systems is proposed, highlighting the needs of non-query based systems in the era of Big Data. For this, we propose a new approach of a non-query based system that combines association rules, generalized rules and sentiment analysis in order to catalogue and discover opinion patterns in the social network Twitter. Association rules have been previously applied for sentiment analysis, but in most cases, they are used once the process of sentiment analysis is finished to see which tokens appear commonly related to a certain sentiment. On the other hand, they have also been used to discover patterns between sentiments. Our work differs from these in that it proposes a non-query based system which combines both techniques, in a mixed proposal of sentiment analysis and association rules to discover patterns and sentiment patterns in microblogging texts. The obtained rules generalize and summarize the sentiments obtained from a group of tweets about any character, brand or product mentioned in them. To study the performance of the proposed system, an initial set of 1.7 million tweets have been employed to analyse the most salient sentiments during the American pre-election campaign. The analysis of the obtained results supports the capability of the system of obtaining association rules and patterns with great descriptive value in this use case. Parallelisms can be established in these patterns that match perfectly with real life events.",
  "keyword": "data mining"
 },
 {
  "No": 211,
  "judul": "Enabling Multilevel Trust in Privacy Preserving Data Mining",
  "abstrak": "Privacy Preserving Data Mining (PPDM) addresses the problem of developing accurate models about aggregated data without access to precise information in individual data record. A widely studied perturbation-based PPDM approach introduces random perturbation to individual values to preserve privacy before data are published. Previous solutions of this approach are limited in their tacit assumption of single-level trust on data miners. In this work, we relax this assumption and expand the scope of perturbation-based PPDM to Multilevel Trust (MLT-PPDM). In our setting, the more trusted a data miner is, the less perturbed copy of the data it can access. Under this setting, a malicious data miner may have access to differently perturbed copies of the same data through various means, and may combine these diverse copies to jointly infer additional information about the original data that the data owner does not intend to release. Preventing such diversity attacks is the key challenge of providing MLT-PPDM services. We address this challenge by properly correlating perturbation across copies at different trust levels. We prove that our solution is robust against diversity attacks with respect to our privacy goal. That is, for data miners who have access to an arbitrary collection of the perturbed copies, our solution prevent them from jointly reconstructing the original data more accurately than the best effort using any individual copy in the collection. Our solution allows a data owner to generate perturbed copies of its data for arbitrary trust levels on-demand. This feature offers data owners maximum flexibility.",
  "keyword": "data mining"
 },
 {
  "No": 212,
  "judul": "Non-Query-Based Pattern Mining and Sentiment Analysis for Massive Microblogging Online Texts",
  "abstrak": "Pattern mining has been widely studied in the last decade given its great interest for research and its numerous applications in the real world. In this paper the definition of query and non-query based systems is proposed, highlighting the needs of non-query based systems in the era of Big Data. For this, we propose a new approach of a non-query based system that combines association rules, generalized rules and sentiment analysis in order to catalogue and discover opinion patterns in the social network Twitter. Association rules have been previously applied for sentiment analysis, but in most cases, they are used once the process of sentiment analysis is finished to see which tokens appear commonly related to a certain sentiment. On the other hand, they have also been used to discover patterns between sentiments. Our work differs from these in that it proposes a non-query based system which combines both techniques, in a mixed proposal of sentiment analysis and association rules to discover patterns and sentiment patterns in microblogging texts. The obtained rules generalize and summarize the sentiments obtained from a group of tweets about any character, brand or product mentioned in them. To study the performance of the proposed system, an initial set of 1.7 million tweets have been employed to analyse the most salient sentiments during the American pre-election campaign. The analysis of the obtained results supports the capability of the system of obtaining association rules and patterns with great descriptive value in this use case. Parallelisms can be established in these patterns that match perfectly with real life events.",
  "keyword": "data mining"
 },
 {
  "No": 213,
  "judul": "Game Data Mining Competition on Churn Prediction and Survival Analysis Using Commercial Game Log Data",
  "abstrak": "Game companies avoid sharing their game data with external researchers. Only a few research groups have been granted limited access to game data so far. The reluctance of these companies to make data publicly available limits the wide use and development of data mining techniques and artificial intelligence research specific to the game industry. In this paper, we developed and implemented an international competition on game data mining using commercial game log data from one of the major game companies in South Korea: NCSOFT. Our approach enabled researchers to develop and apply state-of-the-art data mining techniques to game log data by making the data open. For the competition, data were collected from Blade & Soul, an action role-playing game, from NCSOFT. The data comprised approximately 100 GB of game logs from 10 000 players. The main aim of the competition was to predict whether a player would churn and when the player would churn during two periods between which the business model was changed to a free-to-play model from a monthly subscription. The results of the competition revealed that highly ranked competitors used deep learning, tree boosting, and linear regression.",
  "keyword": "data mining"
 },
 {
  "No": 214,
  "judul": "A Survey on Trajectory Data Mining: Techniques and Applications",
  "abstrak": "Rapid advance of location acquisition technologies boosts the generation of trajectory data, which track the traces of moving objects. A trajectory is typically represented by a sequence of timestamped geographical locations. A wide spectrum of applications can benefit from the trajectory data mining. Bringing unprecedented opportunities, large-scale trajectory data also pose great challenges. In this paper, we survey various applications of trajectory data mining, e.g., path discovery, location prediction, movement behavior analysis, and so on. Furthermore, this paper reviews an extensive collection of existing trajectory data mining techniques and discusses them in a framework of trajectory data mining. This framework and the survey can be used as a guideline for designing future trajectory data mining solutions.",
  "keyword": "data mining"
 },
 {
  "No": 215,
  "judul": "A Paradigm-Shifting From Domain-Driven Data Mining Frameworks to Process-Based Domain-Driven Data Mining-Actionable Knowledge Discovery Framework",
  "abstrak": "The success of data mining learned rules highly depends on its actionability: how useful it is to perform suitable actions in any real business environment. To improve rule actionability, different researchers have initially presented various Data Mining (DM) frameworks by focusing on different factors only from the business domain \ndataset\n. Afterward, different Domain-Driven Data Mining (D3M) frameworks were introduced by focusing \non domain knowledge\n factors from the context of the overall business environment. Despite considering these several \ndataset\n factors and \ndomain knowledge\n factors in different phases of their frameworks, the learned rules still lacked actionability. The objective of our research is to improve the learned rules’ actionability. For this purpose, we have analyzed: (1) what overall actions or tasks are being performed in the overall business process, (2) in which sequence different tasks are being performed, (3) under what certain conditions these tasks are being performed, (4) by whom the tasks are being performed (5) what data is provided and produced in performing these tasks. We observed that the inclusion of rule learning factors only from \ndataset\n or from \ndomain knowledge\n is not sufficient. Our Process-based Domain-Driven Data Mining-Actionable Knowledge Discovery (PD3M-AKD) framework explains its different phases to consider and include additional factors from \nfive perspectives\n of the business process. This PD3M-AKD framework is also in line with the existing phases of current DM and D3M frameworks for considering and including \ndataset\n and \ndomain knowledge\n accordingly. Finally, we evaluated and validated our case study results from different real-life scenarios from education, engineering, and business process domains at the end.",
  "keyword": "data mining"
 },
 {
  "No": 216,
  "judul": "Making Standards for Smart Mining Operations: Intelligent Vehicles for Autonomous Mining Transportation",
  "abstrak": "In face of the safety risk and ageing workforce in traditional mining industry, smart mining is becoming the new trend, especially the autonomous mining, which could completely address the safety issue while boosting efficiency. However, the lack of specific standards for the smart mining has led to the various types of equipment and high development costs. Therefore, we present the effort by Chinese mining research organizations and industrial enterprises in creating standards for automation and autonomy. Besides, we introduce a few key intelligent technologies for the autonomous mining transportation, and several current applications of autonomous trucks in mines around the world. These key intelligent technologies are likely to be further improved with following standards and become more affordable for mines.",
  "keyword": "data mining"
 },
 {
  "No": 217,
  "judul": "Research on Web Data Mining Based on Topic Crawler",
  "abstrak": "This paper analyzes the method of Web information data mining based on topic crawler. This paper puts forward the architecture of Web information search and data mining, and introduces the key technology and operation principle of the architecture. After analyzing the functions and shortcomings of ordinary crawler, this paper focuses on the working principle, implementation method and performance analysis of this crawler, as well as the functions of this crawler different from other crawlers and its application in Web information search and data mining system. The experimental results show that the crawler can get all kinds of information resources on the world wide web, which is helpful to the monitoring and management of network cultural content.",
  "keyword": "data mining"
 },
 {
  "No": 218,
  "judul": "Process Mining of Mining Processes: Analyzing Longwall Coal Excavation Using Event Data",
  "abstrak": "The mining industry faces many challenges, prompting the adoption of new technologies and continuous improvement of processes to improve operational efficiency and personnel safety. Using data from information systems combined with novel process mining (PM) techniques creates new possibilities for improving industrial processes. This article presents a comprehensive method of modeling and analyzing the longwall process in underground mining based on event data using process mining (PM4LMP). The method comprises four basic steps: 1) data gathering; 2) data preprocessing; 3) creation of event logs; and 4) PM tasks. In our method, we proposed, among all, case ID identification based on heuristics using context data and activity identification with supervised and unsupervised approaches, which provide complementary information about process execution. The method assumes an in-depth analysis of processes based on sensor data and knowledge gathered in IT systems, which can significantly improve the quality of information at managers’ disposal when making decisions regarding the mining process.",
  "keyword": "data mining"
 },
 {
  "No": 219,
  "judul": "Linked Data Analytics in Interdisciplinary Studies: The Health Impact of Air Pollution in Urban Areas",
  "abstrak": "The design of solutions that are able to exploit the available data collected in smart cities environments can lead to insights that can guide the implementation of approaches that have the potential to significantly improve the quality of life within a city. Such solutions include tools for the production of advanced analytics considering data fusion challenges. The preparation of qualitative input data sets, collected in many cases through heterogeneous sources and represented in various formats, constitute a very important step toward a meaningful analysis. Such input data sets, combined with approaches that reduce the data processing burden and support the easy and flexible-in terms of configuration-replication of an analysis, can lead to the next generation analytics tools. In this paper, a novel approach toward the production and consumption of linked data analytics in urban environments is presented. The approach is based on the exploitation of linked data principles, enhancing the ability of managing and processing of data, in ways not available before. In addition to the description of the overall technical approach, the application of the proposed solution into a real-life scenario for examining the health impact of outdoor air pollution in urban areas within an international, national, and regional perspective is detailed. A set of interesting results are produced along with their interpretation toward the provision of suggestions for policy making purposes.",
  "keyword": "data mining"
 },
 {
  "No": 220,
  "judul": "Data mining with big data",
  "abstrak": "Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",
  "keyword": "data mining"
 },
 {
  "No": 221,
  "judul": "On the Efficient Representation of Datasets as Graphs to Mine Maximal Frequent Itemsets",
  "abstrak": "Frequent itemsets mining is an active research problem in the domain of data mining and knowledge discovery. With the advances in database technology and an exponential increase in data to be stored, there is a need for efficient approaches that can quickly extract useful information from such large datasets. Frequent Itemsets (FIs) mining is a data mining task to find itemsets in a transactional database which occur together above a certain frequency. Finding these FIs usually requires multiple passes over the databases; therefore, making efficient algorithms crucial for mining FIs. This work presents a graph-based approach for representing a complete transactional database. The proposed graph-based representation enables the storing of all relevant information (for extracting FIs) of the database in one pass. Later, an algorithm that extracts the FIs from the graph-based structure is presented. Experimental results are reported comparing the proposed approach with 17 related FIs mining methods using six benchmark datasets. Results show that the proposed approach performs better than others in terms of time.",
  "keyword": "data mining"
 },
 {
  "No": 222,
  "judul": "CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories",
  "abstrak": "CRISP-DM(CRoss-Industry Standard Process for Data Mining) has its origins in the second half of the nineties and is thus about two decades old. According to many surveys and user polls it is still the de facto standard for developing data mining and knowledge discovery projects. However, undoubtedly the field has moved on considerably in twenty years, with data science now the leading term being favoured over data mining. In this paper we investigate whether, and in what contexts, CRISP-DM is still fit for purpose for data science projects. We argue that if the project is goal-directed and process-driven the process model view still largely holds. On the other hand, when data science projects become more exploratory the paths that the project can take become more varied, and a more flexible model is called for. We suggest what the outlines of such a trajectory-based model might look like and how it can be used to categorise data science projects (goal-directed, exploratory or data management). We examine seven real-life exemplars where exploratory activities play an important role and compare them against 51 use cases extracted from the NIST Big Data Public Working Group. We anticipate this categorisation can help project planning in terms of time and cost characteristics.",
  "keyword": "data mining"
 },
 {
  "No": 223,
  "judul": "Information Security in Big Data: Privacy and Data Mining",
  "abstrak": "The growing popularity and development of data mining technologies bring serious threat to the security of individual,'s sensitive information. An emerging research topic in data mining, known as privacy-preserving data mining (PPDM), has been extensively studied in recent years. The basic idea of PPDM is to modify the data in such a way so as to perform data mining algorithms effectively without compromising the security of sensitive information contained in the data. Current studies of PPDM mainly focus on how to reduce the privacy risk brought by data mining operations, while in fact, unwanted disclosure of sensitive information may also happen in the process of data collecting, data publishing, and information (i.e., the data mining results) delivering. In this paper, we view the privacy issues related to data mining from a wider perspective and investigate various approaches that can help to protect sensitive information. In particular, we identify four different types of users involved in data mining applications, namely, data provider, data collector, data miner, and decision maker. For each type of user, we discuss his privacy concerns and the methods that can be adopted to protect sensitive information. We briefly introduce the basics of related research topics, review state-of-the-art approaches, and present some preliminary thoughts on future research directions. Besides exploring the privacy-preserving approaches for each type of user, we also review the game theoretical approaches, which are proposed for analyzing the interactions among different users in a data mining scenario, each of whom has his own valuation on the sensitive information. By differentiating the responsibilities of different users with respect to security of sensitive information, we would like to provide some useful insights into the study of PPDM.",
  "keyword": "data mining"
 },
 {
  "No": 224,
  "judul": "Combined Mining: Discovering Informative Knowledge in Complex Data",
  "abstrak": "Enterprise data mining applications often involve complex data such as multiple large heterogeneous data sources, user preferences, and business impact. In such situations, a single method or one-step mining is often limited in discovering informative knowledge. It would also be very time and space consuming, if not impossible, to join relevant large data sources for mining patterns consisting of multiple aspects of information. It is crucial to develop effective approaches for mining patterns combining necessary information from multiple relevant business lines, catering for real business settings and decision-making actions rather than just providing a single line of patterns. The recent years have seen increasing efforts on mining more informative patterns, e.g., integrating frequent pattern mining with classifications to generate frequent pattern-based classifiers. Rather than presenting a specific algorithm, this paper builds on our existing works and proposes combined mining as a general approach to mining for informative patterns combining components from either multiple data sets or multiple features or by multiple methods on demand. We summarize general frameworks, paradigms, and basic processes for multifeature combined mining, multisource combined mining, and multimethod combined mining. Novel types of combined patterns, such as incremental cluster patterns, can result from such frameworks, which cannot be directly produced by the existing methods. A set of real-world case studies has been conducted to test the frameworks, with some of them briefed in this paper. They identify combined patterns for informing government debt prevention and improving government service objectives, which show the flexibility and instantiation capability of combined mining in discovering informative knowledge in complex data.",
  "keyword": "data mining"
 },
 {
  "No": 225,
  "judul": "Efficient Sentinel Mining Using Bitmaps on Modern Processors",
  "abstrak": "This paper proposes a highly efficient bitmap-based approach for discovery of so-called sentinels. Sentinels represent schema level relationships between changes over time in certain measures in a multidimensional data cube. Sentinels are actionable and notify users based on previous observations, for example, that revenue might drop within two months if an increase in customer problems combined with a decrease in website traffic is observed. We significantly extend prior art by representing the sentinel mining problem by bitmap operations, using bitmapped encoding of so-called indication streams. We present a very efficient algorithm, SentBit, that is 2-3 orders of magnitude faster than the state of the art, and utilizes CPU specific instructions and the multicore architectures available on modern processors. The SentBit algorithm scales efficiently to very large data sets, which is verified by extensive experiments on both real and synthetic data.",
  "keyword": "data mining"
 },
 {
  "No": 226,
  "judul": "Mining High Utility Itemsets Using Bio-Inspired Algorithms: A Diverse Optimal Value Framework",
  "abstrak": "Mining high utility itemsets (HUI) is an interesting research problem in the field of data mining and knowledge discovery. Recently, bio-inspired computing has attracted considerable attention, leading to the development of new algorithms for mining HUIs. These algorithms have shown good performance in terms of efficiency, but are not guaranteed to find all HUIs in a database. That is, the quality is comparatively poor in terms of the number of discovered HUIs. To solve this problem, a new framework based on bio-inspired algorithms is proposed. This approach adjusts the standard roadmap of bio-inspired algorithms by proportionally selecting discovered HUIs as the target values of the next population, rather than maintaining the current optimal values in the next population. Thus, the diversity within populations can be improved. Three new algorithms based on the Bio-HUI framework are developed using the genetic algorithm, particle swarm optimization, and the bat algorithm, respectively. Extensive tests conducted on publicly available datasets show that the proposed algorithms outperform existing state-of-the-art algorithms in terms of efficiency, quality of results, and convergence speed.",
  "keyword": "data mining"
 },
 {
  "No": 227,
  "judul": "Apriori Versions Based on MapReduce for Mining Frequent Patterns on Big Data",
  "abstrak": "Pattern mining is one of the most important tasks to extract meaningful and useful information from raw data. This task aims to extract item-sets that represent any type of homogeneity and regularity in data. Although many efficient algorithms have been developed in this regard, the growing interest in data has caused the performance of existing pattern mining techniques to be dropped. The goal of this paper is to propose new efficient pattern mining algorithms to work in big data. To this aim, a series of algorithms based on the MapReduce framework and the Hadoop open-source implementation have been proposed. The proposed algorithms can be divided into three main groups. First, two algorithms [Apriori MapReduce (AprioriMR) and iterative AprioriMR] with no pruning strategy are proposed, which extract any existing itemset in data. Second, two algorithms (space pruning AprioriMR and top AprioriMR) that prune the search space by means of the well-known anti-monotone property are proposed. Finally, a last algorithm (maximal AprioriMR) is also proposed for mining condensed representations of frequent patterns. To test the performance of the proposed algorithms, a varied collection of big data datasets have been considered, comprising up to 3·1018 transactions and more than 5 million of distinct single-items. The experimental stage includes comparisons against highly efficient and well-known pattern mining algorithms. Results reveal the interest of applying MapReduce versions when complex problems are considered, and also the unsuitability of this paradigm when dealing with small data.",
  "keyword": "data mining"
 },
 {
  "No": 228,
  "judul": "Big Data Platform for Educational Analytics",
  "abstrak": "Huge amounts of educational data are being produced, and a common challenge that many educational organizations confront, is finding an effective method to harness and analyze this data for continuously delivering enhanced education. Nowadays, the educational data is evolving and has become large in volume, wide in variety and high in velocity. This produced data needs to be handled in an efficient manner to extract value and make informed decisions. For that, this paper confronts such data as a big data challenge and presents a comprehensive platform tailored to perform educational big data analytical applications. Further, present an effective environment for non-data scientists and people in the educational sector to apply their demanding educational big data applications. The implementation stages of the educational big data platform on a cloud computing platform and the organization of educational data in a data lake architecture are highlighted. Furthermore, two analytical applications are performed to test the feasibility of the presented platform in discovering knowledge that potentially promotes the educational institutions.",
  "keyword": "data mining"
 },
 {
  "No": 229,
  "judul": "Exploratory Data Mining for Subgroup Cohort Discoveries and Prioritization",
  "abstrak": "Finding small homogeneous subgroup cohorts in large heterogeneous populations is a critical process for hypothesis development in biomedical research. Concurrent computational approaches are still lacking in robust answers to the question “what hypotheses are likely to be novel and to produce clinically relevant results with well thought-out study designs?” We have developed a novel subgroup discovery method which employs a deep exploratory mining process to slice and dice thousands of potential subpopulations and prioritize potential cohorts based on their explainable contrast patterns and which may provide interventionable insights. We conducted computational experiments on both synthesized data and a clinical autism data set to assess performance quantitatively for coverage of pre-defined cohorts and qualitatively for novel knowledge discovery, respectively. We also conducted a scaling analysis using a distributed computing environment to suggest computational resource needs for when the subpopulation number increases. This work will provide a robust data-driven framework to automatically tailor potential interventions for precision health.",
  "keyword": "data mining"
 },
 {
  "No": 230,
  "judul": "A Distributed Method for Fast Mining Frequent Patterns From Big Data",
  "abstrak": "In recent years, knowledge discovery in databases provides a powerful capability to discover meaningful and useful information. For numerous real-life applications, frequent pattern mining and association rule mining have been extensively studied. In traditional mining algorithms, data are centralized and memory-resident. As a result of the large amount of data, bandwidth limitation, and energy limitations when applying these methods to distributed databases, especially in this era of big data, the performance is not effective enough. Hence, data mining on distributed environments has emerged as an important research area. To improve the performance, we propose a set of algorithms based on FP growth that discover FPs that are capable of providing fast and scalable service in distributed computing environments and a brief data structure to store items and counts to minimize the data for transmission on the network. To ensure completeness and execution capability, DistEclat and BigFIM were considered for the experiment comparison. Experiments show that the proposed method has superior cost-effectiveness for processing massive datasets and good capabilities under various experiment conditions. The proposed method on average required only 33% of the execution time and 45% of the transmission cost of DistEclat. Compared to BigFIM, The proposed method on average required 23.3% of the execution time and 14.2% of the transmission cost of BigFIM.",
  "keyword": "data mining"
 },
 {
  "No": 231,
  "judul": "Deep Learning for Spatio-Temporal Data Mining: A Survey",
  "abstrak": "With the fast development of various positioning techniques such as Global Position System (GPS), mobile devices and remote sensing, spatio-temporal data has become increasingly available nowadays. Mining valuable knowledge from spatio-temporal data is critically important to many real-world applications including human mobility understanding, smart transportation, urban planning, public safety, health care and environmental management. As the number, volume and resolution of spatio-temporal data increase rapidly, traditional data mining methods, especially statistics-based methods for dealing with such data are becoming overwhelmed. Recently deep learning models such as recurrent neural network (RNN) and convolutional neural network (CNN) have achieved remarkable success in many domains due to the powerful ability in automatic feature representation learning, and are also widely applied in various spatio-temporal data mining (STDM) tasks such as predictive learning, anomaly detection and classification. In this paper, we provide a comprehensive review of recent progress in applying deep learning techniques for STDM. We first categorize the spatio-temporal data into five different types, and then briefly introduce the deep learning models that are widely used in STDM. Next, we classify existing literature based on the types of spatio-temporal data, the data mining tasks, and the deep learning models, followed by the applications of deep learning for STDM in different domains including transportation, on-demand service, climate & weather analysis, human mobility, location-based social network, crime analysis, and neuroscience. Finally, we conclude the limitations of current research and point out future research directions.",
  "keyword": "data mining"
 },
 {
  "No": 232,
  "judul": "Random Sample Partition: A Distributed Data Model for Big Data Analysis",
  "abstrak": "With the ever-increasing volume of data, alternative strategies are required to divide big data into statistically consistent data blocks that can be used directly as representative samples of the entire data set in big data analysis. In this paper, we propose the Random Sample Partition (RSP) distributed data model to represent a big data set as a set of disjoint data blocks, called RSP blocks. Each RSP block has a probability distribution similar to that of the entire data set. RSP blocks can be used to estimate the statistical properties of the data and build predictive models without computing the entire data set. We demonstrate the implications of the RSP model on sampling from big data and introduce a new RSP-based method for approximate big data analysis which can be applied to different scenarios in the industry. This method significantly reduces the computational burden of big data and increases the productivity of data scientists.",
  "keyword": "data mining"
 },
 {
  "No": 233,
  "judul": "Urban Flow Pattern Mining Based on Multi-Source Heterogeneous Data Fusion and Knowledge Graph Embedding",
  "abstrak": "Urban flow analysis is an essential research for smart city construction, in which urban flow pattern analysis focuses on the continuous state of urban flow. How to mine, store and reuse traffic patterns from urban multi-source heterogeneous big data is challenging. Therefore, this paper proposes a knowledge mining network for regional flow pattern to mine and store the urban flow pattern. The proposed model consists of two modules. In the first module, the features of the region and its flow pattern are extracted as the entity and relation, respectively. In the second module, POI features are modeled to enhance the embedding representation of relation and entity. Based on the translation distance method, the knowledge triplets of regional flow patterns are mined. Finally, the proposed model is compared with some benchmark methods using Chengdu Didi order and POI datasets. Experimental results show that the proposed model is effective. In addition, the knowledge triplets are visualized and some application examples are introduced.",
  "keyword": "data mining"
 },
 {
  "No": 234,
  "judul": "Data-Driven Rule Mining and Representation of Temporal Patterns in Physiological Sensor Data",
  "abstrak": "Mining and representation of qualitative patterns is a growing field in sensor data analytics. This paper leverages from rule mining techniques to extract and represent temporal relation of prototypical patterns in clinical data streams. The approach is fully data-driven, where the temporal rules are mined from physiological time series such as heart rate, respiration rate, and blood pressure. To validate the rules, a novel similarity method is introduced, that compares the similarity between rule sets. An additional aspect of the proposed approach has been to utilize natural language generation techniques to represent the temporal relations between patterns. In this study, the sensor data in the MIMIC online database was used for evaluation, in which the mined temporal rules as they relate to various clinical conditions (respiratory failure, angina, sepsis, ...) were made explicit as a textual representation. Furthermore, it was shown that the extracted rule set for any particular clinical condition was distinct from other clinical conditions.",
  "keyword": "data mining"
 },
 {
  "No": 235,
  "judul": "Knowledge Fusion for Probabilistic Generative Classifiers with Data Mining Applications",
  "abstrak": "If knowledge such as classification rules are extracted from sample data in a distributed way, it may be necessary to combine or fuse these rules. In a conventional approach this would typically be done either by combining the classifiers' outputs (e.g., in form of a classifier ensemble) or by combining the sets of classification rules (e.g., by weighting them individually). In this paper, we introduce a new way of fusing classifiers at the level of parameters of classification rules. This technique is based on the use of probabilistic generative classifiers using multinomial distributions for categorical input dimensions and multivariate normal distributions for the continuous ones. That means, we have distributions such as Dirichlet or normal-Wishart distributions over parameters of the classifier. We refer to these distributions as hyperdistributions or second-order distributions. We show that fusing two (or more) classifiers can be done by multiplying the hyperdistributions of the parameters and derive simple formulas for that task. Properties of this new approach are demonstrated with a few experiments. The main advantage of this fusion approach is that the hyperdistributions are retained throughout the fusion process. Thus, the fused components may, for example, be used in subsequent training steps (online training).",
  "keyword": "data mining"
 },
 {
  "No": 236,
  "judul": "Medical Data Stream Distribution Pattern Association Rule Mining Algorithm Based on Density Estimation",
  "abstrak": "The traditional data mining method is featured by no analysis over the data distribution and incomplete derived association rule. As a result, the data mining results have the deficiencies of large redundancy probability, large root-mean-square error of approximation (RMSEA) and long consumption time. To handle these issues, this paper proposes a medical data stream distribution pattern association rule mining algorithm based on density estimation. This paper collects medical data, selects the distance method to detect abnormal orphan data in the data stream, detects the duplicate data in the data stream by the similar field matching degree, and eliminates the abnormal data and the duplicate data. Then, the data stream density is estimated based on the histogram estimation samples. According to the data density estimation results, this paper analyzes the distribution of medical data stream from perspectives of concentration, dispersion and morphological characteristics of data distribution. Afterwards, the data distribution pattern association rule mining model is constructed based on compound neural network, data distribution parameters are entered into model’s clustering layer, and in-depth training is conducted over the BP (Back Propagation) neural network at the model’s mining layer. Meanwhile, all rules under the combination of hidden layer’s neuron activity value and corresponding output value, and all rules under the combination of hidden layer’s neuron activity value and corresponding input value are derived, so as to complete association rule mining of medical data stream distribution pattern. The experimental results show that the proposed algorithm has a contour curve closest to the true probability density curve; the dispersion degree of medical data is within a reasonable range, and the medical data has high stability; the data redundancy probability is smaller; the mining result’s RMSEA is small; data mining takes less time.",
  "keyword": "data mining"
 },
 {
  "No": 237,
  "judul": "Mining High Utility Patterns in One Phase without Generating Candidates",
  "abstrak": "Utility mining is a new development of data mining technology. Among utility mining problems, utility mining with the itemset share framework is a hard one as no anti-monotonicity property holds with the interestingness measure. Prior works on this problem all employ a two-phase, candidate generation approach with one exception that is however inefficient and not scalable with large databases. The two-phase approach suffers from scalability issue due to the huge number of candidates. This paper proposes a novel algorithm that finds high utility patterns in a single phase without generating candidates. The novelties lie in a high utility pattern growth approach, a lookahead strategy, and a linear data structure. Concretely, our pattern growth approach is to search a reverse set enumeration tree and to prune search space by utility upper bounding. We also look ahead to identify high utility patterns without enumeration by a closure property and a singleton property. Our linear data structure enables us to compute a tight bound for powerful pruning and to directly identify high utility patterns in an efficient and scalable way, which targets the root cause with prior algorithms. Extensive experiments on sparse and dense, synthetic and real world data suggest that our algorithm is up to 1 to 3 orders of magnitude more efficient and is more scalable than the state-of-the-art algorithms.",
  "keyword": "data mining"
 },
 {
  "No": 238,
  "judul": "Association Discovery in Two-View Data",
  "abstrak": "Two-view datasets are datasets whose attributes are naturally split into two sets, each providing a different view on the same set of objects. We introduce the task of finding small and non-redundant sets of associations that describe how the two views are related. To achieve this, we propose a novel approach in which sets of rules are used to translate one view to the other and vice versa. Our models, dubbed translation tables, contain both unidirectional and bidirectional rules that span both views and provide lossless translation from either of the views to the opposite view. To be able to evaluate different translation tables and perform model selection, we present a score based on the Minimum Description Length (MDL) principle. Next, we introduce three TRANSLATOR algorithms to find good models according to this score. The first algorithm is parameter-free and iteratively adds the rule that improves compression most. The other two algorithms use heuristics to achieve better trade-offs between runtime and compression. The empirical evaluation on real-world data demonstrates that only modest numbers of associations are needed to characterize the two-view structure present in the data, while the obtained translation rules are easily interpretable and provide insight into the data.",
  "keyword": "data mining"
 },
 {
  "No": 239,
  "judul": "The Use of Intelligent Vehicles and Artificial Intelligence in Mining Operations: Ethics, Responsibility, and Sustainability",
  "abstrak": "This letter is resulted from IEEE TIV's Decentralized and Hybrid Workshops (DHW) on Autonomous Mining (AM). We have already conducted 2 distributed\/decentralized and hybrid symposia (DHS), 5 DHWs, and more than 10 seminars on AM in the past year. The following is a brief summary on the key components from our DHS, DHWs, and Seminars about ethics, responsibility, and sustainability in intelligent vehicles (IVs) and artificial intelligence (AI) for AM (Wang, 2022), (Cao et al., 2022).",
  "keyword": "data mining"
 },
 {
  "No": 240,
  "judul": "Revealing Land Surface Deformation Over the Yineng Backfilling Mining Area, China, by Integrating Distributed Scatterer SAR Interferometry and a Mining Subsidence Model",
  "abstrak": "Monitoring the surface deformation of filling coal mines regularly and understanding their spatiotemporal evolution characteristics for mining management and disaster warning is greatly significant. However, there is a lack of research on extracting spatiotemporal evolution characteristics of large-scale and high-resolution surface deformation in backfill mining areas and on evaluating the filling effectiveness of subsidence restraint. In this study, we took the Yineng Coal Mine in the Shandong Province of China and the surrounding area of the coal mine as the study area. The advanced distributed scatterer interferometric synthetic aperture radar (DS InSAR) technique was adopted for time-series analysis. The probability integral method (PIM) model for backfilling mining and an arctangent time function were integrated with DS InSAR to overcome the sparsity of InSAR observation points due to the temporal decorrelation caused by vegetation coverage. The results show that the proposed integration strategy is helpful in improving the number of effective monitoring points and obtaining complete spatiotemporal information of the surface deformation of the working face. The whole study area has eight key deformation zones. All six working faces in the Yineng initial minery display different degrees of deformation during the study period. The comparison between the deformation results of backfill mining and the simulated deformation of nonbackfill mining in the CG1312 working face shows that backfilling mining technology effectively reduces the range (22.00%) and magnitude (61.50%) of surface subsidence and significantly reduces the potential threat to surface buildings.",
  "keyword": "data mining"
 },
 {
  "No": 241,
  "judul": "Big Data Analytics and Mining for Effective Visualization and Trends Forecasting of Crime Data",
  "abstrak": "Big data analytics (BDA) is a systematic approach for analyzing and identifying different patterns, relations, and trends within a large volume of data. In this paper, we apply BDA to criminal data where exploratory data analysis is conducted for visualization and trends prediction. Several the state-of-the-art data mining and deep learning techniques are used. Following statistical analysis and visualization, some interesting facts and patterns are discovered from criminal data in San Francisco, Chicago, and Philadelphia. The predictive results show that the Prophet model and Keras stateful LSTM perform better than neural network models, where the optimal size of the training data is found to be three years. These promising outcomes will benefit for police departments and law enforcement organizations to better understand crime issues and provide insights that will enable them to track activities, predict the likelihood of incidents, effectively deploy resources and optimize the decision making process.",
  "keyword": "data mining"
 },
 {
  "No": 242,
  "judul": "Share-Frequent Sensor Patterns Mining from Wireless Sensor Network Data",
  "abstrak": "Mining interesting knowledge from the huge amount of data gathered from WSNs is a challenge. Works reported in literature use support metric-based sensor association rules which employ the occurrence frequency of patterns as criteria. However, consideration of the binary frequency of a pattern is not a sufficient indicator for finding meaningful patterns because it only reflects the number of epochs which contain that pattern in the dataset. The share measure of sensorsets could discover useful knowledge about trigger values associated with a sensor. Here, we propose a new type of behavioral pattern called share-frequent sensor patterns (SFSPs) by considering the non-binary frequency values of sensors in epochs. SFSPs can find a correlation among a set of sensors and hence can improve the performance of WSNs in a resource management process. In this paper, a share-frequent sensor pattern tree (ShrFSP-tree) has been proposed to facilitate a pattern growth mining technique to discover SFSPs from WSN data. We also present a parallel and distributed method where the ShrFSP-tree is enhanced into PShrFSP-tree and its performance is investigated for both homogeneous and heterogeneous systems. Results show that our method is time and memory efficient in finding SFSPs than the existing most efficient algorithms.",
  "keyword": "data mining"
 },
 {
  "No": 243,
  "judul": "DAG: A General Model for Privacy-Preserving Data Mining",
  "abstrak": "Secure multi-party computation (SMC) allows parties to jointly compute a function over their inputs, while keeping every input confidential. It has been extensively applied in tasks with privacy requirements, such as privacy-preserving data mining (PPDM), to learn task output and at the same time protect input data privacy. However, existing SMC-based solutions are ad-hoc - they are proposed for specific applications, and thus cannot be applied to other applications directly. To address this issue, we propose a privacy model DAG (Directed Acyclic Graph) that consists of a set of fundamental secure operators (e.g., +, -, ×, \/, and power). Our model is general - its operators, if pipelined together, can implement various functions, even complicated ones like Naı̈ve Bayes classifier. It is also extendable - new secure operators can be defined to expand the functions that the model supports. For case study, we have applied our DAG model to two data mining tasks: kernel regression and Naı̈ve Bayes. Experimental results show that DAG generates outputs that are almost the same as those by non-private setting, where multiple parties simply disclose their data. The experimental results also show that our DAG model runs in acceptable time, e.g., in kernel regression, when training data size is 683,093, one prediction in non-private setting takes 5.93 sec, and that by our DAG model takes 12.38 sec.",
  "keyword": "data mining"
 },
 {
  "No": 244,
  "judul": "Mining Temporal Patterns in Time Interval-Based Data",
  "abstrak": "Sequential pattern mining is an important subfield in data mining. Recently, applications using time interval-based event data have attracted considerable efforts in discovering patterns from events that persist for some duration. Since the relationship between two intervals is intrinsically complex, how to effectively and efficiently mine interval-based sequences is a challenging issue. In this paper, two novel representations, endpoint representation and endtime representation, are proposed to simplify the processing of complex relationships among event intervals. Based on the proposed representations, three types of interval-based patterns: temporal pattern, occurrence-probabilistic temporal pattern, and duration-probabilistic temporal pattern, are defined. In addition, we develop two novel algorithms, Temporal Pattern Miner (TPMiner) and Probabilistic Temporal Pattern Miner (P-TPMiner), to discover three types of interval-based sequential patterns. We also propose three pruning techniques to further reduce the search space of the mining process. Experimental studies show that both algorithms are able to find three types of patterns efficiently. Furthermore, we apply proposed algorithms to real datasets to demonstrate the effectiveness and validate the practicability of proposed patterns.",
  "keyword": "data mining"
 },
 {
  "No": 245,
  "judul": "Trust-but-Verify: Verifying Result Correctness of Outsourced Frequent Itemset Mining in Data-Mining-As-a-Service Paradigm",
  "abstrak": "Cloud computing is popularizing the computing paradigm in which data is outsourced to a third-party service provider (server) for data mining. Outsourcing, however, raises a serious security issue: how can the client of weak computational power verify that the server returned correct mining result? In this paper, we focus on the specific task of frequent itemset mining. We consider the server that is potentially untrusted and tries to escape from verification by using its prior knowledge of the outsourced data. We propose efficient probabilistic and deterministic verification approaches to check whether the server has returned correct and complete frequent itemsets. Our probabilistic approach can catch incorrect results with high probability, while our deterministic approach measures the result correctness with 100 percent certainty. We also design efficient verification methods for both cases that the data and the mining setup are updated. We demonstrate the effectiveness and efficiency of our methods using an extensive set of empirical results on real datasets.",
  "keyword": "data mining"
 },
 {
  "No": 246,
  "judul": "Harnessing Multi-Source Data about Public Sentiments and Activities for Informed Design",
  "abstrak": "The intelligence of Smart Cities (SC) is represented by its ability in collecting, managing, integrating, analyzing, and mining multi-source data for valuable insights. In order to harness multi-source data for an informed place design, this paper presents “Public Sentiments and Activities in Places” multi-source data analysis flow (PSAP) in an Informed Design Platform (IDP). In terms of key contributions, PSAP implements 1) an Interconnected Data Model (IDM) to manage multi-source data independently and integrally, 2) an efficient and effective data mining mechanism based on multi-dimension and multi-measure queries (MMQs), and 3) concurrent data processing cascades with Sentiments in Places Analysis Mechanism (SPAM) and Activities in Places Analysis Mechanism (APAM), to fuse social network data with other data on public sentiment and activity comprehensively. As proved by a holistic evaluation, both SPAM and APAM outperform compared methods. Specifically, SPAM improves its classification accuracy gradually and significantly from 72.37 to about 85 percent within nine crowd-calibration cycles, and APAM with an ensemble classifier achieves the highest precision of 92.13 percent, which is approximately 13 percent higher than the second best method. Finally, by applying MMQs on “Sentiment&Activity Linked Data”, various place design insights of our testbed are mined to improve its livability.",
  "keyword": "data mining"
 },
 {
  "No": 247,
  "judul": "Design and Analysis of Low Delay Deterministic Network Based on Data Mining Association Analysis",
  "abstrak": "The purpose of this paper is to research on the design and analysis of low delay deterministic network based on data mining association. This paper studies and implements the algorithm of mining page association rules. A session recognition algorithm based on log reference page and request time is proposed by using the time probability relationship of continuous requests. This method improves the accuracy of log data preprocessing and page association rules mining. This paper studies and tests the efficiency, prefetch timing and cache organization of the two page association rules. The results show that in this prefetch scheme, the prefetch performance of the association rules of unordered pages is better than that of the association rules of ordered pages. The prefetch performance when cache hits is better than that when cache fails and the cache fails to hit has better performance. In the case of a certain size of cache space, reasonable organization of cache space can further improve the cache hit rate and reduce the network delay.",
  "keyword": "data mining"
 },
 {
  "No": 248,
  "judul": "Accelerated PSO Swarm Search Feature Selection for Data Stream Mining Big Data",
  "abstrak": "Big Data though it is a hype up-springing many technical challenges that confront both academic research communities and commercial IT deployment, the root sources of Big Data are founded on data streams and the curse of dimensionality. It is generally known that data which are sourced from data streams accumulate continuously making traditional batch-based model induction algorithms infeasible for real-time data mining. Feature selection has been popularly used to lighten the processing load in inducing a data mining model. However, when it comes to mining over high dimensional data the search space from which an optimal feature subset is derived grows exponentially in size, leading to an intractable demand in computation. In order to tackle this problem which is mainly based on the high-dimensionality and streaming format of data feeds in Big Data, a novel lightweight feature selection is proposed. The feature selection is designed particularly for mining streaming data on the fly, by using accelerated particle swarm optimization (APSO) type of swarm search that achieves enhanced analytical accuracy within reasonable processing time. In this paper, a collection of Big Data with exceptionally large degree of dimensionality are put under test of our new feature selection algorithm for performance evaluation.",
  "keyword": "data mining"
 },
 {
  "No": 249,
  "judul": "Detection and Prediction of Diabetes Using Data Mining: A Comprehensive Review",
  "abstrak": "Diabetes is one of the most rapidly growing chronic diseases, which has affected millions of people around the globe. Its diagnosis, prediction, proper cure, and management are crucial. Data mining based forecasting techniques for data analysis of diabetes can help in the early detection and prediction of the disease and the related critical events such as hypo\/hyperglycemia. Numerous techniques have been developed in this domain for diabetes detection, prediction, and classification. In this paper, we present a comprehensive review of the state-of-the-art in the area of diabetes diagnosis and prediction using data mining. The aim of this paper is twofold; firstly, we explore and investigate the data mining based diagnosis and prediction solutions in the field of glycemic control for diabetes. Secondly, in the light of this investigation, we provide a comprehensive classification and comparison of the techniques that have been frequently used for diagnosis and prediction of diabetes based on important key metrics. Moreover, we highlight the challenges and future research directions in this area that can be considered in order to develop optimized solutions for diabetes detection and prediction.",
  "keyword": "data mining"
 },
 {
  "No": 250,
  "judul": "Data-Pattern Enabled Self-Recovery Low-Power Storage System for Big Video Data",
  "abstrak": "The growing popularity of powerful mobile devices such as smart phones and tablet devices has resulted in the exponential growth of demand for video applications. However, due to the large video data size and intensive computation, mobile video applications require frequent embedded memory access, which consumes a large amount of power and limits battery life. In this paper, we present a low-cost self-recovery video storage system by investigating meaningful data patterns hidden in big video data, by introducing data mining techniques to the hardware design process. We propose a two-dimensional data-pattern approach to explore horizontal data-association and vertical data-correlation characteristics. Such data relationship discovery and pattern identification enable a new dimension for the hardware design space and bring self-recovery ability to memories in the presence of bitcell failures. Based on the identified optimal data patterns, we present a low-cost and efficient SRAM design to enable data self-recovery at low voltages. A 45nm 32 kb SRAM is implemented that delivers good video quality at near-threshold voltage (0.5 V) with negligible area overhead (7.94 percent).",
  "keyword": "data mining"
 },
 {
  "No": 251,
  "judul": "A Data-Driven Approach to Improve the Operation and Maintenance Management of Large Public Buildings",
  "abstrak": "With the development of modern information technologies and more frequent utilization of information systems to operation and maintenance (O&M) management, a great amount of O&M data are collected nowadays. However, because of the large volume and poor quality, as well as a lack of effective data analysis techniques, these data are rarely analyzed and translated into useful knowledge for O&M decisions. This study presents a data model, which is named as datacube with multi-dimensional and unrestrained characteristics, for these data to better support data mining algorithms. The model organizes all the different data in both relational database and in the memories and is able to support analysis-requirements-oriented data extractions. Based on this datacube, an O&M data mining approach is proposed with procedures of data preparation, data clustering and data mining. The proposed datacube-based data mining approach was applied to the Kunming Chang Shui international airport terminal. More than 7 years on-site repairing data were used for data mining and the outcomes verified the model and the approach to be feasible and valuable for improving O&M management.",
  "keyword": "data mining"
 },
 {
  "No": 252,
  "judul": "A Methodology for Direct and Indirect Discrimination Prevention in Data Mining",
  "abstrak": "Data mining is an increasingly important technology for extracting useful knowledge hidden in large collections of data. There are, however, negative social perceptions about data mining, among which potential privacy invasion and potential discrimination. The latter consists of unfairly treating people on the basis of their belonging to a specific group. Automated data collection and data mining techniques such as classification rule mining have paved the way to making automated decisions, like loan granting\/denial, insurance premium computation, etc. If the training data sets are biased in what regards discriminatory (sensitive) attributes like gender, race, religion, etc., discriminatory decisions may ensue. For this reason, anti-discrimination techniques including discrimination discovery and prevention have been introduced in data mining. Discrimination can be either direct or indirect. Direct discrimination occurs when decisions are made based on sensitive attributes. Indirect discrimination occurs when decisions are made based on nonsensitive attributes which are strongly correlated with biased sensitive ones. In this paper, we tackle discrimination prevention in data mining and propose new techniques applicable for direct or indirect discrimination prevention individually or both at the same time. We discuss how to clean training data sets and outsourced data sets in such a way that direct and\/or indirect discriminatory decision rules are converted to legitimate (nondiscriminatory) classification rules. We also propose new metrics to evaluate the utility of the proposed approaches and we compare these approaches. The experimental evaluations demonstrate that the proposed techniques are effective at removing direct and\/or indirect discrimination biases in the original data set while preserving data quality.",
  "keyword": "data mining"
 },
 {
  "No": 253,
  "judul": "Integration of Data Mining Clustering Approach in the Personalized E-Learning System",
  "abstrak": "Educational data-mining is an evolving discipline that focuses on the improvement of self-learning and adaptive methods. It is used for finding hidden patterns or intrinsic structures of educational data. In the arena of education, the heterogeneous data is involving and continuously growing in the paradigm of big-data. To extract meaningful information adaptively from big educational data, some specific data mining techniques are needed. This paper presents a clustering approach to partition students into different groups or clusters based on their learning behavior. Furthermore, the personalized e-learning system architecture is presented, which detects and responds to teaching contents according to the students’ learning capabilities. The primary objective includes the discovery of optimal settings, in which the learners can improve their learning capabilities. Moreover, the administration can find essential hidden patterns to bring the effective reforms in the existing system. The clustering methods K-Means, K-Medoids, Density-based Spatial Clustering of Applications with Noise, Agglomerative Hierarchical Cluster Tree and Clustering by Fast Search and Finding of Density Peaks via Heat Diffusion (CFSFDP-HD) are analyzed using educational data mining. It has been observed that more robust results can be achieved by the replacement of existing methods with CFSFDP-HD. The data mining techniques are equally effective in analyzing the big data to make education systems vigorous.",
  "keyword": "data mining"
 },
 {
  "No": 254,
  "judul": "A Survey of Utility-Oriented Pattern Mining",
  "abstrak": "The main purpose of data mining and analytics is to find novel, potentially useful patterns that can be utilized in real-world applications to derive beneficial knowledge. For identifying and evaluating the usefulness of different kinds of patterns, many techniques and constraints have been proposed, such as support, confidence, sequence order, and utility parameters (e.g., weight, price, profit, quantity, satisfaction, etc.). In recent years, there has been an increasing demand for utility-oriented pattern mining (UPM, or called utility mining). UPM is a vital task, with numerous high-impact applications, including cross-marketing, e-commerce, finance, medical, and biomedical applications. This survey aims to provide a general, comprehensive, and structured overview of the state-of-the-art methods of UPM. First, we introduce an in-depth understanding of UPM, including concepts, examples, and comparisons with related concepts. A taxonomy of the most common and state-of-the-art approaches for mining different kinds of high-utility patterns is presented in detail, including Apriori-based, tree-based, projection-based, vertical-\/horizontal-data-format-based, and other hybrid approaches. A comprehensive review of advanced topics of existing high-utility pattern mining techniques is offered, with a discussion of their pros and cons. Finally, we present several well-known open-source software packages for UPM. We conclude our survey with a discussion on open and practical challenges in this field.",
  "keyword": "data mining"
 },
 {
  "No": 255,
  "judul": "Exploratory Data Mining for Subgroup Cohort Discoveries and Prioritization",
  "abstrak": "Finding small homogeneous subgroup cohorts in large heterogeneous populations is a critical process for hypothesis development in biomedical research. Concurrent computational approaches are still lacking in robust answers to the question “what hypotheses are likely to be novel and to produce clinically relevant results with well thought-out study designs?” We have developed a novel subgroup discovery method which employs a deep exploratory mining process to slice and dice thousands of potential subpopulations and prioritize potential cohorts based on their explainable contrast patterns and which may provide interventionable insights. We conducted computational experiments on both synthesized data and a clinical autism data set to assess performance quantitatively for coverage of pre-defined cohorts and qualitatively for novel knowledge discovery, respectively. We also conducted a scaling analysis using a distributed computing environment to suggest computational resource needs for when the subpopulation number increases. This work will provide a robust data-driven framework to automatically tailor potential interventions for precision health.",
  "keyword": "data mining"
 },
 {
  "No": 256,
  "judul": "SecEDMO: Enabling Efficient Data Mining with Strong Privacy Protection in Cloud Computing",
  "abstrak": "Frequent itemsets mining and association rules mining are among the top used algorithms in the area of data mining. Secure outsourcing of data mining tasks to the third-party cloud is an effective option for data owners. However, due to the untrust cloud and the distrust between data owners, the traditional algorithms which only work over plaintext should be re-considered to take security and privacy concerns into account. For example, each data owner may not be willing to disclose their own private data to others during the cooperative data mining process. The previous solutions are either not sufficiently secure or not efficient. Therefore, we propose a \nSec\nure and \nE\nfficient \nD\nata \nM\nining \nO\nutsourcing (SecEDMO) scheme for secure outsourcing of frequent itemsets mining and association rules mining over the joint database (i.e., database aggregated from multiple data owners) in the paradigm of cloud computing. Based on our customized lightweight symmetric homomorphic encryption algorithm and a secure comparison algorithm, SecEDMO can ensure strong privacy protection and low data mining latency simultaneously. Moreover, the well-designed virtual transaction insertion algorithm can hide the information of the original database while still preserving the cloud’s ability to perform data mining over the obfuscated data. By evaluation of a numerical experiment and theoretical comparisons, the correctness, security, and efficiency of SecEDMO are confirmed.",
  "keyword": "data mining"
 },
 {
  "No": 257,
  "judul": "Scalable Daily Human Behavioral Pattern Mining from Multivariate Temporal Data",
  "abstrak": "This work introduces a set of scalable algorithms to identify patterns of human daily behaviors. These patterns are extracted from multivariate temporal data that have been collected from smartphones. We have exploited sensors that are available on these devices, and have identified frequent behavioral patterns with a temporal granularity, which has been inspired by the way individuals segment time into events. These patterns are helpful to both end-users and third parties who provide services based on this information. We have demonstrated our approach on two real-world datasets and showed that our pattern identification algorithms are scalable. This scalability makes analysis on resource constrained and small devices such as smartwatches feasible. Traditional data analysis systems are usually operated in a remote system outside the device. This is largely due to the lack of scalability originating from software and hardware restrictions of mobile\/wearable devices. By analyzing the data on the device, the user has the control over the data, i.e., privacy, and the network costs will also be removed.",
  "keyword": "data mining"
 },
 {
  "No": 258,
  "judul": "A Data-Driven Knowledge Acquisition System: An End-to-End Knowledge Engineering Process for Generating Production Rules",
  "abstrak": "Data-driven knowledge acquisition is one of the key research fields in data mining. Dealing with large amounts of data has received a lot of attention in the field recently, and a number of methodologies have been proposed to extract insights from data in an automated or semi-automated manner. However, these methodologies generally target a specific aspect of the data mining process, such as data acquisition, data preprocessing, or data classification. However, a comprehensive knowledge acquisition method is crucial to support the end-to-end knowledge engineering process. In this paper, we introduce a knowledge acquisition system that covers all major phases of the cross-industry standard process for data mining. Acknowledging the importance of an end-to-end knowledge engineering process, we designed and developed an easy-to-use data-driven knowledge acquisition tool (DDKAT). The major features of the DDKAT are: (1) a novel unified features scoring approach for data selection; (2) a user-friendly data processing interface to improve the quality of the raw data; (3) an appropriate decision tree algorithm selection approach to build a classification model; and (4) the generation of production rules from various decision tree classification models in an automated manner. Furthermore, two diabetes studies were performed to assess the value of the DDKAT in terms of user experience. A total of 19 experts were involved in the first study and 102 students in the artificial intelligence domain were involved in the second study. The results showed that the overall user experience of the DDKAT was positive in terms of its attractiveness, as well as its pragmatic and hedonic quality factors.",
  "keyword": "data mining"
 },
 {
  "No": 259,
  "judul": "Efficient Frequent Chronicle Mining Algorithms: Application to Sleep Disorder",
  "abstrak": "Sequential pattern mining is a dynamic and thriving research field that aims to extract recurring sequences of events from complex datasets. Traditionally, focusing solely on the order of events often falls short of providing precise insights. Consequently, incorporating the temporal intervals between events has emerged as a vital necessity across various domains, e.g. medicine. Analyzing temporal event sequences within patients’ clinical histories, drug prescriptions, and monitoring alarms exemplifies this critical need. This paper presents innovative and efficient methodologies for mining frequent chronicles from temporal data. The mined graphs offer a significantly more expressive representation than mere event sequences, capturing intricate details of a series of events in a factual manner. The experimental stage includes a series of analyses of diverse databases with distinct characteristics. The proposed approaches were also applied to real-world data comprising information about subjects suffering from sleep disorders. Alluring frequent complete event graphs were obtained on patients who were under the effect of sleep medication.",
  "keyword": "data mining"
 },
 {
  "No": 260,
  "judul": "Online Incremental Mining Based on Trusted Behavior Interval",
  "abstrak": "Incremental mining improves the quality of process mining by analyzing the differences between event logs and a reference model to obtain valuable information to update the reference model. Existing incremental mining methods focus on offline logs by setting thresholds for analysis, which limits process mining efforts by the domain knowledge, log completeness, and business completion time. Aiming at these problems, a real-time incremental mining algorithm based on the trusted behavior interval is proposed to analyze online event streams for updating the reference model. First, a clustering technique to analyze an existing reference model selects the core structure of the model and calculates the trusted behavior interval. Then, the behavioral and structural relationships between the online event streams and the reference model are analyzed to obtain a valid candidate set. Based on this set, an incremental update algorithm is proposed to optimize the model structure to achieve an online dynamic update of the reference model. The proposed algorithm is implemented in PM4PY and Scikit-learn frameworks; a reasonable number of clusters is determined using the elbow method and validated with artificial and real data. Experimental results show that the algorithm improves the efficiency of incremental mining and enhances the quality of the model with both complete and incomplete data.",
  "keyword": "data mining"
 },
 {
  "No": 261,
  "judul": "Aggregating Time Series and Tabular Data in Deep Learning Model for University Students’ GPA Prediction",
  "abstrak": "Current approaches of university students' Grade Point Average (GPA) prediction rely on the use of tabular data as input. Intuitively, adding historical GPA data can help to improve the performance of a GPA prediction model. In this study, we present a dual-input deep learning model that is able to simultaneously process time-series and tabular data for predicting student GPA. Our proposed model achieved the best performance among all tested models with 0.4142 MSE (Mean Squared Error) and 0.418 MAE (Mean Absolute Error) for GPA with a 4.0 scale. It also has the best R\n2\n-score of 0.4879, which means it explains the true distribution of students' GPA better than other models.",
  "keyword": "data mining"
 },
 {
  "No": 262,
  "judul": "Signal-Integrity Optimization for Complicated Multiple-Input Multiple-Output Networks Based on Data Mining of S-Parameters",
  "abstrak": "In this paper, an efficient signal-integrity analysis and optimization method for complicated multiple-input multiple-output (MIMO) networks is proposed, in which data mining is applied to discover the concealed information in black-box S-parameter models. Instead of performing a number of circuit simulations, data mining employs mathematical search algorithm directly into the model, which can save significant analyzed time and improve the efficiency. An optimized mining flow is presented for large scale data set, where the data processing and data mining are performed simultaneously, and thus it is unnecessary to save large extracted data. The proposed data mining method consider both the interconnect structure and the stimulated pattern of a MIMO system, which can perform thoughtful analysis and optimization with high efficiency. Two examples, signal-integrity analysis of two coupled microstrip lines and noise coupling among multiple signal vias through a power-ground plane pair, are presented to demonstrate the efficiency of the proposed data mining method in signal-integrity optimization design.",
  "keyword": "data mining"
 },
 {
  "No": 263,
  "judul": "Data Mining Algorithm for Cloud Network Information Based on Artificial Intelligence Decision Mechanism",
  "abstrak": "Due to the rapid development of information technology and network technology, there is a lot of data, but the phenomenon of lack of knowledge is becoming more and more serious. Data mining technology has developed vigorously in this environment, and it has shown more and more vitality. Based on Spark programming model, this paper designs the parallel extension of fuzzy c-means. In order to enhance the performance of fuzzy c-means parallel expansion, the improvement strategy of k-means during the initialization phase is borrowed, and k-means\/\/ is extended to fuzzy c-means to obtain better clustering performance. Combined with Spark's programming model, this paper can obtain extended parallel fuzzy c-means algorithm. Several experiments on the data set of the algorithm proposed in this paper have shown good scalability and parallelism, effectively expanding fuzzy c-means clustering to distributed applications, greatly increasing the scale of the data processed by the algorithm. This improves the robustness of the algorithm and the adaptability of the algorithm to the shape and structure of the data, so that the parallel and scalable clustering algorithm can more effectively perform cluster analysis on big data. Three algorithms were simulated on MATLAB platform. We use simple data sets and complex two-dimensional data sets, and compare with the traditional fuzzy c-means algorithm and fuzzy c-means algorithm based on fuzzy entropy. Experiments show that the scalable parallel fuzzy c-means algorithm not only greatly improves the anti-noise performance, but also improves the convergence speed, and it can automatically determine the optimal number of clusters.",
  "keyword": "data mining"
 },
 {
  "No": 264,
  "judul": "A Fast Approach for Up-Scaling Frequent Itemsets",
  "abstrak": "With the rapid growth of data scale and diversification of demand, people have an urgent desire to extract useful frequent itemset from datasets of different scales. It is no doubt that the traditional method can solve the problem. However, the relationships among datasets of different scales are not fully utilized. A fast approach proposed in this paper is as follows: the frequent itemsets on the large-scale data are directly inferred based on the frequent itemsets that are belonged small-scale datasets, instead of mined from the large-scale dataset again on condition that the frequent itemsets on the small-scale datasets have been mined. We conduct extensive experiments on one synthetic data and four UCI data sets. The experimental results show that our algorithm is significantly faster and consumes less memory than these leading algorithms.",
  "keyword": "data mining"
 },
 {
  "No": 265,
  "judul": "Unmanned Aerial Vehicle (UAV) Photogrammetry Technology for Dynamic Mining Subsidence Monitoring and Parameter Inversion: A Case Study in China",
  "abstrak": "This study mainly presents the method for monitoring the surface dynamic subsidence basin (SDSB) caused by underground coal mining and obtaining parameters of mining subsidence (PMS) in the short term by using an unmanned aerial vehicle (UAV) Photogrammetry Technology. The basic ideas and methods are proposed; that is, the two-stage surface digital elevation model (DEM) is obtained in the short term by UAV; The SDSB is obtained through two phases of DEM subtraction; Based on the dynamic inversion method established in this study, the PMS was obtained. The UAV method was used to monitor the Wangjiata coal mine in Inner Mongolia of China three times in three months to obtain the three phases of DEM; we obtained the two phases SDSB by the three phases of DEM subtraction. The accuracy of DEM and SDSB were 118 mm and 121 mm respectively, Although the accuracy cannot fulfill the requirements of mining subsidence, the PMS was obtained by dynamic inversion method of full subsidence basin fitting, which has better resistance to errors; hence, the obtained PMS are reliable. Based on the engineering application, this study concludes that as a new technology, UAV photogrammetry technology can obtain the SDSB of coal mining areas in a short period with reliable PMS. Meanwhile, it has the advantages of low cost, flexible maneuverability, and so on, and overcomes the shortages of traditional observation stations, which has long observation time and high labor intensity, needs to bury fixed measuring points and easily lose them, and just has a small amount of “spot-like” observation data that cannot reflect the deformation characteristics of the whole subsidence basin. Furthermore, it is feasible to apply this method to mining subsidence monitoring and parameter inversion, and it has a great application prospect for the mining area purposes.",
  "keyword": "data mining"
 },
 {
  "No": 266,
  "judul": "Mapping Data Mining Technique and Gamification Approach for Studying Post-Stroke Rehabilitation Training: A Systematic Literature Review",
  "abstrak": "Data mining has been widely used in healthcare to provide treatment and care recommendations based on the collective predictions of individual conditions. Various data mining techniques have been applied for rehabilitation to predict and recommend suitable recovery paths and training. In addition, the gamification concept was applied to rehabilitation training to motivate the patients to follow the training until the end. Considerable research has been conducted to investigate the validity and effectiveness of these techniques using massive patient data on specific conditions and treatment contexts. However, it is still unclear how to effectively offer customized rehabilitation training for stroke patients using gamification and data mining approaches. Thus, to understand how researchers studied them, we examined 34 peer-reviewed articles published in computer science and medical proceedings and journals between 2012 and 2022. We systematically reviewed the data mining and gamification techniques researchers had applied for post-stroke rehabilitation and related prediction models resulting from the data mining processes. As a result of the analyses, three significant contributions are identified. This article 1) identifies trends in data mining and gamification used in personalized post-stroke rehabilitation training; 2) maps trends in the study of data mining and gamification in post-stroke rehabilitation; and 3) identifies underexplored studies for future work. There is a definite need to continue developing and researching intervention strategies related to rehabilitation to address recovery problems by providing accuracy and protection of healthcare, as well as incorporating components that promote patients’ motivation and engagement.",
  "keyword": "data mining"
 },
 {
  "No": 267,
  "judul": "A Survey on Multi-Label Data Stream Classification",
  "abstrak": "Nowadays, many real-world applications of our daily life generate massive volume of streaming data at a higher speed than ever before, to name a few, Web clicking data streams, sensor network data and credit transaction streams. Contrary to traditional data mining using static datasets, there are several challenges for data stream mining, for instance, finite memory, one-pass and timely reaction. In this survey, we provide a comprehensive review of existing multi-label streams mining algorithms and categorize these methods based on different perspectives, which mainly focus on the multi-label data stream classification. We first briefly summarize existing multi-label and data stream classification algorithms and discuss their merits and demerits. Secondly, we identify mining constraints on classification for multi-label streaming data, and present a comprehensive study in algorithms for multi-label data stream classification. Finally, several challenges and open issues in multi-label data stream classification are discussed, which are worthwhile to be pursued by the researchers in the future.",
  "keyword": "data mining"
 },
 {
  "No": 268,
  "judul": "Processes Meet Big Data: Connecting Data Science with Process Science",
  "abstrak": "As more and more companies are embracing Big data, it has become apparent that the ultimate challenge is to relate massive amounts of event data to processes that are highly dynamic. To unleash the value of event data, events need to be tightly connected to the control and management of operational processes. However, the primary focus of Big data technologies is currently on storage, processing, and rather simple analytical tasks. Big data initiatives rarely focus on the improvement of end-to-end processes. To address this mismatch, we advocate a better integration of data science, data technology and process science. Data science approaches tend to be process agonistic whereas process science approaches tend to be model-driven without considering the “evidence” hidden in the data. Process mining aims to bridge this gap. This editorial discusses the interplay between data science and process science and relates process mining to Big data technologies, service orientation, and cloud computing.",
  "keyword": "data mining"
 },
 {
  "No": 269,
  "judul": "Incremental Fuzzy Mining of Gene Expression Data for Gene Function Prediction",
  "abstrak": "Due to the complexity of the underlying biological processes, gene expression data obtained from DNA microarray technologies are typically noisy and have very high dimensionality and these make the mining of such data for gene function prediction very difficult. To tackle these difficulties, we propose to use an incremental fuzzy mining technique called incremental fuzzy mining (IFM). By transforming quantitative expression values into linguistic terms, such as highly or lowly expressed, IFM can effectively capture heterogeneity in expression data for pattern discovery. It does so using a fuzzy measure to determine if interesting association patterns exist between the linguistic gene expression levels. Based on these patterns, IFM can make accurate gene function predictions and these predictions can be made in such a way that each gene can be allowed to belong to more than one functional class with different degrees of membership. Gene function prediction problem can be formulated both as classification and clustering problems, and IFM can be used either as a classification technique or together with existing clustering algorithms to improve the cluster groupings discovered for greater prediction accuracies. IFM is characterized also by its being an incremental data mining technique so that the discovered patterns can be continually refined based only on newly collected data without the need for retraining using the whole dataset. For performance evaluation, IFM has been tested with real expression datasets for both classification and clustering tasks. Experimental results show that it can effectively uncover hidden patterns for accurate gene function predictions.",
  "keyword": "data mining"
 },
 {
  "No": 270,
  "judul": "Sparse Trust Data Mining",
  "abstrak": "As recommendation systems continue to evolve, researchers are using trust data to improve the accuracy of recommendation prediction and help users find relevant information. However, large recommendation systems with trust data suffer from the sparse trust problem, which leads to grade inflation and severely affects the reliability of trust propagation. This paper presents a novel research on sparse trust data mining, which includes the new concept of sparse trust, a sparse trust model, and a trust mining framework. It lays a foundation for the trust-related research in large recommended systems. The new trust mining framework is based on customized normalization functions and a novel transitive gossip trust model, which discovers potential trust information between entities in a large-scale user network and applies it to a recommendation system. We conducts a comprehensive performance evaluation on both real-world and synthetic datasets. The results confirm that our framework mines new trust and effectively ameliorates sparse trust problem.",
  "keyword": "data mining"
 },
 {
  "No": 271,
  "judul": "Using 4-D Seismic Data for Detecting Gob Areas of Coal Mines: A Case Study From the Zhangji Coal Mine",
  "abstrak": "Subsidence of the gob area is an important geological problem in coal mines. Such events not only pose potential safety risks, but also can seriously impair the subsequent mining of coal mines. Therefore, detection of gob subsidence is important to ensure sustainable coal mining. Due to the complex seismic and geological conditions in the gob subsidence area, it is difficult to accurately identify the gob area by using traditional three-dimensional (3-D) seismic detection method. Herein, a new, model-driven gob detection method based on four-dimensional (4-D) seismic data is proposed. According to the proposed strategy, geological models before and after coal seam mining are firstly established, and synthetic data are generated through forward modeling. Then, the prestack time migration (PSTM) sections of synthetic data are used to retrospectively analyze the seismic reflection characteristics of the gob area, and the gob area is identified according to the variations of reflection characteristics and velocity fields caused by the gobs. Next, the characteristics of gob areas in synthetic data are used to guide the identification of the gob area in real seismic data. Meanwhile, a 3-D stratum stripping technique is used to finely depict the gob area. According to the method, the scope of the caving zone, the height of the overburden fractured zone in the gob area, and the geological structure of the lower coal seam in the gob area can be ascertained. This article presents the successful use of the proposed method in detecting the gob area from a real coal mine.",
  "keyword": "data mining"
 },
 {
  "No": 272,
  "judul": "Privacy Preserving Data Mining Framework for Negative Association Rules: An Application to Healthcare Informatics",
  "abstrak": "Protecting the privacy of healthcare information is an important part of encouraging data custodians to give accurate records so that mining may proceed with confidence. The application of association rule mining in healthcare data has been widespread to this point in time. Most applications focus on positive association rules, ignoring the negative consequences of particular diagnostic techniques. When it comes to bridging divergent diseases and drugs, negative association rules may give more helpful information than positive ones. This is especially true when it comes to physicians and social organizations (e.g., a certain symptom will not arise when certain symptoms exist). Data mining in healthcare must be done in a way that protects the identity of patients, especially when dealing with sensitive information. However, revealing this information puts it at risk of attack. Healthcare data privacy protection has lately been addressed by technologies that disrupt data (data sanitization) and reconstruct aggregate distributions in the interest of doing research in data mining. In this study, metaheuristic-based data sanitization for healthcare data mining is investigated in order to keep patient privacy protected. It is hoped that by using the Tabu-genetic algorithm as an optimization tool, the suggested technique chooses item sets to be sanitized (modified) from transactions that satisfy sensitive negative criteria with the goal of minimizing changes to the original database. Experiments with benchmark healthcare datasets show that the suggested privacy preserving data mining (PPDM) method outperforms existing algorithms in terms of Hiding Failure (HF), Artificial Rule Generation (AR), and Lost Rules (LR).",
  "keyword": "data mining"
 },
 {
  "No": 273,
  "judul": "Mining Probabilistically Frequent Sequential Patterns in Large Uncertain Databases",
  "abstrak": "Data uncertainty is inherent in many real-world applications such as environmental surveillance and mobile tracking. Mining sequential patterns from inaccurate data, such as those data arising from sensor readings and GPS trajectories, is important for discovering hidden knowledge in such applications. In this paper, we propose to measure pattern frequentness based on the possible world semantics. We establish two uncertain sequence data models abstracted from many real-life applications involving uncertain sequence data, and formulate the problem of mining probabilistically frequent sequential patterns (or p-FSPs) from data that conform to our models. However, the number of possible worlds is extremely large, which makes the mining prohibitively expensive. Inspired by the famous PrefixSpan algorithm, we develop two new algorithms, collectively called U-PrefixSpan, for p-FSP mining. U-PrefixSpan effectively avoids the problem of “possible worlds explosion”, and when combined with our four pruning and validating methods, achieves even better performance. We also propose a fast validating method to further speed up our U-PrefixSpan algorithm. The efficiency and effectiveness of U-PrefixSpan are verified through extensive experiments on both real and synthetic datasets.",
  "keyword": "data mining"
 },
 {
  "No": 274,
  "judul": "Intelligent Gear Decision Method for Automatic Vehicles Based on Data Mining Under Uphill Conditions",
  "abstrak": "Existing gear decision methods are affected by the limiting factors of deviation of numerical models as well as the subjectivity of the designer, which results in poor adaptability to the driving intentions and driving environments. Excellent drivers are able to choose the appropriate gear to satisfy their driving intentions and respond to changes in driving environments when driving manual transmission vehicles. Therefore, an intelligent gear decision method and design methodology of vehicle automatic transmission system that adapts to different driving intentions and complex driving environments are constructed from the massive driving data of excellent drivers by using data mining method. Excellent drivers are employed to drive vehicles with manual transmission to obtain a large amount of driving data. Subsequently, data preprocessing, data cleaning, and outlier removal are performed on the collected driving data to extract the shift boundary points of each gear aiming at constructing the shift rule surface of each gear. Using the uphill condition as an example, a data-mining-based shift control strategy is established, and the comparison results verify that the proposed gear decision design method can mine the shift strategy of excellent drivers from massive driving data, and the constructed strategy can better adapt to the driver’s intention while obtaining better fuel economy and avoiding unreasonable gearshifts compared with the automatic transmission’s default strategy.",
  "keyword": "data mining"
 },
 {
  "No": 275,
  "judul": "Event Detection Through Differential Pattern Mining in Cyber-Physical Systems",
  "abstrak": "Extracting knowledge from sensor data for various purposes has received a great deal of attention by the data mining community. For the purpose of event detection in cyber-physical systems (CPS), e.g., damage in building or aerospace vehicles from the continuous arriving data is challenging due to the detection quality. Traditional data mining schemes are used to reduce data that often use metrics, association rules, and binary values for frequent patterns as indicators for finding interesting knowledge about an event. However, these may not be directly applicable to the network due to certain constraints (communication, computation, bandwidth). We discover that, the indicators may not reveal meaningful information for event detection in practice. In this paper, we propose a comprehensive data mining framework for event detection in the CPS named DPminer, which functions in a distributed and parallel manner (data in a partitioned database processed by one or more sensor processors) and is able to extract a pattern of sensors that may have event information with a low communication cost. To achieve this, we introduce a new sensor behavioral pattern mining technique called differential sensor pattern (DSP) which considers different frequencies and values (non-binary) with a set of sensors, instead of traditional binary patterns. We present an algorithm for data preparation and then use a highly-compact data tree structure (called DP-Tree) for generating the DSP. An important tradeoff between the communication and computation costs for the event detection via data mining is made. Evaluation results show that DPminer can be very useful for networked sensing with a superior performance in terms of communication cost and event detection quality compared to existing data mining schemes.",
  "keyword": "data mining"
 },
 {
  "No": 276,
  "judul": "Analysis and Prediction of Students’ Academic Performance Based on Educational Data Mining",
  "abstrak": "The development of intelligent technologies gains popularity in the education field. The rapid growth of educational data indicates traditional processing methods may have limitations and distortion. Therefore, reconstructing the research technology of data mining in the education field has become increasingly prominent. In order to avoid unreasonable evaluation results and monitor the students’ future performance in advance, this paper comprehensively uses the relevant theories of clustering, discrimination and convolution neural network to analyze and predict students’ academic performance. Firstly, this paper proposes that the clustering-number determination is optimized by using a statistic which has never been used in the algorithm of K-means. Then, the clustering effect of K-means algorithm is tested by discriminant analysis. The convolutional neural network is introduced for training and testing data that are labeled with categories. The generated model can be used to predict prospective performance. Finally, in order to validate the prediction results, the effectiveness of the generated model is evaluated by using two metrics in two cross-validation methods. The experimental result demonstrates that the statistic not only solves the difficulty to determine the clustering number in K-means algorithm from an objective and quantitative point of view, but also improves the reliability of prediction results.",
  "keyword": "data mining"
 },
 {
  "No": 277,
  "judul": "A Model to Predict Low Academic Performance at a Specific Enrollment Using Data Mining",
  "abstrak": "This paper presents the results of applying an educational data mining approach to model academic attrition (loss of academic status) at the Universidad Nacional de Colombia. Two data mining models were defined to analyze the academic and nonacademic data; the models use two classification techniques, naïve Bayes and a decision tree classifier, in order to acquire a better understanding of the attrition during the first enrollments and to assess the quality of the data for the classification task, which can be understood as the prediction of the loss of academic status due to low academic performance. The models aim to predict the attrition in the student's first four enrollments. First, considering any of these periods, and then, at a specific enrollment. Historical academic records and data from the admission process were used to train the models, which were evaluated using cross-validation and previously unseen records from a full academic period. Experimental results show that the prediction of the loss of academic status is improved when the academic data are added.",
  "keyword": "data mining"
 },
 {
  "No": 278,
  "judul": "Scalable Daily Human Behavioral Pattern Mining from Multivariate Temporal Data",
  "abstrak": "This work introduces a set of scalable algorithms to identify patterns of human daily behaviors. These patterns are extracted from multivariate temporal data that have been collected from smartphones. We have exploited sensors that are available on these devices, and have identified frequent behavioral patterns with a temporal granularity, which has been inspired by the way individuals segment time into events. These patterns are helpful to both end-users and third parties who provide services based on this information. We have demonstrated our approach on two real-world datasets and showed that our pattern identification algorithms are scalable. This scalability makes analysis on resource constrained and small devices such as smartwatches feasible. Traditional data analysis systems are usually operated in a remote system outside the device. This is largely due to the lack of scalability originating from software and hardware restrictions of mobile\/wearable devices. By analyzing the data on the device, the user has the control over the data, i.e., privacy, and the network costs will also be removed.",
  "keyword": "data mining"
 },
 {
  "No": 279,
  "judul": "A New Methodology for Mining Frequent Itemsets on Temporal Data",
  "abstrak": "Temporal data contain time-stamping information that affects the results of data mining. Traditional techniques for finding frequent itemsets assume that datasets are static and the induced rules are relevant across the entire dataset. However, this is not the case when data is temporal. In this paper, we are trying to improve the efficiency of mining frequent itemsets on temporal data. Since patterns can hold in either all or some of the intervals, we propose a new algorithm to restrict time intervals, which is called frequent itemset mining with time cubes. Our focus is developing an efficient algorithm for this mining problem by extending the well-known a priori algorithm. The notion of time cubes is proposed to handle time hierarchies. This is the way by which the patterns that happen periodically, during a time interval or both, are recognized. A new density threshold is also proposed to solve the overestimating problem of time periods and also make sure that discovered patterns are valid. We evaluate our algorithms via experiments.",
  "keyword": "data mining"
 },
 {
  "No": 280,
  "judul": "A Fuzzy Mining Approach for Energy Efficiency in a Big Data Framework",
  "abstrak": "The discovery and exploitation of hidden information in collected data have gained attention in many areas, particularly in the energy field due to their economic and environmental impact. Data mining techniques have then emerged as a suitable toolbox for analyzing the data collected in modern network management systems in order to obtain a meaningful insight into consumption patterns and equipment operation. However, the enormous amount of data generated by sensors, occupational, and meteorological data involve the use of new management systems and data processing. Big Data presents great opportunities for implementing new solutions to manage these massive data sets. In addition, these data present values whose nature complicates and hides the understanding and interpretation of the data and results. Therefore, the use of fuzzy methods to adequately transform the data can improve their interpretability. This article presents an automatic fuzzification method implemented using the Big Data paradigm, which enables, in a later step, the detection of interrelations and patterns among different sensors and weather data recovered from an office building.",
  "keyword": "data mining"
 },
 {
  "No": 281,
  "judul": "Frequent Pattern Mining on Time and Location Aware Air Quality Data",
  "abstrak": "With the advent of big data era, enormous volumes of data are generated every second. Varied data processing algorithms and architectures have been proposed in the past to achieve better execution of data mining algorithms. One such algorithm is extracting most frequently occurring patterns from the transactional database. Dependency of transactions on time and location further makes frequent itemset mining task more complex. The present work targets to identify and extract the frequent patterns from such time and location-aware transactional data. Primarily, the spatio-temporal dependency of air quality data is leveraged to find out frequently co-occurring pollutants over several locations of Delhi, the capital city of India. Varied approaches have been proposed in the past to extract frequent patterns efficiently, but this work suggests a generalized approach that can be applied to any numeric spatio-temporal transactional data, including air quality data. Furthermore, a comprehensive description of the algorithm along with a sample running example on air quality dataset is shown in this work. A detailed experimental evaluation is carried out on the synthetically generated datasets, benchmark datasets, and real world datasets. Furthermore, a comparison with spatio-temporal apriori as well as the other state-of-the-art non-apriori-based algorithms is shown. Results suggest that the proposed algorithm outperformed the existing approaches in terms of execution time of algorithm and memory resources.",
  "keyword": "data mining"
 },
 {
  "No": 282,
  "judul": "Feature Selection and Its Use in Big Data: Challenges, Methods, and Trends",
  "abstrak": "Feature selection has been an important research area in data mining, which chooses a subset of relevant features for use in the model building. This paper aims to provide an overview of feature selection methods for big data mining. First, it discusses the current challenges and difficulties faced when mining valuable information from big data. A comprehensive review of existing feature selection methods in big data is then presented. Herein, we approach the review from two aspects: methods specific to a particular kind of big data with certain characteristics and applications of methods in classification analysis, which are significantly different to the existing review work. This paper also highlights the current issues of feature selection in big data and suggests the future research directions.",
  "keyword": "data mining"
 },
 {
  "No": 283,
  "judul": "Using 4-D Seismic Data for Detecting Gob Areas of Coal Mines: A Case Study From the Zhangji Coal Mine",
  "abstrak": "Subsidence of the gob area is an important geological problem in coal mines. Such events not only pose potential safety risks, but also can seriously impair the subsequent mining of coal mines. Therefore, detection of gob subsidence is important to ensure sustainable coal mining. Due to the complex seismic and geological conditions in the gob subsidence area, it is difficult to accurately identify the gob area by using traditional three-dimensional (3-D) seismic detection method. Herein, a new, model-driven gob detection method based on four-dimensional (4-D) seismic data is proposed. According to the proposed strategy, geological models before and after coal seam mining are firstly established, and synthetic data are generated through forward modeling. Then, the prestack time migration (PSTM) sections of synthetic data are used to retrospectively analyze the seismic reflection characteristics of the gob area, and the gob area is identified according to the variations of reflection characteristics and velocity fields caused by the gobs. Next, the characteristics of gob areas in synthetic data are used to guide the identification of the gob area in real seismic data. Meanwhile, a 3-D stratum stripping technique is used to finely depict the gob area. According to the method, the scope of the caving zone, the height of the overburden fractured zone in the gob area, and the geological structure of the lower coal seam in the gob area can be ascertained. This article presents the successful use of the proposed method in detecting the gob area from a real coal mine.",
  "keyword": "data mining"
 },
 {
  "No": 284,
  "judul": "Privacy-Preserving-Outsourced Association Rule Mining on Vertically Partitioned Databases",
  "abstrak": "Association rule mining and frequent itemset mining are two popular and widely studied data analysis techniques for a range of applications. In this paper, we focus on privacy-preserving mining on vertically partitioned databases. In such a scenario, data owners wish to learn the association rules or frequent itemsets from a collective data set and disclose as little information about their (sensitive) raw data as possible to other data owners and third parties. To ensure data privacy, we design an efficient homomorphic encryption scheme and a secure comparison scheme. We then propose a cloud-aided frequent itemset mining solution, which is used to build an association rule mining solution. Our solutions are designed for outsourced databases that allow multiple data owners to efficiently share their data securely without compromising on data privacy. Our solutions leak less information about the raw data than most existing solutions. In comparison to the only known solution achieving a similar privacy level as our proposed solutions, the performance of our proposed solutions is three to five orders of magnitude higher. Based on our experiment findings using different parameters and data sets, we demonstrate that the run time in each of our solutions is only one order higher than that in the best non-privacy-preserving data mining algorithms. Since both data and computing work are outsourced to the cloud servers, the resource consumption at the data owner end is very low.",
  "keyword": "data mining"
 },
 {
  "No": 285,
  "judul": "Mining Sequential Risk Patterns From Large-Scale Clinical Databases for Early Assessment of Chronic Diseases: A Case Study on Chronic Obstructive Pulmonary Disease",
  "abstrak": "Chronic diseases have been among the major concerns in medical fields since they may cause a heavy burden on healthcare resources and disturb the quality of life. In this paper, we propose a novel framework for early assessment on chronic diseases by mining sequential risk patterns with time interval information from diagnostic clinical records using sequential rules mining, and classification modeling techniques. With a complete workflow, the proposed framework consists of four phases namely data preprocessing, risk pattern mining, classification modeling, and post analysis. For empiricasl evaluation, we demonstrate the effectiveness of our proposed framework with a case study on early assessment of COPD. Through experimental evaluation on a large-scale nationwide clinical database in Taiwan, our approach can not only derive rich sequential risk patterns but also extract novel patterns with valuable insights for further medical investigation such as discovering novel markers and better treatments. To the best of our knowledge, this is the first work addressing the issue of mining sequential risk patterns with time-intervals as well as classification models for early assessment of chronic diseases.",
  "keyword": "data mining"
 },
 {
  "No": 286,
  "judul": "Sparse Trust Data Mining",
  "abstrak": "As recommendation systems continue to evolve, researchers are using trust data to improve the accuracy of recommendation prediction and help users find relevant information. However, large recommendation systems with trust data suffer from the sparse trust problem, which leads to grade inflation and severely affects the reliability of trust propagation. This paper presents a novel research on sparse trust data mining, which includes the new concept of sparse trust, a sparse trust model, and a trust mining framework. It lays a foundation for the trust-related research in large recommended systems. The new trust mining framework is based on customized normalization functions and a novel transitive gossip trust model, which discovers potential trust information between entities in a large-scale user network and applies it to a recommendation system. We conducts a comprehensive performance evaluation on both real-world and synthetic datasets. The results confirm that our framework mines new trust and effectively ameliorates sparse trust problem.",
  "keyword": "data mining"
 },
 {
  "No": 287,
  "judul": "A Survey of Key Technologies for High Utility Patterns Mining",
  "abstrak": "Recently, high utility pattern mining (HUPM) is one of the most important research issues in data mining. Because it can consider the non-binary frequency values of items in a transaction and the different profit values of each item. It has been widely used. First of all, this paper briefly describes the related concepts, formulas and examples of application for HUPM. Secondly, the key technologies for HUMP are introduced in detail, and they are divided into main methods including Apriori-based, tree-based, projection-based, list-based, data format-based, and index-based and so on. The paper further compares data sets, uses, advantages and disadvantages of algorithms, laid the foundation for the next research direction. Then, this article outlines the high utility derivative patterns, including high average utility pattern, high utility sequential pattern, and high utility compact pattern and so on. Because static data is difficult to meet the actual needs, this paper summarizes the efficient use of HUPMs' methods over data streams, mainly based on incremental methods, based on the sliding window model methods, based on the time decay model methods and based on the landmark model methods and so on.",
  "keyword": "data mining"
 },
 {
  "No": 288,
  "judul": "Efficient Frequent Chronicle Mining Algorithms: Application to Sleep Disorder",
  "abstrak": "Sequential pattern mining is a dynamic and thriving research field that aims to extract recurring sequences of events from complex datasets. Traditionally, focusing solely on the order of events often falls short of providing precise insights. Consequently, incorporating the temporal intervals between events has emerged as a vital necessity across various domains, e.g. medicine. Analyzing temporal event sequences within patients’ clinical histories, drug prescriptions, and monitoring alarms exemplifies this critical need. This paper presents innovative and efficient methodologies for mining frequent chronicles from temporal data. The mined graphs offer a significantly more expressive representation than mere event sequences, capturing intricate details of a series of events in a factual manner. The experimental stage includes a series of analyses of diverse databases with distinct characteristics. The proposed approaches were also applied to real-world data comprising information about subjects suffering from sleep disorders. Alluring frequent complete event graphs were obtained on patients who were under the effect of sleep medication.",
  "keyword": "data mining"
 },
 {
  "No": 289,
  "judul": "Multi-Tasks Discovery Method Based on the Concept Network for Data Mining",
  "abstrak": "Business and data understanding, that aims to identify multiple mining tasks, is the most primary phase in planning a practical data mining project. However, traditional tasks determination problem could only be solved by experienced analysts, which suffers from high communication cost and low efficiency. In this paper, we study the automatic task discovery method following the problem solving theory. First, we establish the concept network (CN) model to represent human knowledge and experience in the problem solving process. Then, we propose and demonstrate the structure of two major mining tasks (clustering and classification) in a CN. Finally, a data mining tasks discovery method (DMTD) is put forward, followed by two analysis subject evaluation algorithms. Experiment results illustrate that the DMTD is able to discover all the potential mining tasks from a predefined concept network, filtered by the important or interesting analysis subjects. Moreover, these tasks defined by the DMTD are proven to be available and valuable by fifty published papers.",
  "keyword": "data mining"
 },
 {
  "No": 290,
  "judul": "Efficient Method for Mining High Utility Occupancy Patterns Based on Indexed List Structure",
  "abstrak": "High utility pattern mining has been proposed to improve the traditional support-based pattern mining methods that process binary databases. High utility patterns are discovered by effectively considering the quantity and importance of items. Recently, high utility occupancy pattern mining studies have been conducted to extract high-quality patterns by utilizing both the occupancy utility and frequency measure. Although the previous approaches provide worthy information in terms of utility occupancy, they require time-consuming tasks because of numerous comparison operations in exploring entries in global data structures. This results in significant performance degradation when the database is large, or a pre-defined threshold is low. An indexed list structure improves the inefficiency of the list-based approach by structurally connecting each tuple. In this paper, we propose an efficient high utility occupancy mining approach based on novel indexed list-based structures. The two newly designed data structures maintain index information on items or patterns and facilitate rapid pattern extension. Our approach improves the cost of generating long patterns of list-based ones by reducing a large number of comparison overheads. In addition, we devise novel constructing and mining methods that are suitable for the proposed data structures and utility occupancy functions. To narrow the wide search space, efficient pruning techniques apply to the designed methods. Thorough performance experiments using real and synthetic datasets show that our method is more efficient than state-of-the-art methods in environments where given thresholds change.",
  "keyword": "data mining"
 },
 {
  "No": 291,
  "judul": "When Road Information Meets Data Mining: Precision Detection for Heading and Width of Roads",
  "abstrak": "Real-time road information plays a crucial role in enabling intelligent transportation systems (ITS) applications. With sufficient road information, the map of road topography can be built and updated more easily. Furthermore, many appealing ITS applications can be enabled accordingly. Aiming at improving the quality and update rate of road information, a hot topic today is how to mine information from global positioning systems (GPS) trajectories by the clustering-based methods. Such schemes, however, encounter two challenges: 1) GPS noise and 2) low sampling rate of GPS traces data. As a result, it is difficult to infer road information from these irregular clusters. To tackle the above issues, we directly mine useful road information, heading, and width of roads, for ITS applications from GPS point cloud, i.e., a set of GPS points. First, the distribution of GPS points is discussed and the least squares method (LSM) is demonstrated to be outstanding for mining the heading of the road under a huge number of GPS points. Second, the weighted approximation least squares method is proposed to improve the accuracy of the LSM. Furthermore, combining with relevant distribution features in GPS points, the data distribution variance-road width discrete model is proposed to mine road width from GPS point cloud. Finally, using real-world datasets, we demonstrate that these proposed methods can achieve satisfactory performance in practice.",
  "keyword": "data mining"
 },
 {
  "No": 292,
  "judul": "Developing a Hybrid Intrusion Detection System Using Data Mining for Power Systems",
  "abstrak": "Synchrophasor systems provide an immense volume of data for wide area monitoring and control of power systems to meet the increasing demand of reliable energy. The construction of traditional intrusion detection systems (IDSs) that use manually created rules based upon expert knowledge is knowledge-intensive and is not suitable in the context of this big data problem. This paper presents a systematic and automated approach to build a hybrid IDS that learns temporal state-based specifications for power system scenarios including disturbances, normal control operations, and cyber-attacks. A data mining technique called common path mining is used to automatically and accurately learn patterns for scenarios from a fusion of synchrophasor measurement data, and power system audit logs. As a proof of concept, an IDS prototype was implemented and validated. The IDS prototype accurately classifies disturbances, normal control operations, and cyber-attacks for the distance protection scheme for a two-line three-bus power transmission system.",
  "keyword": "data mining"
 },
 {
  "No": 293,
  "judul": "A Dimensionally Reduced Clustering Methodology for Heterogeneous Occupational Medicine Data Mining",
  "abstrak": "Clustering is a set of techniques of the statistical learning aimed at finding structures of heterogeneous partitions grouping homogenous data called clusters. There are several fields in which clustering was successfully applied, such as medicine, biology, finance, economics, etc. In this paper, we introduce the notion of clustering in multifactorial data analysis problems. A case study is conducted for an occupational medicine problem with the purpose of analyzing patterns in a population of 813 individuals. To reduce the data set dimensionality, we base our approach on the Principal Component Analysis (PCA), which is the statistical tool most commonly used in factorial analysis. However, the problems in nature, especially in medicine, are often based on heterogeneous-type qualitative-quantitative measurements, whereas PCA only processes quantitative ones. Besides, qualitative data are originally unobservable quantitative responses that are usually binary-coded. Hence, we propose a new set of strategies allowing to simultaneously handle quantitative and qualitative data. The principle of this approach is to perform a projection of the qualitative variables on the subspaces spanned by quantitative ones. Subsequently, an optimal model is allocated to the resulting PCA-regressed subspaces.",
  "keyword": "data mining"
 },
 {
  "No": 294,
  "judul": "Trajectory Mining Using Uncertain Sensor Data",
  "abstrak": "Trajectory mining is an interesting data mining problem. Traditionally, it is either assumed that the time-ordered location data recorded as trajectories are either deterministic or that the uncertainty, e.g., due to equipment or technological limitations, is removed by incorporating some pre-processing routines. Thus, the trajectories are processed as deterministic paths of mobile object location data. However, it is important to understand that the transformation from uncertain to deterministic trajectory data may result in the loss of information about the level of confidence in the recorded events. Probabilistic databases offer ways to model uncertainties using possible world semantics. In this paper, we consider uncertain sensor data and transform this to probabilistic trajectory data using pre-processing routines. Next, we model this data as tuple level uncertain data and propose dynamic programming-based algorithms to mine interesting trajectories. A comprehensive empirical study is performed to evaluate the effectiveness of the approach. The results show that the trajectories could be modeled and worked as probabilistic data and that the results could be computed efficiently using dynamic programming.",
  "keyword": "data mining"
 },
 {
  "No": 295,
  "judul": "An Optimized Mining Algorithm for Analyzing Students’ Learning Degree Based on Dynamic Data",
  "abstrak": "With the rapid development of educational informatization, it has enabled education to enter the era of big data. How to extract effective information from educational big data and realize adaptive personalized learning goals have become the current research hotspot. The traditional static data only analyzes the students' learning degree based on the students' final answer, but ignores the dynamic data in the process of answering questions, such as the modification and the time it answered on the question, which makes it difficult to fully and accurately mine the correlation between the massive data, so it turns from static data mining to dynamic data mining. The paper proposes an optimized mining algorithm for analyzing students' learning degree based on dynamic data. The algorithm first uses the optimized text classification technology to match the question texts to the knowledge points automatically, so as to improves the efficiency and quality. Then, it uses the subjective weighting method combined with the expert experience to generate the learning degree matrix of students on knowledge points based on dynamic data of the students' records. Finally, the DBSCAN clustering algorithm is used to cluster the personalized learning characteristics of students according to the learning degree matrix. The experimental result shows that the algorithm can deal with massive data automatically and effectively, and analyze the students' learning degree on knowledge points comprehensively and accurately, so as to classify students and realize personalized teaching.",
  "keyword": "data mining"
 },
 {
  "No": 296,
  "judul": "Probabilistic Static Load-Balancing of Parallel Mining of Frequent Sequences",
  "abstrak": "Frequent sequence mining is well known and well studied problem in datamining. The output of the algorithm is used in many other areas like bioinformatics, chemistry, and market basket analysis. Unfortunately, the frequent sequence mining is computationally quite expensive. In this paper, we present a novel parallel algorithm for mining of frequent sequences based on a static load-balancing. The static load-balancing is done by measuring the computational time using a probabilistic algorithm. For reasonable size of instance, the algorithms achieve speedups up to \n≈3\/4⋅P\n where \n P \n is the number of processors. In the experimental evaluation, we show that our method performs significantly better then the current state-of-the-art methods. The presented approach is very universal: it can be used for static load-balancing of other pattern mining algorithms such as itemset\/tree\/graph mining algorithms.",
  "keyword": "data mining"
 },
 {
  "No": 297,
  "judul": "Infrequent Weighted Itemset Mining Using Frequent Pattern Growth",
  "abstrak": "Frequent weighted itemsets represent correlations frequently holding in data in which items may weight differently. However, in some contexts, e.g., when the need is to minimize a certain cost function, discovering rare data correlations is more interesting than mining frequent ones. This paper tackles the issue of discovering rare and weighted itemsets, i.e., the infrequent weighted itemset (IWI) mining problem. Two novel quality measures are proposed to drive the IWI mining process. Furthermore, two algorithms that perform IWI and Minimal IWI mining efficiently, driven by the proposed measures, are presented. Experimental results show efficiency and effectiveness of the proposed approach.",
  "keyword": "data mining"
 },
 {
  "No": 298,
  "judul": "A weighted frequent itemset mining algorithm for intelligent decision in smart systems",
  "abstrak": "Intelligent decision is the key technology of smart systems. Data mining technology has been playing an increasingly important role in decision-making activities. Frequent itemset mining (FIM), as an important step of association rule analysis, is becoming one of the most important research fields in data mining. Weighted FIM in uncertain databases should take both existential probability and importance of items into account in order to find frequent itemsets of great importance to users. However, the introduction of weight makes the weighted frequent itemsets not satisfy the downward closure property any longer. As a result, the search space of frequent itemsets cannot be narrowed according to downward closure property which leads to a poor time efficiency. In this paper, the weight judgment downward closure property for the weighted frequent itemsets and the existence property of weighted frequent subsets are introduced and proved first. Based on these two properties, the Weight judgment downward closure property-based FIM (WD-FIM) algorithm is proposed to narrow the searching space of the weighted frequent itemsets and improve the time efficiency. Moreover, the completeness and time efficiency of WD-FIM algorithm are analyzed theoretically. Finally, the performance of the proposed WD-FIM algorithm is verified on both synthetic and real-life data sets.",
  "keyword": "data mining"
 },
 {
  "No": 299,
  "judul": "Mining Order-Preserving Submatrices from Data with Repeated Measurements",
  "abstrak": "Order-preserving submatrices (OPSM's) have been shown useful in capturing concurrent patterns in data when the relative magnitudes of data items are more important than their exact values. For instance, in analyzing gene expression profiles obtained from microarray experiments, the relative magnitudes are important both because they represent the change of gene activities across the experiments, and because there is typically a high level of noise in data that makes the exact values untrustable. To cope with data noise, repeated experiments are often conducted to collect multiple measurements. We propose and study a more robust version of OPSM, where each data item is represented by a set of values obtained from replicated experiments. We call the new problem OPSM-RM (OPSM with repeated measurements). We define OPSM-RM based on a number of practical requirements. We discuss the computational challenges of OPSM-RM and propose a generic mining algorithm. We further propose a series of techniques to speed up two time dominating components of the algorithm. We show the effectiveness and efficiency of our methods through a series of experiments conducted on real microarray data.",
  "keyword": "data mining"
 },
 {
  "No": 300,
  "judul": "Similarity Measure Based on Incremental Warping Window for Time Series Data Mining",
  "abstrak": "A similarity measure is one of the most important tasks in the fields of time series data mining. Its quality often affects the efficiency and effectiveness of the related algorithms that need to measure the similarity between two time series in advance. Dynamic time warping is one of the most robust methods to compare one time series with another based onwarping alignments. In this paper, the design of an incremental warping window is used to improve the performance of dynamic time warping. The incremental warping window is changeable for various time series with different lengths. Moreover, the improved dynamic time warping based on the novel window considers the recent alignments as much as possible, which indicates that the proposed method concentrates on more information of the recent data points than that of the previous data points. In addition, it is suitable for online similarity measure between data stream. The experimental evaluation shows that the proposed method is effective and efficient for time series mining.",
  "keyword": "data mining"
 },
 {
  "No": 301,
  "judul": "Moving window-based double haar wavelet transform for image processing",
  "abstrak": "Image denoising is a lively research field. The classical nonlinear filters used for image denoising, such as median filter, are based on a local analysis of the pixels within a moving window. Recently, the research of image denoising has been focused on the wavelet domain. Compared to the classical nonlinear filters, it is based on a global multiscale analysis of images. Apparently, the wavelet transform can be embedded in a moving window. Thus, a moving window-based local multiscale analysis is obtained. In this paper, based on the Haar wavelet, a class of nonorthogonal multichannel filter bank with its corresponding wavelet shrinkage called Lee shrinkage is derived. As a special case of this filter bank, the double Haar wavelet transform is introduced. Examples show that it is suitable for a moving window-based local multiscale analysis used for image denoising, edge detection, and edge enhancement.",
  "keyword": "image processing"
 },
 {
  "No": 302,
  "judul": "Multi-Scale Patch-Based Image Restoration",
  "abstrak": "Many image restoration algorithms in recent years are based on patch processing. The core idea is to decompose the target image into fully overlapping patches, restore each of them separately, and then merge the results by a plain averaging. This concept has been demonstrated to be highly effective, leading often times to the state-of-the-art results in denoising, inpainting, deblurring, segmentation, and other applications. While the above is indeed effective, this approach has one major flaw: the prior is imposed on intermediate (patch) results, rather than on the final outcome, and this is typically manifested by visual artifacts. The expected patch log likelihood (EPLL) method by Zoran and Weiss was conceived for addressing this very problem. Their algorithm imposes the prior on the patches of the \nfinal image\n, which in turn leads to an iterative restoration of diminishing effect. In this paper, we propose to further extend and improve the EPLL by considering a multi-scale prior. Our algorithm imposes the very same prior on different scale patches extracted from the target image. While all the treated patches are of the same size, their footprint in the destination image varies due to subsampling. Our scheme comes to alleviate another shortcoming existing in patch-based restoration algorithms—the fact that a local (patch-based) prior is serving as a model for a global stochastic phenomenon. We motivate the use of the multi-scale EPLL by restricting ourselves to the simple Gaussian case, comparing the aforementioned algorithms and showing a clear advantage to the proposed method. We then demonstrate our algorithm in the context of image denoising, deblurring, and super-resolution, showing an improvement in performance both visually and quantitatively.",
  "keyword": "image processing"
 },
 {
  "No": 303,
  "judul": "Crack Defect Detection Processing Algorithm and Method of MEMS Devices Based on Image Processing Technology",
  "abstrak": "In order to solve the problem that the crack defects generated on the surface of MEMS devices are difficult to detect under high overload impact, this paper proposes a crack detection method based on attribute weighted naive Bayes improved OTSU algorithm. Based on the analysis of the surface defects in MEMS devices image, the edge information of the crack defect in the image is extracted by image processing such as image detail sharpening, grayscale processing, image enhancement and edge extraction based on Canny operator, and the pseudo crack in the image is removed by the least square method; the attribute weighted naive Bayes algorithm is introduced to improve the traditional OTSU image processing method, the crack defect detection results of the MEMS devices image are obtained, the crack defects are quantitatively characterized, and the length and width of the crack defects are calculated. Comparative experiments were conducted using multiple detection methods, the results showed that the crack detection method proposed in this paper can obtain the crack defect information of MEMS devices efficiently and accurately.",
  "keyword": "image processing"
 },
 {
  "No": 304,
  "judul": "No-Reference Image Sharpness Assessment in Autoregressive Parameter Space",
  "abstrak": "In this paper, we propose a new no-reference (NR)\/ blind sharpness metric in the autoregressive (AR) parameter space. Our model is established via the analysis of AR model parameters, first calculating the energy- and contrast-differences in the locally estimated AR coefficients in a pointwise way, and then quantifying the image sharpness with percentile pooling to predict the overall score. In addition to the luminance domain, we further consider the inevitable effect of color information on visual perception to sharpness and thereby extend the above model to the widely used YIQ color space. Validation of our technique is conducted on the subsets with blurring artifacts from four large-scale image databases (LIVE, TID2008, CSIQ, and TID2013). Experimental results confirm the superiority and efficiency of our method over existing NR algorithms, the state-of-the-art blind sharpness\/blurriness estimators, and classical full-reference quality evaluators. Furthermore, the proposed metric can be also extended to stereoscopic images based on binocular rivalry, and attains remarkably high performance on LIVE3D-I and LIVE3D-II databases.",
  "keyword": "image processing"
 },
 {
  "No": 305,
  "judul": "A New Method of Plane-Wave Ultrasound Imaging Based on Reverse Time Migration",
  "abstrak": "Coherent plane-wave compounding technique enables rapid ultrasound imaging with comparable image quality to traditional B-mode imaging that relies on focused beam transmission. However, existing methods assume homogeneity in the imaged medium, neglecting the heterogeneity in sound velocities and densities present in real tissues, resulting in noise reverberation. This study introduces the Reverse Time Migration (RTM) method for ultrasound plane-wave imaging to overcome this limitation, which is combined with a method for estimating the speed of sound in layered media. Simulation results in a homogeneous background demonstrate that RTM reduces side lobes and grating lobes by approximately 30 dB, enhancing the contrast-to-noise ratio by 20% compared to conventional delay and sum (DAS) beamforming. Moreover, RTM achieves superior imaging outcomes with fewer compounding angles. The lateral resolution of the RTM with 5–9 angle compounding is able to achieve the effectiveness of the DAS method with 15–19 angle compounding, and the CNR of the RTM with 11-angle compounding is almost the same as that of the DAS with 21-angle compounding. In a heterogeneous background, experimental simulations and in vitro wire phantom experiments confirm RTM's capability to correct depth imaging, focusing reflected waves on point targets. In vitro porcine tissue experiments enable accurate imaging of layer interfaces by estimating the velocities of multiple layers containing muscle and fat. The proposed imaging procedure optimizes velocity estimation in complex media, compensates for the impact of velocity differences, provides more reliable imaging results.",
  "keyword": "image processing"
 },
 {
  "No": 306,
  "judul": "Efficient implementation of accurate geometric transformations for 2-D and 3-D image processing",
  "abstrak": "This paper proposes the use of a polynomial interpolator structure (based on Horner's scheme) which is efficiently realizable in hardware, for high-quality geometric transformation of two- and three-dimensional images. Polynomial-based interpolators such as cubic B-splines and optimal interpolators of shortest support are shown to be exactly implementable in the Horner structure framework. This structure suggests a hardware\/software partition which can lead to efficient implementations for multidimensional interpolation.",
  "keyword": "image processing"
 },
 {
  "No": 307,
  "judul": "Vector-Valued Image Processing by Parallel Level Sets",
  "abstrak": "Vector-valued images such as RGB color images or multimodal medical images show a strong interchannel correlation, which is not exploited by most image processing tools. We propose a new notion of treating vector-valued images which is based on the angle between the spatial gradients of their channels. Through minimizing a cost functional that penalizes large angles, images with parallel level sets can be obtained. After formally introducing this idea and the corresponding cost functionals, we discuss their Gâteaux derivatives that lead to a diffusion-like gradient descent scheme. We illustrate the properties of this cost functional by several examples in denoising and demosaicking of RGB color images. They show that parallel level sets are a suitable concept for color image enhancement. Demosaicking with parallel level sets gives visually perfect results for low noise levels. Furthermore, the proposed functional yields sharper images than the other approaches in comparison.",
  "keyword": "image processing"
 },
 {
  "No": 308,
  "judul": "Effective Five Directional Partial Derivatives-Based Image Smoothing and a Parallel Structure Design",
  "abstrak": "Image smoothing has been used for image segmentation, image reconstruction, object classification, and 3D content generation. Several smoothing approaches have been used at the pre-processing step to retain the critical edge, while removing noise and small details. However, they have limited performance, especially in removing small details and smoothing discrete regions. Therefore, to provide fast and accurate smoothing, we propose an effective scheme that uses a weighted combination of the gradient, Laplacian, and diagonal derivatives of a smoothed image. In addition, to reduce computational complexity, we designed and implemented a parallel processing structure for the proposed scheme on a graphics processing unit (GPU). For an objective evaluation of the smoothing performance, the images were linearly quantized into several layers to generate experimental images, and the quantized images were smoothed using several methods for reconstructing the smoothly changed shape and intensity of the original image. Experimental results showed that the proposed scheme has higher objective scores and better successful smoothing performance than similar schemes, while preserving and removing critical and trivial details, respectively. For computational complexity, the proposed smoothing scheme running on a GPU provided 18 and 16 times lower complexity than the proposed smoothing scheme running on a CPU and the L0-based smoothing scheme, respectively. In addition, a simple noise reduction test was conducted to show the characteristics of the proposed approach; it reported that the presented algorithm outperforms the state-of-the art algorithms by more than 5.4 dB. Therefore, we believe that the proposed scheme can be a useful tool for efficient image smoothing.",
  "keyword": "image processing"
 },
 {
  "No": 309,
  "judul": "Photoacoustic Tomography Image Restoration With Measured Spatially Variant Point Spread Functions",
  "abstrak": "The spatial resolution of photoacoustic tomography (PAT) can be characterized by the point spread function (PSF) of the imaging system. Due to the tomographic detection geometry, the PAT image degradation model could be generally described by using spatially variant PSFs. Deconvolution of the PAT image with these PSFs could restore image resolution and recover object details. Previous PAT image restoration algorithms assume that the degraded images can be restored by either a single uniform PSF, or some blind estimation of the spatially variant PSFs. In this work, we propose a PAT image restoration method to improve image quality and resolution based on experimentally measured spatially variant PSFs. Using photoacoustic absorbing microspheres, we design a rigorous PSF measurement procedure, and successfully acquire a dense set of spatially variant PSFs for a commercial cross-sectional PAT system. A pixel-wise PSF map is further obtained by employing a multi-Gaussian-based fitting and interpolation algorithm. To perform image restoration, an optimization-based iterative restoration model with two kinds of regularizations is proposed. We perform phantom and in vivo mice imaging experiments to verify the proposed method, and the results show significant image quality and resolution improvement.",
  "keyword": "image processing"
 },
 {
  "No": 310,
  "judul": "Comprehensive Comparison of Image Quality Aspects Between Conventional and Plane-Wave Imaging Methods on a Commercial Scanner",
  "abstrak": "Coherent plane-wave compound imaging (CPWCI) is used as alternative for conventional focused imaging (CFI) to increase frame rates linearly with the ratio number of imaging lines to steering angles. In this study, the image quality was compared between CPWCI and CFI, and the effect of steering angles (range and number) and beamforming strategies was evaluated in CPWCI. In automated breast volume scanners (ABVSs), which suffer from reduced volume rates, CPWCI might be an excellent candidate to replace CFI. Therefore, the image quality of CFI currently in ABVS and CPWCI was also compared in an \nin vivo\n breast lesion. Images were obtained by a Siemens Sequoia ultrasound system, and two transducers (14L5 and 10L4) in a CIRS multipurpose phantom (040GSE) and a breast lesion. Phantom results showed that contrast sensitivity and resolution, axial resolution, and generalized contrast-to-noise ratio (gCNR; imaging depths <45 mm) were similar for most imaging sequences. CNR (imaging depths ≥45 mm), penetration, and lateral resolution were significantly improved for CPWCI (15 angles) compared to CFI for both transducers. In CPWCI, certain combinations of steering angles and beamforming methods yielded improved gCNR (small angles and delay-and-sum) or lateral resolution (large angles and Lu’s-fk). Image quality seemed similar between CPWCI and CFI (three angles incoherent compounded as in ABVS) by visual inspection of the \nin vivo\n breast lesion images.",
  "keyword": "image processing"
 },
 {
  "No": 311,
  "judul": "Distributed vector Processing of a new local MultiScale Fourier transform for medical imaging applications",
  "abstrak": "The recently developed S-transform (ST) combines features of the Fourier and Wavelet transforms; it reveals frequency variation over both space and time. It is a potentially powerful tool that can be applied to medical image processing including texture analysis and noise filtering. However, calculation of the ST is computationally intensive, making conventional implementations too slow for many medical applications. This problem was addressed by combining parallel and vector computations to provide a 25-fold reduction in computation time. This approach could help accelerate many medical image processing algorithms.",
  "keyword": "image processing"
 },
 {
  "No": 312,
  "judul": "Post-DAE: Anatomically Plausible Segmentation via Post-Processing With Denoising Autoencoders",
  "abstrak": "We introduce Post-DAE, a post-processing method based on denoising autoencoders (DAE) to improve the anatomical plausibility of arbitrary biomedical image segmentation algorithms. Some of the most popular segmentation methods (e.g. based on convolutional neural networks or random forest classifiers) incorporate additional post-processing steps to ensure that the resulting masks fulfill expected connectivity constraints. These methods operate under the hypothesis that contiguous pixels with similar aspect should belong to the same class. Even if valid in general, this assumption does not consider more complex priors like topological restrictions or convexity, which cannot be easily incorporated into these methods. Post-DAE leverages the latest developments in manifold learning via denoising autoencoders. First, we learn a compact and non-linear embedding that represents the space of anatomically plausible segmentations. Then, given a segmentation mask obtained with an arbitrary method, we reconstruct its anatomically plausible version by projecting it onto the learnt manifold. The proposed method is trained using unpaired segmentation mask, what makes it independent of intensity information and image modality. We performed experiments in binary and multi-label segmentation of chest X-ray and cardiac magnetic resonance images. We show how erroneous and noisy segmentation masks can be improved using Post-DAE. With almost no additional computation cost, our method brings erroneous segmentations back to a feasible space.",
  "keyword": "image processing"
 },
 {
  "No": 313,
  "judul": "Multiview Deblurring for 3-D Images from Light-Sheet-Based Fluorescence Microscopy",
  "abstrak": "We propose an algorithm for 3-D multiview deblurring using spatially variant point spread functions (PSFs). The algorithm is applied to multiview reconstruction of volumetric microscopy images. It includes registration and estimation of the PSFs using irregularly placed point markers (beads). We formulate multiview deblurring as an energy minimization problem subject to L1-regularization. Optimization is based on the regularized Lucy-Richardson algorithm, which we extend to deal with our more general model. The model parameters are chosen in a profound way by optimizing them on a realistic training set. We quantitatively and qualitatively compare with existing methods and show that our method provides better signal-to-noise ratio and increases the resolution of the reconstructed images.",
  "keyword": "image processing"
 },
 {
  "No": 314,
  "judul": "A Sparse Reconstruction Framework for Fourier-Based Plane-Wave Imaging",
  "abstrak": "Ultrafast imaging based on plane-wave (PW) insonification is an active area of research due to its capability of reaching high frame rates. Among PW imaging methods, Fourier-based approaches have demonstrated to be competitive compared with traditional delay and sum methods. Motivated by the success of compressed sensing techniques in other Fourier imaging modalities, like magnetic resonance imaging, we propose a new sparse regularization framework to reconstruct high-quality ultrasound (US) images. The framework takes advantage of both the ability to formulate the imaging inverse problem in the Fourier domain and the sparsity of US images in a sparsifying domain. We show, by means of simulations, in vitro and in vivo data, that the proposed framework significantly reduces image artifacts, i.e., measurement noise and sidelobes, compared with classical methods, leading to an increase of the image quality.",
  "keyword": "image processing"
 },
 {
  "No": 315,
  "judul": "High Dynamic Range Image Display With Halo and Clipping Prevention",
  "abstrak": "The dynamic range of an image is defined as the ratio between the highest and the lowest luminance level. In a high dynamic range (HDR) image, this value exceeds the capabilities of conventional display devices; as a consequence, dedicated visualization techniques are required. In particular, it is possible to process an HDR image in order to reduce its dynamic range without producing a significant change in the visual sensation experienced by the observer. In this paper, we propose a dynamic range reduction algorithm that produces high-quality results with a low computational cost and a limited number of parameters. The algorithm belongs to the category of methods based upon the Retinex theory of vision and was specifically designed in order to prevent the formation of common artifacts, such as halos around the sharp edges and clipping of the highlights, that often affect methods of this kind. After a detailed analysis of the state of the art, we shall describe the method and compare the results and performance with those of two techniques recently proposed in the literature and one commercial software.",
  "keyword": "image processing"
 },
 {
  "No": 316,
  "judul": "Edge Tracing Using Gaussian Process Regression",
  "abstrak": "We introduce a novel edge tracing algorithm using Gaussian process regression. Our edge-based segmentation algorithm models an edge of interest using Gaussian process regression and iteratively searches the image for edge pixels in a recursive Bayesian scheme. This procedure combines local edge information from the image gradient and global structural information from posterior curves, sampled from the model’s posterior predictive distribution, to sequentially build and refine an observation set of edge pixels. This accumulation of pixels converges the distribution to the edge of interest. Hyperparameters can be tuned by the user at initialisation and optimised given the refined observation set. This tunable approach does not require any prior training and is not restricted to any particular type of imaging domain. Due to the model’s uncertainty quantification, the algorithm is robust to artefacts and occlusions which degrade the quality and continuity of edges in images. Our approach also has the ability to efficiently trace edges in image sequences by using previous-image edge traces as a priori information for consecutive images. Various applications to medical imaging and satellite imaging are used to validate the technique and comparisons are made with two commonly used edge tracing algorithms.",
  "keyword": "image processing"
 },
 {
  "No": 317,
  "judul": "MRI Information-Based Correction and Restoration of Photoacoustic Tomography",
  "abstrak": "As an emerging molecular imaging modality, Photoacoustic Tomography (PAT) is capable of mapping tissue physiological metabolism and exogenous contrast agent information with high specificity. Due to its ultrasonic detection mechanism, the precise localization of targeted lesions has long been a challenge for PAT imaging. The poor soft-tissue contrast of the PAT image makes this process difficult and inaccurate. To meet this challenge, in this study, we first make use of the rich and clear structural information brought about by another advanced imaging modality, Magnetic Resonance Imaging (MRI), to assist organ segmentation and correct for the light fluence attenuation of PAT. We demonstrate improved feature visibility and enhanced localization of endogenous and exogenous agents in the fluence corrected PAT images. Compared with PAT-based methods, the contrast-to-noise ratio (CNR) of our MRI-assisted method increases by 29.1% in live animal experiments. Furthermore, we show that the co-registered MRI image can also be incorporated into PAT image restoration, and achieves improved anatomical landscape and soft-tissue contrast (CNR increased by 25.36%) while preserving similar spatial resolution. This PAT-MRI combination provides excellent structural, functional and molecular images of the subject, and may enable more comprehensive analysis of various preclinical research applications.",
  "keyword": "image processing"
 },
 {
  "No": 318,
  "judul": "Reduced-Complexity Delayed-Decision Algorithm for Context-Based Image Processing Systems",
  "abstrak": "It is well known that the performance of context-based image processing systems can be improved by allowing the processor (e.g., an encoder or a denoiser) a delay of several samples before making a processing decision. Often, however, for such systems, traditional delayed-decision algorithms can become computationally prohibitive due to the growth in the size of the space of possible solutions. In this paper, we propose a reduced-complexity, one-pass, delayed-decision algorithm that systematically reduces the size of the search space, while also preserving its structure. In particular, we apply the proposed algorithm to two examples of adaptive context-based image processing systems, an image coding system that employs a context-based entropy coder, and a spatially adaptive image-denoising system. For these two types of widely used systems, we show that the proposed delayed-decision search algorithm outperforms instantaneous-decision algorithms with only a small increase in complexity. We also show that the performance of the proposed algorithm is better than that of other, higher complexity, delayed-decision algorithms.",
  "keyword": "image processing"
 },
 {
  "No": 319,
  "judul": "Nonrigid Image Registration Using Conditional Mutual Information",
  "abstrak": "Maximization of mutual information (MMI) is a popular similarity measure for medical image registration. Although its accuracy and robustness has been demonstrated for rigid body image registration, extending MMI to nonrigid image registration is not trivial and an active field of research. We propose conditional mutual information (cMI) as a new similarity measure for nonrigid image registration. cMI starts from a 3-D joint histogram incorporating, besides the intensity dimensions, also a spatial dimension expressing the location of the joint intensity pair. cMI is calculated as the expected value of the cMI between the image intensities given the spatial distribution. The cMI measure was incorporated in a tensor-product \nB\n-spline nonrigid registration method, using either a Parzen window or generalized partial volume kernel for histogram construction. cMI was compared to the classical global mutual information (gMI) approach in theoretical, phantom, and clinical settings. We show that cMI significantly outperforms gMI for all applications.",
  "keyword": "image processing"
 },
 {
  "No": 320,
  "judul": "Apparent Ultra-High b-Value Diffusion-Weighted Image Reconstruction via Hidden Conditional Random Fields",
  "abstrak": "A promising, recently explored, alternative to ultra-high b-value diffusion weighted imaging (UHB-DWI) is apparent ultra-high b-value diffusion-weighted image reconstruction (AUHB-DWR), where a computational model is used to assist in the reconstruction of apparent DW images at ultra-high b-values. Firstly, we present a novel approach to AUHB-DWR that aims to improve image quality. We formulate the reconstruction of an apparent DW image as a hidden conditional random field (HCRF) in which tissue model diffusion parameters act as hidden states in this random field. The second contribution of this paper is a new generation of fully connected conditional random fields, called the hidden stochastically fully connected conditional random fields (HSFCRF) that allows for efficient inference with significantly reduced computational complexity via stochastic clique structures. The proposed AUHB-DWR algorithms, HCRF and HSFCRF, are evaluated quantitatively in nine different patient cases using Fisher's criteria, probability of error, and coefficient of variation metrics to validate its effectiveness for the purpose of improving intensity delineation between expert identified suspected cancerous and healthy tissue within the prostate gland. The proposed methods are also examined using a prostate phantom, where the apparent ultra-high b-value DW images reconstructed using the tested AUHB-DWR methods are compared with real captured UHB-DWI. The results illustrate that the proposed AUHB-DWR methods has improved reconstruction quality and improved intensity delineation compared with existing AUHB-DWR approaches.",
  "keyword": "image processing"
 },
 {
  "No": 321,
  "judul": "SpineParseNet: Spine Parsing for Volumetric MR Image by a Two-Stage Segmentation Framework With Semantic Image Representation",
  "abstrak": "Spine parsing (i.e., multi-class segmentation of vertebrae and intervertebral discs (IVDs)) for volumetric magnetic resonance (MR) image plays a significant role in various spinal disease diagnoses and treatments of spine disorders, yet is still a challenge due to the inter-class similarity and intra-class variation of spine images. Existing fully convolutional network based methods failed to explicitly exploit the dependencies between different spinal structures. In this article, we propose a novel two-stage framework named SpineParseNet to achieve automated spine parsing for volumetric MR images. The SpineParseNet consists of a 3D graph convolutional segmentation network (GCSN) for 3D coarse segmentation and a 2D residual U-Net (ResUNet) for 2D segmentation refinement. In 3D GCSN, region pooling is employed to project the image representation to graph representation, in which each node representation denotes a specific spinal structure. The adjacency matrix of the graph is designed according to the connection of spinal structures. The graph representation is evolved by graph convolutions. Subsequently, the proposed region unpooling module re-projects the evolved graph representation to a semantic image representation, which facilitates the 3D GCSN to generate reliable coarse segmentation. Finally, the 2D ResUNet refines the segmentation. Experiments on T2-weighted volumetric MR images of 215 subjects show that SpineParseNet achieves impressive performance with mean Dice similarity coefficients of 87.32 ± 4.75%, 87.78 ± 4.64%, and 87.49 ± 3.81% for the segmentations of 10 vertebrae, 9 IVDs, and all 19 spinal structures respectively. The proposed method has great potential in clinical spinal disease diagnoses and treatments.",
  "keyword": "image processing"
 },
 {
  "No": 322,
  "judul": "Improving Optoacoustic Image Quality via Geometric Pixel Super-Resolution Approach",
  "abstrak": "High fidelity optoacoustic (photoacoustic) tomography requires dense spatial sampling of optoacoustic signals using point acoustic detectors. However, in practice, spatial resolution of the images is often limited by limited sampling either due to coarse multi-element arrays or time in raster scan measurements. Herein, we investigate a method that integrates information from multiple optoacoustic images acquired at sub-diffraction steps into one high resolution image by means of an iterative registration algorithm. Experimental validations performed in target phantoms and ex vivo tissue samples confirm that the suggested approach renders significant improvements in terms of optoacoustic image resolution and quality without introducing significant alterations into the signal acquisition hardware or inversion algorithms.",
  "keyword": "image processing"
 },
 {
  "No": 323,
  "judul": "Reconstructing the 3D Shape and Bone Mineral Density Distribution of the Proximal Femur From Dual-Energy X-Ray Absorptiometry",
  "abstrak": "The accurate diagnosis of osteoporosis has gained increasing importance due to the aging of our society. Areal bone mineral density (BMD) measured by dual-energy X-ray absorptiometry (DXA) is an established criterion in the diagnosis of osteoporosis. This measure, however, is limited by its two-dimensionality. This work presents a method to reconstruct both the 3D bone shape and 3D BMD distribution of the proximal femur from a single DXA image used in clinical routine. A statistical model of the combined shape and BMD distribution is presented, together with a method for its construction from a set of quantitative computed tomography (QCT) scans. A reconstruction is acquired in an intensity based 3D-2D registration process whereby an instance of the model is found that maximizes the similarity between its projection and the DXA image. Reconstruction experiments were performed on the DXA images of 30 subjects, with a model constructed from a database of QCT scans of 85 subjects. The accuracy was evaluated by comparing the reconstructions with the same subject QCT scans. The method presented here can potentially improve the diagnosis of osteoporosis and fracture risk assessment from the low radiation dose and low cost DXA devices currently used in clinical routine.",
  "keyword": "image processing"
 },
 {
  "No": 324,
  "judul": "Interactive Image Segmentation Using Dirichlet Process Multiple-View Learning",
  "abstrak": "Segmenting semantically meaningful whole objects from images is a challenging problem, and it becomes especially so without higher level common sense reasoning. In this paper, we present an interactive segmentation framework that integrates image appearance and boundary constraints in a principled way to address this problem. In particular, we assume that small sets of pixels, which are referred to as seed pixels, are labeled as the object and background. The seed pixels are used to estimate the labels of the unlabeled pixels using Dirichlet process multiple-view learning, which leverages 1) multiple-view learning that integrates appearance and boundary constraints and 2) Dirichlet process mixture-based nonlinear classification that simultaneously models image features and discriminates between the object and background classes. With the proposed learning and inference algorithms, our segmentation framework is experimentally shown to produce both quantitatively and qualitatively promising results on a standard dataset of images. In particular, our proposed framework is able to segment whole objects from images given insufficient seeds.",
  "keyword": "image processing"
 },
 {
  "No": 325,
  "judul": "PDE-Based Enhancement of Color Images in RGB Space",
  "abstrak": "A novel method for color image enhancement is proposed as an extension of the scalar-diffusion–shock-filter coupling model, where noisy and blurred images are denoised and sharpened. The proposed model is based on using the single vectors of the gradient magnitude and the second derivatives as a manner to relate different color components of the image. This model can be viewed as a generalization of the Bettahar–Stambouli filter to multivalued images. The proposed algorithm is more efficient than the mentioned filter and some previous works at color images denoising and deblurring without creating false colors.",
  "keyword": "image processing"
 },
 {
  "No": 326,
  "judul": "IMAGE Resolution Enhancement by Using Discrete and Stationary Wavelet Decomposition",
  "abstrak": "In this correspondence, the authors propose an image resolution enhancement technique based on interpolation of the high frequency subband images obtained by discrete wavelet transform (DWT) and the input image. The edges are enhanced by introducing an intermediate stage by using stationary wavelet transform (SWT). DWT is applied in order to decompose an input image into different subbands. Then the high frequency subbands as well as the input image are interpolated. The estimated high frequency subbands are being modified by using high frequency subband obtained through SWT. Then all these subbands are combined to generate a new high resolution image by using inverse DWT (IDWT). The quantitative and visual results are showing the superiority of the proposed technique over the conventional and state-of-art image resolution enhancement techniques.",
  "keyword": "image processing"
 },
 {
  "No": 327,
  "judul": "Exponential Contrast Restoration in Fog Conditions for Driving Assistance",
  "abstrak": "The images captured in fog conditions have degraded contrast, which makes current image processing applications sensitive and error prone. We propose in this paper an efficient single-image enhancement algorithm suitable for daytime fog conditions and, based on an original mathematical model, for computing the atmospheric veil, taking into account the variation in fog density to the distance. This model is inspired by the functions that appear in partition of unity in the differential geometry field. When observing images captured in fog conditions, usually the fog has a very low density in front of the camera and this density has a nonlinear increase with the distance, such that objects are no longer visible at greater distances. By using our mathematical model, we are able to obtain superior reconstructions of the original fog-free image when compared with traditional methods. Another advantage of our method is the ability to adapt the model in accordance to the density of the fog. A quantitative and qualitative evaluation is performed on both synthetic and real camera images. This evaluation proves that our mathematical model is more suitable for contrast restoration in both homogeneous and heterogeneous fog conditions. Our algorithm is able to perform contrast restoration in real time for both color and grayscale images.",
  "keyword": "image processing"
 },
 {
  "No": 328,
  "judul": "Optimized Plane Wave Imaging for Fast and High-Quality Ultrasound Imaging",
  "abstrak": "This paper presents a method for optimizing parameters affecting the image quality in plane wave imaging. More specifically, the number of emissions and steering angles is optimized to attain the best images with the highest frame rate possible. The method is applied to a specific problem, where image quality for a λ-pitch transducer is compared with a λ\/2-pitch transducer. Grating lobe artifacts for λ-pitch transducers degrade the contrast in plane wave images, and the impact on frame rate is studied. FieldII simulations of plane wave images are made for all combinations of the parameters, and the optimal setup is selected based on Pareto optimality. The optimal setup for a simulated 4.1-MHz λ-pitch transducer uses 61 emissions and a maximum steering angle of 20° for depths from 0 to 60 mm. The achieved lateral full-width at half-maximum (FWHM) is 1.5λ and the contrast is -29 dB for a scatterer at 9 mm (24λ). Using a λ\/2-pitch transducer and only 21 emissions within the same angle range, the image quality is improved in terms of contrast, which is -37 dB. For imaging in regions deeper than 25 mm (66λ), only 21 emissions are optimal for both the transducers, resulting in a -36 dB contrast at 34 mm (90λ). Measurements are performed using the experimental SARUS scanner connected to a λ-pitch and λ\/2-pitch transducer. A wire phantom and a tissue mimicking phantom containing anechoic cysts are scanned and show the performance using the optimized sequences for the transducers. FWHM is 1.6λ and contrast is -25 dB for a wire at 9 mm using the λ-pitch transducer. For the λ\/2-pitch transducer, contrast is -29 dB. In vivo scans of the carotid artery of a healthy volunteer show improved contrast and present fewer artifacts, when using the λ\/2-pitch transducer compared with the λ-pitch. It is demonstrated with a frame rate, which is three times higher for the λ\/2-pitch transducer.",
  "keyword": "image processing"
 },
 {
  "No": 329,
  "judul": "Geodesic Active Fields—A Geometric Framework for Image Registration",
  "abstrak": "In this paper we present a novel geometric framework called geodesic active fields for general image registration. In image registration, one looks for the underlying deformation field that best maps one image onto another. This is a classic ill-posed inverse problem, which is usually solved by adding a regularization term. Here, we propose a multiplicative coupling between the registration term and the regularization term, which turns out to be equivalent to embed the deformation field in a weighted minimal surface problem. Then, the deformation field is driven by a minimization flow toward a harmonic map corresponding to the solution of the registration problem. This proposed approach for registration shares close similarities with the well-known geodesic active contours model in image segmentation, where the segmentation term (the edge detector function) is coupled with the regularization term (the length functional) via multiplication as well. As a matter of fact, our proposed geometric model is actually the exact mathematical generalization to vector fields of the weighted length problem for curves and surfaces introduced by Caselles-Kimmel-Sapiro. The energy of the deformation field is measured with the Polyakov energy weighted by a suitable image distance, borrowed from standard registration models. We investigate three different weighting functions, the squared error and the approximated absolute error for monomodal images, and the local joint entropy for multimodal images. As compared to specialized state-of-the-art methods tailored for specific applications, our geometric framework involves important contributions. Firstly, our general formulation for registration works on any parametrizable, smooth and differentiable surface, including nonflat and multiscale images. In the latter case, multiscale images are registered at all scales simultaneously, and the relations between space and scale are intrinsically being accounted for. Second, this method is, to t...",
  "keyword": "image processing"
 },
 {
  "No": 330,
  "judul": "Difference Value Network for Image Super-Resolution",
  "abstrak": "Recently, improved performance has been achieved in image super-resolution (SR) by using deep convolutional neural networks (CNNs). However, most existing networks neglect the feature correlations of adjacent layers, causing features at different levels to not be fully utilized. In this paper, a novel difference value network (DVN) is proposed to address this problem. The proposed network makes full use of different levels of features by using the difference values (D-values) of adjacent layers. Specifically, a difference value block (DVB) is designed to extract the difference values of adjacent layers. The extracted difference value can highlight which regions should be paid more attention to, so as to guide image SR. Further, a difference value group (DVG) is designed to integrate the difference values extracted by the difference value block into its output. In this way, the DVG can provide additional structure prior for image SR. Finally, to make our network more stable, a multipath supervised reconstruction block is proposed to supervise the reconstruction process. The experimental results on five benchmark datasets show that the proposed network can achieve better reconstruction results than the compared SR methods.",
  "keyword": "image processing"
 },
 {
  "No": 331,
  "judul": "Visual Quality Enhancement in Optoacoustic Tomography Using Active Contour Segmentation Priors",
  "abstrak": "Segmentation of biomedical images is essential for studying and characterizing anatomical structures as well as for detection and evaluation of tissue pathologies. Segmentation has been further shown to enhance the reconstruction performance in many tomographic imaging modalities by accounting for heterogeneities in the excitation field and tissue properties in the imaged region. This is particularly relevant in optoacoustic tomography, where discontinuities in the optical and acoustic tissue properties, if not properly accounted for, may result in deterioration of the imaging performance. Efficient segmentation of optoacoustic images is often hampered by the relatively low intrinsic contrast of large anatomical structures, which is further impaired by the limited angular coverage of some commonly employed tomographic imaging configurations. Herein, we analyze the performance of active contour models for boundary segmentation in cross-sectional optoacoustic tomography. The segmented mask is employed to construct a two compartment model for the acoustic and optical parameters of the imaged tissues, which is subsequently used to improve accuracy of the image reconstruction routines. The performance of the suggested segmentation and modeling approach are showcased in tissue-mimicking phantoms and small animal imaging experiments.",
  "keyword": "image processing"
 },
 {
  "No": 332,
  "judul": "Partial FOV Center Imaging (PCI): A Robust X-Space Image Reconstruction for Magnetic Particle Imaging",
  "abstrak": "Magnetic Particle Imaging (MPI) is an emerging medical imaging modality that images the spatial distribution of superparamagnetic iron oxide (SPIO) nanoparticles using their nonlinear response to applied magnetic fields. In standard x-space approach to MPI, the image is reconstructed by gridding the speed-compensated nanoparticle signal to the instantaneous position of the field free point (FFP). However, due to safety limits on the drive field, the field-of-view (FOV) needs to be covered by multiple relatively small partial field-of-views (pFOVs). The image of the entire FOV is then pieced together from individually processed pFOVs. These processing steps can be sensitive to non-ideal signal conditions such as harmonic interference, noise, and relaxation effects. In this work, we propose a robust x-space reconstruction technique, Partial FOV Center Imaging (PCI), with substantially simplified pFOV processing. PCI first forms a raw image of the entire FOV by mapping MPI signal directly to pFOV center locations. The corresponding MPI image is then obtained by deconvolving this raw image by a compact kernel, whose fully-known shape solely depends on the pFOV size. We analyze the performance of the proposed reconstruction via extensive simulations, as well as imaging experiments on our in-house FFP MPI scanner. The results show that PCI offers a trade-off between noise robustness and interference robustness, outperforming standard x-space reconstruction in terms of both robustness against non-ideal signal conditions and image quality.",
  "keyword": "image processing"
 },
 {
  "No": 333,
  "judul": "A Generalized Metaphor of Chinese Restaurant Franchise to Fusing Both Panchromatic and Multispectral Images for Unsupervised Classification",
  "abstrak": "Two-step ways are often used for fusing both panchromatic (PAN) and multispectral (MS) images for classification, e.g., classifying MS images sharpened by PAN images or directly pouring fine spatial details of PAN images into a classification result of MS images. In this paper, we present a unified Bayesian framework to iteratively discovering semantic segments from PAN images and allocating cluster labels for the segments using MS images. Specifically, the probabilistic generative process of both PAN and MS images is explained with a generalized metaphor of the Chinese restaurant franchise (CRF) (gCRF), in which the two iterative random processes, i.e., table selection and dish selection, are adapted to discovering semantic segments in PAN images and inferring cluster labels for the discovered segments using MS images, respectively. Our major contributions are twofold: 1) The CRF is generalized into an image fusion framework by elegantly decomposing its two random processes, and 2) the random process of table selection in the CRF is transformed into stochastic image segmentation by enforcing spatial constraints over adjacent pixels. The qualitative analysis of experimental results shows that the gCRF can effectively utilize both the spatial details of the PAN images and the spectral information of the MS images. In terms of quantitative evaluation, the gCRF is comparable with support vector machine-based supervised classification methods.",
  "keyword": "image processing"
 },
 {
  "No": 334,
  "judul": "Image enhancement and denoising by complex diffusion processes",
  "abstrak": "The linear and nonlinear scale spaces, generated by the inherently real-valued diffusion equation, are generalized to complex diffusion processes, by incorporating the free Schrodinger equation. A fundamental solution for the linear case of the complex diffusion equation is developed. Analysis of its behavior shows that the generalized diffusion process combines properties of both forward and inverse diffusion. We prove that the imaginary part is a smoothed second derivative, scaled by time, when the complex diffusion coefficient approaches the real axis. Based on this observation, we develop two examples of nonlinear complex processes, useful in image processing: a regularized shock filter for image enhancement and a ramp preserving denoising process.",
  "keyword": "image processing"
 },
 {
  "No": 335,
  "judul": "Multi-Scale Tokens-Aware Transformer Network for Multi-Region and Multi-Sequence MR-to-CT Synthesis in a Single Model",
  "abstrak": "The superiority of magnetic resonance (MR)-only radiotherapy treatment planning (RTP) has been well demonstrated, benefiting from the synthesis of computed tomography (CT) images which supplements electron density and eliminates the errors of multi-modal images registration. An increasing number of methods has been proposed for MR-to-CT synthesis. However, synthesizing CT images of different anatomical regions from MR images with different sequences using a single model is challenging due to the large differences between these regions and the limitations of convolutional neural networks in capturing global context information. In this paper, we propose a multi-scale tokens-aware Transformer network (MTT-Net) for multi-region and multi-sequence MR-to-CT synthesis in a single model. Specifically, we develop a multi-scale image tokens Transformer to capture multi-scale global spatial information between different anatomical structures in different regions. Besides, to address the limited attention areas of tokens in Transformer, we introduce a multi-shape window self-attention into Transformer to enlarge the receptive fields for learning the multi-directional spatial representations. Moreover, we adopt a domain classifier in generator to introduce the domain knowledge for distinguishing the MR images of different regions and sequences. The proposed MTT-Net is evaluated on a multi-center dataset and an unseen region, and remarkable performance was achieved with MAE of 69.33 ± 10.39 HU, SSIM of 0.778 ± 0.028, and PSNR of 29.04 ± 1.32 dB in head & neck region, and MAE of 62.80 ± 7.65 HU, SSIM of 0.617 ± 0.058 and PSNR of 25.94 ± 1.02 dB in abdomen region. The proposed MTT-Net outperforms state-of-the-art methods in both accuracy and visual quality.",
  "keyword": "image processing"
 },
 {
  "No": 336,
  "judul": "Orthogonal Rotation-Invariant Moments for Digital Image Processing",
  "abstrak": "Orthogonal rotation-invariant moments (ORIMs), such as Zernike moments, are introduced and defined on a continuous unit disk and have been proven powerful tools in optics applications. These moments have also been digitized for applications in digital image processing. Unfortunately, digitization compromises the orthogonality of the moments and, therefore, digital ORIMs are incapable of representing subtle details in images and cannot accurately reconstruct images. Typical approaches to alleviate the digitization artifact can be divided into two categories: 1) careful selection of a set of pixels as close approximation to the unit disk and using numerical integration to determine the ORIM values, and 2) representing pixels using circular shapes such that they resemble that of the unit disk and then calculating ORIMs in polar space. These improvements still fall short of preserving the orthogonality of the ORIMs. In this paper, in contrast to the previous methods, we propose a different approach of using numerical optimization techniques to improve the orthogonality. We prove that with the improved orthogonality, image reconstruction becomes more accurate. Our simulation results also show that the optimized digital ORIMs can accurately reconstruct images and can represent subtle image details.",
  "keyword": "image processing"
 },
 {
  "No": 337,
  "judul": "Reverse-Time Migration Based Optical Imaging",
  "abstrak": "We theoretically demonstrated a new optical imaging technique based on reverse-time migration (RTM) for reconstructing optical structures in homogeneous media for the first time. RTM is a powerful wave-equation-based method to reconstruct the image of the structure by modeling the wave propagation inside the media with both forward modeling and reverse-time extrapolation. While RTM is commonly used with acoustic seismic waves, this paper represents the first effort to develop optical RTM imaging method for biomedical research. To refine the image quality, we further developed new methods to suppress the low-wavenumber artifact (LWA). When compared with the conventional means for LWA suppression such as Laplacian filtering, illumination normalization, and the ratio method, our new derivative-based and power-image methods are able to significantly reduce LWA, resulting in high-quality reconstructed images with sufficient contrasts and spatial resolutions for structure identification. The optical RTM imaging technique may provide a new platform for non-invasive optical imaging of structures in deep layers of tissues for biomedical applications.",
  "keyword": "image processing"
 },
 {
  "No": 338,
  "judul": "A High-Dynamic-Range-Based Approach for the Display of Hyperspectral Images",
  "abstrak": "This letter presents a new approach for hyperspectral image visualization based on high-dynamic-range (HDR) image processing. The proposed approach is inspired by techniques that are used to display HDR images on low-dynamic-range media by reducing the contrast, yet preserving the image detail. This is the first time this concept is utilized for the display of hyperspectral images. In the presented approach, an edge-preserving filter, called the bilateral filter, is used to extract the base and detail images, and the final image is reconstructed by reducing contrast in the base image but preserving the detail, so that the significance of the detail image is enhanced. It is shown that the proposed approach improves the visual appearance and perceived detail of hyperspectral images.",
  "keyword": "image processing"
 },
 {
  "No": 339,
  "judul": "OIF-Net: An Optical Flow Registration-Based PET\/MR Cross-Modal Interactive Fusion Network for Low-Count Brain PET Image Denoising",
  "abstrak": "The short frames of low-count positron emission tomography (PET) images generally cause high levels of statistical noise. Thus, improving the quality of low-count images by using image postprocessing algorithms to achieve better clinical diagnoses has attracted widespread attention in the medical imaging community. Most existing deep learning-based low-count PET image enhancement methods have achieved satisfying results, however, few of them focus on denoising low-count PET images with the magnetic resonance (MR) image modality as guidance. The prior context features contained in MR images can provide abundant and complementary information for single low-count PET image denoising, especially in ultralow-count (2.5%) cases. To this end, we propose a novel two-stream dual PET\/MR cross-modal interactive fusion network with an optical flow pre-alignment module, namely, OIF-Net. Specifically, the learnable optical flow registration module enables the spatial manipulation of MR imaging inputs within the network without any extra training supervision. Registered MR images fundamentally solve the problem of feature misalignment in the multimodal fusion stage, which greatly benefits the subsequent denoising process. In addition, we design a spatial-channel feature enhancement module (SC-FEM) that considers the interactive impacts of multiple modalities and provides additional information flexibility in both the spatial and channel dimensions. Furthermore, instead of simply concatenating two extracted features from these two modalities as an intermediate fusion method, the proposed cross-modal feature fusion module (CM-FFM) adopts cross-attention at multiple feature levels and greatly improves the two modalities’ feature fusion procedure. Extensive experimental assessments conducted on real clinical datasets, as well as an independent clinical testing dataset, demonstrate that the proposed OIF-Net outperforms the state-of-the-art methods.",
  "keyword": "image processing"
 },
 {
  "No": 340,
  "judul": "Uncluttered Single-Image Visualization of Vascular Structures Using GPU and Integer Programming",
  "abstrak": "Direct projection of 3D branching structures, such as networks of cables, blood vessels, or neurons onto a 2D image creates the illusion of intersecting structural parts and creates challenges for understanding and communication. We present a method for visualizing such structures, and demonstrate its utility in visualizing the abdominal aorta and its branches, whose tomographic images might be obtained by computed tomography or magnetic resonance angiography, in a single 2D stylistic image, without overlaps among branches. The visualization method, termed uncluttered single-image visualization (USIV), involves optimization of geometry. This paper proposes a novel optimization technique that utilizes an interesting connection of the optimization problem regarding USIV to the protein structure prediction problem. Adopting the integer linear programming-based formulation for the protein structure prediction problem, we tested the proposed technique using 30 visualizations produced from five patient scans with representative anatomical variants in the abdominal aortic vessel tree. The novel technique can exploit commodity-level parallelism, enabling use of general-purpose graphics processing unit (GPGPU) technology that yields a significant speedup. Comparison of the results with the other optimization technique previously reported elsewhere suggests that, in most aspects, the quality of the visualization is comparable to that of the previous one, with a significant gain in the computation time of the algorithm.",
  "keyword": "image processing"
 },
 {
  "No": 341,
  "judul": "Hyperconnections and Hierarchical Representations for Grayscale and Multiband Image Processing",
  "abstrak": "Connections in image processing are an important notion that describes how pixels can be grouped together according to their spatial relationships and\/or their gray-level values. In recent years, several works were devoted to the development of new theories of connections among which hyperconnection (h-connection) is a very promising notion. This paper addresses two major issues of this theory. First, we propose a new axiomatic that ensures that every h-connection generates decompositions that are consistent for image processing and, more precisely, for the design of h-connected filters. Second, we develop a general framework to represent the decomposition of an image into h-connections as a tree that corresponds to the generalization of the connected component tree. Such trees are indeed an efficient and intuitive way to design attribute filters or to perform detection tasks based on qualitative or quantitative attributes. These theoretical developments are applied to a particular fuzzy h-connection, and we test this new framework on several classical applications in image processing, i.e., segmentation, connected filtering, and document image binarization. The experiments confirm the suitability of the proposed approach: It is robust to noise, and it provides an efficient framework to design selective filters.",
  "keyword": "image processing"
 },
 {
  "No": 342,
  "judul": "Sparse Stochastic Processes and Discretization of Linear Inverse Problems",
  "abstrak": "We present a novel statistically-based discretization paradigm and derive a class of maximum a posteriori (MAP) estimators for solving ill-conditioned linear inverse problems. We are guided by the theory of sparse stochastic processes, which specifies continuous-domain signals as solutions of linear stochastic differential equations. Accordingly, we show that the class of admissible priors for the discretized version of the signal is confined to the family of infinitely divisible distributions. Our estimators not only cover the well-studied methods of Tikhonov and \nl\n1\n-type regularizations as particular cases, but also open the door to a broader class of sparsity-promoting regularization schemes that are typically nonconvex. We provide an algorithm that handles the corresponding nonconvex problems and illustrate the use of our formalism by applying it to deconvolution, magnetic resonance imaging, and X-ray tomographic reconstruction problems. Finally, we compare the performance of estimators associated with models of increasing sparsity.",
  "keyword": "image processing"
 },
 {
  "No": 343,
  "judul": "SMU-Net: Saliency-Guided Morphology-Aware U-Net for Breast Lesion Segmentation in Ultrasound Image",
  "abstrak": "Deep learning methods, especially convolutional neural networks, have been successfully applied to lesion segmentation in breast ultrasound (BUS) images. However, pattern complexity and intensity similarity between the surrounding tissues (i.e., background) and lesion regions (i.e., foreground) bring challenges for lesion segmentation. Considering that such rich texture information is contained in background, very few methods have tried to explore and exploit background-salient representations for assisting foreground segmentation. Additionally, other characteristics of BUS images, i.e., 1) low-contrast appearance and blurry boundary, and 2) significant shape and position variation of lesions, also increase the difficulty in accurate lesion segmentation. In this paper, we present a saliency-guided morphology-aware U-Net (SMU-Net) for lesion segmentation in BUS images. The SMU-Net is composed of a main network with an additional middle stream and an auxiliary network. Specifically, we first propose generation of saliency maps which incorporate both low-level and high-level image structures, for foreground and background. These saliency maps are then employed to guide the main network and auxiliary network for respectively learning foreground-salient and background-salient representations. Furthermore, we devise an additional middle stream which basically consists of background-assisted fusion, shape-aware, edge-aware and position-aware units. This stream receives the coarse-to-fine representations from the main network and auxiliary network for efficiently fusing the foreground-salient and background-salient features and enhancing the ability of learning morphological information for network. Extensive experiments on five datasets demonstrate higher performance and superior robustness to the scale of dataset than several state-of-the-art deep learning approaches in breast lesion segmentation in ultrasound image.",
  "keyword": "image processing"
 },
 {
  "No": 344,
  "judul": "Image guidance of endovascular interventions on a clinical MR scanner",
  "abstrak": "Magnetic resonance imaging (MRI) offers potential advantages over conventional X-ray techniques for guiding and evaluating vascular interventions. Image guidance of such interventions via passive catheter tracking requires real-time image processing. Commercially available MR scanners currently do not provide this functionality. This paper describes an image processing environment that allows near-real-time MR-guided vascular interventions. It demonstrates (1) that flexibility can be achieved by separating the scanner and the image processing\/display system, thereby preserving the stability of the scanner and (2) that sufficiently rapid visualization can be achieved by low-cost workstations equipped with graphics hardware. The setup of the hardware and the software is described in detail. Furthermore, image processing techniques are presented for guiding the interventionalist through simple vascular anatomy. Finally, results of a phantom balloon angioplasty experiment are presented.",
  "keyword": "image processing"
 },
 {
  "No": 345,
  "judul": "A sensor array processing approach to object region detection",
  "abstrak": "This paper presents a new approach for medical image analysis. It translates the object region-detection problem into a sensor array processing framework and detects the number of object regions based on the signal eigenstructure of the converted array system. The theoretical and experimental results obtained by using this approach on various medical images were in good agreement.",
  "keyword": "image processing"
 },
 {
  "No": 346,
  "judul": "Digital image processing techniques for estimating power released from the corona discharges",
  "abstrak": "Digital image processing is being widely used in all fields of science and engineering, like the detection of cancer cells, identification of the soil properties etc. Application of image processing to high voltage phenomenon has not been well explored. The present work attempts to identify the possible application of image processing techniques to obtain the image parameters which can be used as indicators to estimate the corona power. A color thresholding based image segmentation method is employed on the digital images of corona and thereby effective spread of corona plasma has been extracted. The variations in the parameters: average intensity, area of the spread, energy, entropy and luminosity content in the processed images are studied in relation with the measured power. It is observed that the parameters extracted from the corona images share a unique relation with the measured corona power, and can be used for the prediction analysis.",
  "keyword": "image processing"
 },
 {
  "No": 347,
  "judul": "Adaptive Image Registration via Hierarchical Voronoi Subdivision",
  "abstrak": "Advances in image acquisition systems have made it possible to capture high-resolution images of a scene, recording considerable scene details. With increased resolution comes increased image size and geometric difference between multiview images, complicating image registration. Through Voronoi subdivision, we subdivide large images into small corresponding regions, and by registering small regions, we register the images in a piecewise manner. Image subdivision reduces the geometric difference between regions that are registered and simplifies the correspondence process. The proposed method is a hierarchical one. While previous methods use the same block size and shape at a hierarchy, the proposed method adapts the block size and shape to the local image details and geometric difference between the images. This adaptation makes it possible to keep geometric difference between corresponding regions small and simplifies the correspondence process. Implementational details of the proposed image registration method are provided, and experimental results on various types of images are presented and analyzed.",
  "keyword": "image processing"
 },
 {
  "No": 348,
  "judul": "Design and Demonstration of a Configurable Imaging Platform for Combined Laser, Ultrasound, and Elasticity Imaging",
  "abstrak": "This paper introduces a configurable combined laser, ultrasound, and elasticity (CLUE) imaging platform. The CLUE platform enables imaging sequences capable of simultaneously providing quantitative acoustic, optical, and mechanical contrast for comprehensive diagnosis and monitoring of complex diseases, such as cancer. The CLUE imaging platform was developed on a Verasonics ultrasound scanner integrated with a pulsed laser, and it was designed to be modular and scalable to allow researchers to create their own specific imaging sequences efficiently. The CLUE imaging platform and sequence were demonstrated in a tissue-mimicking phantom containing a stiff inclusion labeled with optically-activated nanodroplets and in an ex vivo mouse spleen. We have shown that CLUE imaging can simultaneously capture multi-functional imaging signals providing quantitative information on tissue.",
  "keyword": "image processing"
 },
 {
  "No": 349,
  "judul": "CNN-Based Ultrasound Image Reconstruction for Ultrafast Displacement Tracking",
  "abstrak": "Thanks to its capability of acquiring full-view frames at multiple kilohertz, ultrafast ultrasound imaging unlocked the analysis of rapidly changing physical phenomena in the human body, with pioneering applications such as ultrasensitive flow imaging in the cardiovascular system or shear-wave elastography. The accuracy achievable with these motion estimation techniques is strongly contingent upon two contradictory requirements: a high quality of consecutive frames and a high frame rate. Indeed, the image quality can usually be improved by increasing the number of steered ultrafast acquisitions, but at the expense of a reduced frame rate and possible motion artifacts. To achieve accurate motion estimation at uncompromised frame rates and immune to motion artifacts, the proposed approach relies on single ultrafast acquisitions to reconstruct high-quality frames and on only two consecutive frames to obtain 2-D displacement estimates. To this end, we deployed a convolutional neural network-based image reconstruction method combined with a speckle tracking algorithm based on cross-correlation. Numerical and in vivo experiments, conducted in the context of plane-wave imaging, demonstrate that the proposed approach is capable of estimating displacements in regions where the presence of side lobe and grating lobe artifacts prevents any displacement estimation with a state-of-the-art technique that relies on conventional delay-and-sum beamforming. The proposed approach may therefore unlock the full potential of ultrafast ultrasound, in applications such as ultrasensitive cardiovascular motion and flow analysis or shear-wave elastography.",
  "keyword": "image processing"
 },
 {
  "No": 350,
  "judul": "Sparse Matrix Beamforming and Image Reconstruction for 2-D HIFU Monitoring Using Harmonic Motion Imaging for Focused Ultrasound (HMIFU) With In Vitro Validation",
  "abstrak": "Harmonic motion imaging for focused ultrasound (HMIFU) utilizes an amplitude-modulated HIFU beam to induce a localized focal oscillatory motion simultaneously estimated. The objective of this study is to develop and show the feasibility of a novel fast beamforming algorithm for image reconstruction using GPU-based sparse-matrix operation with real-time feedback. In this study, the algorithm was implemented onto a fully integrated, clinically relevant HMIFU system. A single divergent transmit beam was used while fast beamforming was implemented using a GPU-based delay-and-sum method and a sparse-matrix operation. Axial HMI displacements were then estimated from the RF signals using a 1-D normalized cross-correlation method and streamed to a graphic user interface with frame rates up to 15 Hz, a 100-fold increase compared to conventional CPU-based processing. The real-time feedback rate does not require interrupting the HIFU treatment. Results in phantom experiments showed reproducible HMI images and monitoring of 22 in vitro HIFU treatments using the new 2-D system demonstrated reproducible displacement imaging, and monitoring of 22 in vitro HIFU treatments using the new 2-D system showed a consistent average focal displacement decrease of 46.7 ±14.6% during lesion formation. Complementary focal temperature monitoring also indicated an average rate of displacement increase and decrease with focal temperature at 0.84±1.15%\/ ° C, and 2.03±0.93%\/ ° C, respectively. These results reinforce the HMIFU capability of estimating and monitoring stiffness related changes in real time. Current ongoing studies include clinical translation of the presented system for monitoring of HIFU treatment for breast and pancreatic tumor applications.",
  "keyword": "image processing"
 },
 {
  "No": 351,
  "judul": "Multi-Scale Tokens-Aware Transformer Network for Multi-Region and Multi-Sequence MR-to-CT Synthesis in a Single Model",
  "abstrak": "The superiority of magnetic resonance (MR)-only radiotherapy treatment planning (RTP) has been well demonstrated, benefiting from the synthesis of computed tomography (CT) images which supplements electron density and eliminates the errors of multi-modal images registration. An increasing number of methods has been proposed for MR-to-CT synthesis. However, synthesizing CT images of different anatomical regions from MR images with different sequences using a single model is challenging due to the large differences between these regions and the limitations of convolutional neural networks in capturing global context information. In this paper, we propose a multi-scale tokens-aware Transformer network (MTT-Net) for multi-region and multi-sequence MR-to-CT synthesis in a single model. Specifically, we develop a multi-scale image tokens Transformer to capture multi-scale global spatial information between different anatomical structures in different regions. Besides, to address the limited attention areas of tokens in Transformer, we introduce a multi-shape window self-attention into Transformer to enlarge the receptive fields for learning the multi-directional spatial representations. Moreover, we adopt a domain classifier in generator to introduce the domain knowledge for distinguishing the MR images of different regions and sequences. The proposed MTT-Net is evaluated on a multi-center dataset and an unseen region, and remarkable performance was achieved with MAE of 69.33 ± 10.39 HU, SSIM of 0.778 ± 0.028, and PSNR of 29.04 ± 1.32 dB in head & neck region, and MAE of 62.80 ± 7.65 HU, SSIM of 0.617 ± 0.058 and PSNR of 25.94 ± 1.02 dB in abdomen region. The proposed MTT-Net outperforms state-of-the-art methods in both accuracy and visual quality.",
  "keyword": "image processing"
 },
 {
  "No": 352,
  "judul": "Adaptive Image Resizing Based on Continuous-Domain Stochastic Modeling",
  "abstrak": "We introduce an adaptive continuous-domain modeling approach to texture and natural images. The continuous-domain image is assumed to be a smooth function, and we embed it in a parameterized Sobolev space. We point out a link between Sobolev spaces and stochastic auto-regressive models, and exploit it for optimally choosing Sobolev parameters from available pixel values. To this aim, we use exact continuous-to-discrete mapping of the auto-regressive model that is based on symmetric exponential splines. The mapping is computationally efficient, and we exploit it for maximizing an approximated Gaussian likelihood function. We account for non-Gaussian Lévy-type processes by deriving a more robust estimator that is based on the sample auto-correlation sequence. Both estimators use multiple initialization values for overcoming the local minima structure of the fitting criteria. Experimental image resizing results indicate that the auto-correlation criterion can cope better with non-Gaussian processes and model mismatch. Our work demonstrates the importance of the auto-correlation function in adaptive image interpolation and image modeling tasks, and we believe it is instrumental in other image processing tasks as well.",
  "keyword": "image processing"
 },
 {
  "No": 353,
  "judul": "Cross-Modality Image Registration Using a Training-Time Privileged Third Modality",
  "abstrak": "In this work, we consider the task of pairwise cross-modality image registration, which may benefit from exploiting additional images available only at training time from an additional modality that is different to those being registered. As an example, we focus on aligning intra-subject multiparametric Magnetic Resonance (mpMR) images, between T2-weighted (T2w) scans and diffusion-weighted scans with high b-value (DWI\n high-b \n). For the application of localising tumours in mpMR images, diffusion scans with zero b-value (DWI\n b=0 \n) are considered easier to register to T2w due to the availability of corresponding features. We propose a learning from privileged modality algorithm, using a training-only imaging modality DWI\n b=0 \n, to support the challenging multi-modality registration problems. We present experimental results based on 369 sets of 3D multiparametric MRI images from 356 prostate cancer patients and report, with statistical significance, a lowered median target registration error of 4.34 mm, when registering the holdout DWI\n high-b \n and T2w image pairs, compared with that of 7.96 mm before registration. Results also show that the proposed learning-based registration networks enabled efficient registration with comparable or better accuracy, compared with a classical iterative algorithm and other tested learning-based methods with\/without the additional modality. These compared algorithms also failed to produce any significantly improved alignment between DWI\n high-b \n and T2w in this challenging application.",
  "keyword": "image processing"
 },
 {
  "No": 354,
  "judul": "Goal-Oriented Rectification of Camera-Based Document Images",
  "abstrak": "Document digitization with either flatbed scanners or camera-based systems results in document images which often suffer from warping and perspective distortions that deteriorate the performance of current OCR approaches. In this paper, we present a goal-oriented rectification methodology to compensate for undesirable document image distortions aiming to improve the OCR result. Our approach relies upon a coarse-to-fine strategy. First, a coarse rectification is accomplished with the aid of a computationally low cost transformation which addresses the projection of a curved surface to a 2-D rectangular area. The projection of the curved surface on the plane is guided only by the textual content's appearance in the document image while incorporating a transformation which does not depend on specific model primitives or camera setup parameters. Second, pose normalization is applied on the word level aiming to restore all the local distortions of the document image. Experimental results on various document images with a variety of distortions demonstrate the robustness and effectiveness of the proposed rectification methodology using a consistent evaluation methodology that encounters OCR accuracy and a newly introduced measure using a semi-automatic procedure.",
  "keyword": "image processing"
 },
 {
  "No": 355,
  "judul": "Groupwise Multichannel Image Registration",
  "abstrak": "Multichannel image registration is an important challenge in medical image analysis. Multichannel images result from modalities such as dual-energy CT or multispectral microscopy. Besides, multichannel feature images can be derived from acquired images, for instance, by applying multiscale feature banks to the original images to register. Multichannel registration techniques have been proposed, but most of them are applicable to only two multichannel images at a time. In the present study, we propose to formulate multichannel registration as a groupwise image registration problem. In this way, we derive a method that allows the registration of two or more multichannel images in a fully symmetric manner (i.e., all images play the same role in the registration procedure), and therefore, has transitive consistency by definition. The method that we introduce is applicable to any number of multichannel images, any number of channels per image, and it allows to take into account correlation between any pair of images and not just corresponding channels. In addition, it is fully modular in terms of dissimilarity measure, transformation model, regularisation method, and optimisation strategy. For two multimodal datasets, we computed feature images from the initially acquired images, and applied the proposed registration technique to the newly created sets of multichannel images. MIND descriptors were used as feature images, and we chose total correlation as groupwise dissimilarity measure. Results show that groupwise multichannel image registration is a competitive alternative to the pairwise multichannel scheme, in terms of registration accuracy and insensitivity towards registration reference spaces.",
  "keyword": "image processing"
 },
 {
  "No": 356,
  "judul": "Quad-Net: Quad-Domain Network for CT Metal Artifact Reduction",
  "abstrak": "Metal implants and other high-density objects in patients introduce severe streaking artifacts in CT images, compromising image quality and diagnostic performance. Although various methods were developed for CT metal artifact reduction over the past decades, including the latest dual-domain deep networks, remaining metal artifacts are still clinically challenging in many cases. Here we extend the state-of-the-art dual-domain deep network approach into a quad-domain counterpart so that all the features in the sinogram, image, and their corresponding Fourier domains are synergized to eliminate metal artifacts optimally without compromising structural subtleties. Our proposed quad-domain network for MAR, referred to as Quad-Net, takes little additional computational cost since the Fourier transform is highly efficient, and works across the four receptive fields to learn both global and local features as well as their relations. Specifically, we first design a Sinogram-Fourier Restoration Network (SFR-Net) in the sinogram domain and its Fourier space to faithfully inpaint metal-corrupted traces. Then, we couple SFR-Net with an Image-Fourier Refinement Network (IFR-Net) which takes both an image and its Fourier spectrum to improve a CT image reconstructed from the SFR-Net output using cross-domain contextual information. Quad-Net is trained on clinical datasets to minimize a composite loss function. Quad-Net does not require precise metal masks, which is of great importance in clinical practice. Our experimental results demonstrate the superiority of Quad-Net over the state-of-the-art MAR methods quantitatively, visually, and statistically. The Quad-Net code is publicly available at https:\/\/github.com\/longzilicart\/Quad-Net .",
  "keyword": "image processing"
 },
 {
  "No": 357,
  "judul": "Structure-Preserving Guided Retinal Image Filtering and Its Application for Optic Disk Analysis",
  "abstrak": "Retinal fundus photographs have been used in the diagnosis of many ocular diseases such as glaucoma, pathological myopia, age-related macular degeneration, and diabetic retinopathy. With the development of computer science, computer aided diagnosis has been developed to process and analyze the retinal images automatically. One of the challenges in the analysis is that the quality of the retinal image is often degraded. For example, a cataract in human lens will attenuate the retinal image, just as a cloudy camera lens which reduces the quality of a photograph. It often obscures the details in the retinal images and posts challenges in retinal image processing and analyzing tasks. In this paper, we approximate the degradation of the retinal images as a combination of human-lens attenuation and scattering. A novel structure-preserving guided retinal image filtering (SGRIF) is then proposed to restore images based on the attenuation and scattering model. The proposed SGRIF consists of a step of global structure transferring and a step of global edge-preserving smoothing. Our results show that the proposed SGRIF method is able to improve the contrast of retinal images, measured by histogram flatness measure, histogram spread, and variability of local luminosity. In addition, we further explored the benefits of SGRIF for subsequent retinal image processing and analyzing tasks. In the two applications of deep learning-based optic cup segmentation and sparse learning-based cup-to-disk ratio (CDR) computation, our results show that we are able to achieve more accurate optic cup segmentation and CDR measurements from images processed by SGRIF.",
  "keyword": "image processing"
 },
 {
  "No": 358,
  "judul": "A Reproducing Kernel Hilbert Space Approach for Q-Ball Imaging",
  "abstrak": "Diffusion magnetic resonance (MR) imaging has enabled us to reveal the white matter geometry in the living human brain. The Q-ball technique is widely used nowadays to recover the orientational heterogeneity of the intra-voxel fiber architecture. This article proposes to employ the Funk-Radon transform in a Hilbert space with a reproducing kernel derived from the spherical Laplace-Beltrami operator, thus generalizing previous approaches that assume a bandlimited diffusion signal. The function estimation problem is solved within a Tikhonov regularization framework, while a Gaussian process model allows for the selection of the smoothing parameter and the specification of confidence bands. Shortcomings of Q-ball imaging are discussed.",
  "keyword": "image processing"
 },
 {
  "No": 359,
  "judul": "Multi-Modal Deep Guided Filtering for Comprehensible Medical Image Processing",
  "abstrak": "Deep learning-based image processing is capable of creating highly appealing results. However, it is still widely considered as a “blackbox” transformation. In medical imaging, this lack of comprehensibility of the results is a sensitive issue. The integration of known operators into the deep learning environment has proven to be advantageous for the comprehensibility and reliability of the computations. Consequently, we propose the use of the locally linear guided filter in combination with a learned guidance map for general purpose medical image processing. The output images are only processed by the guided filter while the guidance map can be trained to be task-optimal in an end-to-end fashion. We investigate the performance based on two popular tasks: image super resolution and denoising. The evaluation is conducted based on pairs of multi-modal magnetic resonance imaging and cross-modal computed tomography and magnetic resonance imaging datasets. For both tasks, the proposed approach is on par with state-of-the-art approaches. Additionally, we can show that the input image's content is almost unchanged after the processing which is not the case for conventional deep learning approaches. On top, the proposed pipeline offers increased robustness against degraded input as well as adversarial attacks.",
  "keyword": "image processing"
 },
 {
  "No": 360,
  "judul": "3D+t Morphological Processing: Applications to Embryogenesis Image Analysis",
  "abstrak": "We propose to directly process 3D+t image sequences with mathematical morphology operators, using a new classification of the 3D+t structuring elements. Several methods (filtering, tracking, segmentation) dedicated to the analysis of 3D+t datasets of zebrafish embryogenesis are introduced and validated through a synthetic dataset. Then, we illustrate the application of these methods to the analysis of datasets of zebrafish early development acquired with various microscopy techniques. This processing paradigm produces spatio-temporal coherent results as it benefits from the intrinsic redundancy of the temporal dimension, and minimizes the needs for human intervention in semi-automatic algorithms.",
  "keyword": "image processing"
 },
 {
  "No": 361,
  "judul": "Three-Layer Image Representation by an Enhanced Illumination-Based Image Fusion Method",
  "abstrak": "The recently developed multiscale-based fusion methods can be improved with two approaches: an advanced image decomposition scheme and an advanced fusion rule. In this paper, three-layer image decomposition, enhanced illumination fusion rule-based method is proposed. The proposed method includes three steps. First, each input image is decomposed into its corresponding smooth, texture, and edge layers using defined local extrema and low-pass filters in the spatial domain. Second, three different strategies are applied as fusion rules for the three-layer representation. To preserve the illumination closely related to tumors, the illumination is corrected by applying a higher contrast to the decomposed image details, including the texture and edge inputs, such as those found in grayscale CT and MRI images. The final fused image is created by the addition of the normalized smooth, texture, and edge image layers. The experiments demonstrate that the proposed method performs better than the existing state-of-the-art fusion methods.",
  "keyword": "image processing"
 },
 {
  "No": 362,
  "judul": "A Very Large Cardiac Channel Data Database (VLCD) Used to Evaluate Global Image Coherence (GIC) as an In Vivo Image Quality Metric",
  "abstrak": "Ultrasound image quality is of utmost importance for a clinician to reach a correct diagnosis. Conventionally, image quality is evaluated using metrics to determine the contrast and resolution. These metrics require localization of specific regions and targets in the image such as a region of interest (ROI), a background region, and\/or a point scatterer. Such objects can all be difficult to identify in in-vivo images, especially for automatic evaluation of image quality in large amounts of data. Using a matrix array probe, we have recorded a Very Large cardiac Channel data Database (VLCD) to evaluate coherence as an in vivo image quality metric. The VLCD consists of 33280 individual image frames from 538 recordings of 106 patients. We also introduce a global image coherence (GIC), an in vivo image quality metric that does not require any identified ROI since it is defined as an average coherence value calculated from all the data pixels used to form the image, below a preselected range. The GIC is shown to be a quantitative metric for in vivo image quality when applied to the VLCD. We demonstrate, on a subset of the dataset, that the GIC correlates well with the conventional metrics contrast ratio (CR) and the generalized contrast-to-noise ratio (gCNR) with \n R \n = 0.74 (\n p<0.005 \n) and \n R \n = 0.62 (\n p<0.005 \n), respectively. There exist multiple methods to estimate the coherence of the received signal across the ultrasound array. We further show that all coherence measures investigated in this study are highly correlated (\n R> \n 0.9 and \n p<0.001 \n) when applied to the VLCD. Thus, even though there are differences in the implementation of coherence measures, all quantify the similarity of the signal across the array and can be averaged into a GIC to evaluate image quality automatically and quantitatively.",
  "keyword": "image processing"
 },
 {
  "No": 363,
  "judul": "A Novel Software Platform for Medical Image Processing and Analyzing",
  "abstrak": "The design of software platform for medical imaging application has been increasingly prioritized as the sophisticated application of medical imaging. With this demand, we have designed and implemented a novel software platform in traditional object-oriented fashion with some common design patterns. This platform integrates the mainstream algorithms for medical image processing and analyzing within a consistent framework, including reconstruction, segmentation, registration, visualization, etc., and provides a powerful tool for both scientists and engineers. The overall framework and certain key technologies are introduced in detail. Presented experiment examples, numerous downloads, extensive uses, and practical applications commendably demonstrate the validity and flexibility of the platform.",
  "keyword": "image processing"
 },
 {
  "No": 364,
  "judul": "Comparison of Objective Image Quality Metrics to Expert Radiologists’ Scoring of Diagnostic Quality of MR Images",
  "abstrak": "Image quality metrics (IQMs) such as root mean square error (RMSE) and structural similarity index (SSIM) are commonly used in the evaluation and optimization of accelerated magnetic resonance imaging (MRI) acquisition and reconstruction strategies. However, it is unknown how well these indices relate to a radiologist’s perception of diagnostic image quality. In this study, we compare the image quality scores of five radiologists with the RMSE, SSIM, and other potentially useful IQMs: peak signal to noise ratio (PSNR) multi-scale SSIM (MSSSIM), information-weighted SSIM (IWSSIM), gradient magnitude similarity deviation (GMSD), feature similarity index (FSIM), high dynamic range visible difference predictor (HDRVDP), noise quality metric (NQM), and visual information fidelity (VIF). The comparison uses a database of MR images of the brain and abdomen that have been retrospectively degraded by noise, blurring, undersampling, motion, and wavelet compression for a total of 414 degraded images. A total of 1017 subjective scores were assigned by five radiologists. IQM performance was measured via the Spearman rank order correlation coefficient (SROCC) and statistically significant differences in the residuals of the IQM scores and radiologists’ scores were tested. When considering SROCC calculated from combining scores from all radiologists across all image types, RMSE and SSIM had lower SROCC than six of the other IQMs included in the study (VIF, FSIM, NQM, GMSD, IWSSIM, and HDRVDP). In no case did SSIM have a higher SROCC or significantly smaller residuals than RMSE. These results should be considered when choosing an IQM in future imaging studies.",
  "keyword": "image processing"
 },
 {
  "No": 365,
  "judul": "High-Resolution Imaging Via Moving Random Exposure and Its Simulation",
  "abstrak": "In this correspondence, we introduce a new imaging method to obtain high-resolution (HR) images. The image acquisition is performed in two stages, compressive measurement and optimization reconstruction. In order to reconstruct HR images by a small number of sensors, compressive measurements are made. Specifically, compressive measurements are made by a low-resolution (LR) camera with randomly fluttering shutter, which can be viewed as a moving random exposure pattern. In the optimization reconstruction stage, the HR image is computed by different models according to the prior knowledge of scenes. The proposed imaging method offers a new way of acquiring HR images of essentially static scenes when the camera resolution is limited by severe constraints such as cost, battery capacity, memory space, transmission bandwidth, etc. and when the prior knowledge of scenes is available. The simulation results demonstrate the effectiveness of the proposed imaging method.",
  "keyword": "image processing"
 },
 {
  "No": 366,
  "judul": "Breath-Hold CBCT-Guided CBCT-to-CT Synthesis via Multimodal Unsupervised Representation Disentanglement Learning",
  "abstrak": "Adaptive radiation therapy (ART) aims to deliver radiotherapy accurately and precisely in the presence of anatomical changes, in which the synthesis of computed tomography (CT) from cone-beam CT (CBCT) is an important step. However, because of serious motion artifacts, CBCT-to-CT synthesis remains a challenging task for breast-cancer ART. Existing synthesis methods usually ignore motion artifacts, thereby limiting their performance on chest CBCT images. In this paper, we decompose CBCT-to-CT synthesis into artifact reduction and intensity correction, and we introduce breath-hold CBCT images to guide them. To achieve superior synthesis performance, we propose a multimodal unsupervised representation disentanglement (MURD) learning framework that disentangles the content, style, and artifact representations from CBCT and CT images in the latent space. MURD can synthesize different forms of images using the recombination of disentangled representations. Also, we propose a multipath consistency loss to improve structural consistency in synthesis and a multidomain generator to improve synthesis performance. Experiments on our breast-cancer dataset show that MURD achieves impressive performance with a mean absolute error of 55.23±9.94 HU, a structural similarity index measurement of 0.721±0.042, and a peak signal-to-noise ratio of 28.26±1.93 dB in synthetic CT. The results show that compared to state-of-the-art unsupervised synthesis methods, our method produces better synthetic CT images in terms of both accuracy and visual quality.",
  "keyword": "image processing"
 },
 {
  "No": 367,
  "judul": "Recurrent Tissue-Aware Network for Deformable Registration of Infant Brain MR Images",
  "abstrak": "Deformable registration is fundamental to longitudinal and population-based image analyses. However, it is challenging to precisely align longitudinal infant brain MR images of the same subject, as well as cross-sectional infant brain MR images of different subjects, due to fast brain development during infancy. In this paper, we propose a recurrently usable deep neural network for the registration of infant brain MR images. There are three main highlights of our proposed method. (i) We use brain tissue segmentation maps for registration, instead of intensity images, to tackle the issue of rapid contrast changes of brain tissues during the first year of life. (ii) A single registration network is trained in a one-shot manner, and then recurrently applied in inference for multiple times, such that the complex deformation field can be recovered incrementally. (iii) We also propose both the adaptive smoothing layer and the tissue-aware anti-folding constraint into the registration network to ensure the physiological plausibility of estimated deformations without degrading the registration accuracy. Experimental results, in comparison to the state-of-the-art registration methods, indicate that our proposed method achieves the highest registration accuracy while still preserving the smoothness of the deformation field. The implementation of our proposed registration network is available online https:\/\/github.com\/Barnonewdm\/ACTA-Reg-Net .",
  "keyword": "image processing"
 },
 {
  "No": 368,
  "judul": "A Supervised Framework for the Registration and Segmentation of White Matter Fiber Tracts",
  "abstrak": "A supervised framework is presented for the automatic registration and segmentation of white matter (WM) tractographies extracted from brain DT-MRI. The framework relies on the direct registration between the fibers, without requiring any intensity-based registration as preprocessing. An affine transform is recovered together with a set of segmented fibers. A recently introduced probabilistic boosting tree classifier is used in a segmentation refinement step to improve the precision of the target tract segmentation. The proposed method compares favorably with a state-of-the-art intensity-based algorithm for affine registration of DTI tractographies. Segmentation results for 12 major WM tracts are demonstrated. Quantitative results are also provided for the segmentation of a particularly difficult case, the optic radiation tract. An average precision of 80% and recall of 55% were obtained for the optimal configuration of the presented method.",
  "keyword": "image processing"
 },
 {
  "No": 369,
  "judul": "MATR: Multimodal Medical Image Fusion via Multiscale Adaptive Transformer",
  "abstrak": "Owing to the limitations of imaging sensors, it is challenging to obtain a medical image that simultaneously contains functional metabolic information and structural tissue details. Multimodal medical image fusion, an effective way to merge the complementary information in different modalities, has become a significant technique to facilitate clinical diagnosis and surgical navigation. With powerful feature representation ability, deep learning (DL)-based methods have improved such fusion results but still have not achieved satisfactory performance. Specifically, existing DL-based methods generally depend on convolutional operations, which can well extract local patterns but have limited capability in preserving global context information. To compensate for this defect and achieve accurate fusion, we propose a novel unsupervised method to fuse multimodal medical images via a multiscale adaptive Transformer termed MATR. In the proposed method, instead of directly employing vanilla convolution, we introduce an adaptive convolution for adaptively modulating the convolutional kernel based on the global complementary context. To further model long-range dependencies, an adaptive Transformer is employed to enhance the global semantic extraction capability. Our network architecture is designed in a multiscale fashion so that useful multimodal information can be adequately acquired from the perspective of different scales. Moreover, an objective function composed of a structural loss and a region mutual information loss is devised to construct constraints for information preservation at both the structural-level and the feature-level. Extensive experiments on a mainstream database demonstrate that the proposed method outperforms other representative and state-of-the-art methods in terms of both visual quality and quantitative evaluation. We also extend the proposed method to address other biomedical image fusion issues, and the pleasing fusion results illustrate that MATR has good generalization capability. The code of the proposed method is available at https:\/\/github.com\/tthinking\/MATR .",
  "keyword": "image processing"
 },
 {
  "No": 370,
  "judul": "Automated Segmentation of Breast in 3-D MR Images Using a Robust Atlas",
  "abstrak": "This paper presents a robust atlas-based segmentation (ABS) algorithm for segmentation of the breast boundary in 3-D MR images. The proposed algorithm combines the well-known methodologies of ABS namely probabilistic atlas and atlas selection approaches into a single framework where two configurations are realized. The algorithm uses phase congruency maps to create an atlas which is robust to intensity variations. This allows an atlas derived from images acquired with one MR imaging sequence to be used to segment images acquired with a different MR imaging sequence and eliminates the need for intensity-based registration. Images acquired using a Dixon sequence were used to create an atlas which was used to segment both Dixon images (intra-sequence) and T1-weighted images (inter-sequence). In both cases, highly accurate results were achieved with the median Dice similarity coefficient values of 94% ±4% and 87±6.5%, respectively.",
  "keyword": "image processing"
 },
 {
  "No": 371,
  "judul": "4-D Intracardiac Ultrasound Vector Flow Imaging–Feasibility and Comparison to Phase-Contrast MRI",
  "abstrak": "In vivo characterization of intracardiac blood velocity vector fields may provide new clinical information but is currently not available for bedside evaluation. In this paper, 4-D vector flow imaging for intracardiac flow assessment is demonstrated using a clinical ultrasound (US) system and a matrix array transducer, without the use of contrast agent. Two acquisition schemes were developed, one for full volumetric coverage of the left ventricle (LA) at 50 vps and a 3-D thick-slice setup with continuous frame acquisition (4000 vps), both utilizing ECG-gating. The 3-D vector velocity estimates were obtained using a novel method combining phase and envelope information. In vitro validation in a rotating tissue-mimicking phantom revealed velocity estimates in compliance with the ground truth, with a linear regression slope of 0.80, 0.77, and 1.03 for the x, y, and z velocity components, and with standard deviations of 2.53, 3.19, and 0.95 cm\/s, respectively. In vivo measurements in a healthy LV showed good agreement with PC-MRI. Quantitative analysis of energy loss (EL) and kinetic energy (KE) further showed similar trends, with peak KE at 1.5 and 2.4 mJ during systole and 3.6 and 3.1 mJ for diastole for US and PC-MRI. Similar for EL, 0.15-0.2 and 0.7 mW was found during systole and 0.6 and 0.7 mW during diastole, for US and PC-MRI, respectively. Overall, a potential for US as a future modality for 4D cardiac vector flow imaging was demonstrated, which will be further evaluated in clinical studies.",
  "keyword": "image processing"
 },
 {
  "No": 372,
  "judul": "Context Coding of Depth Map Images Under the Piecewise-Constant Image Model Representation",
  "abstrak": "This paper introduces an efficient method for lossless compression of depth map images, using the representation of a depth image in terms of three entities: 1) the crack-edges; 2) the constant depth regions enclosed by them; and 3) the depth value over each region. The starting representation is identical with that used in a very efficient coder for palette images, the piecewise-constant image model coding, but the techniques used for coding the elements of the representation are more advanced and especially suitable for the type of redundancy present in depth images. Initially, the vertical and horizontal crack-edges separating the constant depth regions are transmitted by 2D context coding using optimally pruned context trees. Both the encoder and decoder can reconstruct the regions of constant depth from the transmitted crack-edge image. The depth value in a given region is encoded using the depth values of the neighboring regions already encoded, exploiting the natural smoothness of the depth variation, and the mutual exclusiveness of the values in neighboring regions. The encoding method is suitable for lossless compression of depth images, obtaining compression of about 10-65 times, and additionally can be used as the entropy coding stage for lossy depth compression.",
  "keyword": "image processing"
 },
 {
  "No": 373,
  "judul": "Unified Blind Method for Multi-Image Super-Resolution and Single\/Multi-Image Blur Deconvolution",
  "abstrak": "This paper presents, for the first time, a unified blind method for multi-image super-resolution (MISR or SR), single-image blur deconvolution (SIBD), and multi-image blur deconvolution (MIBD) of low-resolution (LR) images degraded by linear space-invariant (LSI) blur, aliasing, and additive white Gaussian noise (AWGN). The proposed approach is based on alternating minimization (AM) of a new cost function with respect to the unknown high-resolution (HR) image and blurs. The regularization term for the HR image is based upon the Huber-Markov random field (HMRF) model, which is a type of variational integral that exploits the piecewise smooth nature of the HR image. The blur estimation process is supported by an edge-emphasizing smoothing operation, which improves the quality of blur estimates by enhancing strong soft edges toward step edges, while filtering out weak structures. The parameters are updated gradually so that the number of salient edges used for blur estimation increases at each iteration. For better performance, the blur estimation is done in the filter domain rather than the pixel domain, i.e., using the gradients of the LR and HR images. The regularization term for the blur is Gaussian (L2 norm), which allows for fast noniterative optimization in the frequency domain. We accelerate the processing time of SR reconstruction by separating the upsampling and registration processes from the optimization procedure. Simulation results on both synthetic and real-life images (from a novel computational imager) confirm the robustness and effectiveness of the proposed method.",
  "keyword": "image processing"
 },
 {
  "No": 374,
  "judul": "Automatic Nonrigid Calibration of Image Registration for Real Time MR-Guided HIFU Ablations of Mobile Organs",
  "abstrak": "Real time magnetic resonance imaging (MRI) is rapidly gaining importance in interventional therapies. An accurate motion estimation is required for mobile targets and can be conveniently addressed using an image registration algorithm. Since the adaptation of the control parameters of the algorithm depends on the application (targeted organ, location of the tumor, slice orientation, etc.), typically an individual calibration is required. However, the assessment of the estimated motion accuracy is difficult since the real target motion is unknown. In this paper, existing criteria based only on anatomical image similarity are demonstrated to be inadequate. A new criterion is introduced, which is based on the local magnetic field distribution. The proposed criterion was used to assess, during a preparative calibration step, the optimal configuration of an image registration algorithm derived from the Horn and Schunck method. The accuracy of the proposed method was evaluated in a moving phantom experiment, which allows the comparison with the known motion pattern and to an established criterion based on anatomical images. The usefulness of the method for the calibration of optical-flow based algorithms was also demonstrated in vivo under conditions similar to thermo-ablation for the abdomen of twelve volunteers. In average over all volunteers, a resulting displacement error of 1.5 mm was obtained (largest observed error equal to 4-5 mm) using a criterion based on anatomical image similarity. A better average accuracy of 1 mm was achieved using the proposed criterion (largest observed error equal to 2 mm). In both kidney and liver, the proposed criterion was shown to provide motion field accuracy in the range of the best achievable.",
  "keyword": "image processing"
 },
 {
  "No": 375,
  "judul": "User-Agent Cooperation in Multiagent IVUS Image Segmentation",
  "abstrak": "Automated interpretation of complex images requires elaborate knowledge and model-based image analysis, but often needs interaction with an expert as well. This research describes expert interaction with a multiagent image interpretation system using only a restricted vocabulary of high-level user interactions. The aim is to minimize inter- and intra-observer variability by keeping the total number of interactions as low and simple as possible. The multiagent image interpretation system has elaborate high-level knowledge-based control over low-level image segmentation algorithms. Agents use contextual knowledge to keep the number of interactions low but, when in doubt, present the user with the most likely interpretation of the situation. The user, in turn, can correct, supplement, and\/or confirm the results of image-processing agents. This is done at a very high level of abstraction such that no knowledge of the underlying segmentation methods, parameters or agent functioning is needed. High-level interaction thereby replaces more traditional contour correction methods like inserting points and\/or (re)drawing contours. This makes it easier for the user to obtain good results, while inter- and intra-observer variability are kept minimal, since the image segmentation itself remains under control of image-processing agents. The system has been applied to intravascular ultrasound (IVUS) images. Experiments show that with an average of 2-3 high-level user interactions per correction, segmentation results substantially improve while the variation is greatly reduced. The achieved level of accuracy and repeatability is equivalent to that of manual drawing by an expert.",
  "keyword": "image processing"
 },
 {
  "No": 376,
  "judul": "Anatomy-Regularized Representation Learning for Cross-Modality Medical Image Segmentation",
  "abstrak": "An increasing number of studies are leveraging unsupervised cross-modality synthesis to mitigate the limited label problem in training medical image segmentation models. They typically transfer ground truth annotations from a label-rich imaging modality to a label-lacking imaging modality, under an assumption that different modalities share the same anatomical structure information. However, since these methods commonly use voxel\/pixel-wise cycle-consistency to regularize the mappings between modalities, high-level semantic information is not necessarily preserved. In this paper, we propose a novel anatomy-regularized representation learning approach for segmentation-oriented cross-modality image synthesis. It learns a common feature encoding across different modalities to form a shared latent space, where 1) the input and its synthesis present consistent anatomical structure information, and 2) the transformation between two images in one domain is preserved by their syntheses in another domain. We applied our method to the tasks of cross-modality skull segmentation and cardiac substructure segmentation. Experimental results demonstrate the superiority of our method in comparison with state-of-the-art cross-modality medical image segmentation methods.",
  "keyword": "image processing"
 },
 {
  "No": 377,
  "judul": "Weakly Supervised Estimation of Shadow Confidence Maps in Fetal Ultrasound Imaging",
  "abstrak": "Detecting acoustic shadows in ultrasound images is important in many clinical and engineering applications. Real-time feedback of acoustic shadows can guide sonographers to a standardized diagnostic viewing plane with minimal artifacts and can provide additional information for other automatic image analysis algorithms. However, automatically detecting shadow regions using learning-based algorithms is challenging because pixel-wise ground truth annotation of acoustic shadows is subjective and time consuming. In this paper, we propose a weakly supervised method for automatic confidence estimation of acoustic shadow regions. Our method is able to generate a dense shadow-focused confidence map. In our method, a shadow-seg module is built to learn general shadow features for shadow segmentation, based on global image-level annotations as well as a small number of coarse pixel-wise shadow annotations. A transfer function is introduced to extend the obtained binary shadow segmentation to a reference confidence map. In addition, a confidence estimation network is proposed to learn the mapping between input images and the reference confidence maps. This network is able to predict shadow confidence maps directly from input images during inference. We use evaluation metrics such as DICE, inter-class correlation, and so on, to verify the effectiveness of our method. Our method is more consistent than human annotation and outperforms the state-of-the-art quantitatively in shadow segmentation and qualitatively in confidence estimation of shadow regions. Furthermore, we demonstrate the applicability of our method by integrating shadow confidence maps into tasks such as ultrasound image classification, multi-view image fusion, and automated biometric measurements.",
  "keyword": "image processing"
 },
 {
  "No": 378,
  "judul": "Complementary DNA Microarray Image Processing Based on the Fuzzy Gaussian Mixture Model",
  "abstrak": "The objective of this paper was to investigate the segmentation ability of the fuzzy Gaussian mixture model (FGMM) clustering algorithm, applied on complementary DNA (cDNA) images. Following a standard established procedure, a simulated microarray image of 1600 cells, each containing one spot, was produced. For further evaluation of the algorithm, three real microarray images were also used, each containing 6400 spots. For the task of locating spot borders and surrounding background (BG) in each cell, an automatic gridding process was developed and applied on microarray images. The FGMM and the Gaussian mixture model (GMM) algorithms were applied to each cell with the purpose of discriminating foreground (FG) from BG. The segmentation abilities of both algorithms were evaluated by means of the segmentation matching factor, coefficient of determination, and concordance correlation, in respect to the actual classes (FG-BG pixels) of the simulated spots. Pairwise correlation and mean absolute error of the real images among replicates were also calculated. The FGMM was found to perform better and with equal processing time, as compared to the GMM, rendering the FGMM algorithm an efficient alternative for segmenting cDNA microarray images.",
  "keyword": "image processing"
 },
 {
  "No": 379,
  "judul": "3D+t Morphological Processing: Applications to Embryogenesis Image Analysis",
  "abstrak": "We propose to directly process 3D+t image sequences with mathematical morphology operators, using a new classification of the 3D+t structuring elements. Several methods (filtering, tracking, segmentation) dedicated to the analysis of 3D+t datasets of zebrafish embryogenesis are introduced and validated through a synthetic dataset. Then, we illustrate the application of these methods to the analysis of datasets of zebrafish early development acquired with various microscopy techniques. This processing paradigm produces spatio-temporal coherent results as it benefits from the intrinsic redundancy of the temporal dimension, and minimizes the needs for human intervention in semi-automatic algorithms.",
  "keyword": "image processing"
 },
 {
  "No": 380,
  "judul": "Efficient Joint Image Reconstruction of Multi-Patch Data Reusing a Single System Matrix in Magnetic Particle Imaging",
  "abstrak": "Due to peripheral nerve stimulation, the magnetic particle imaging (MPI) method is limited in the maximum applicable excitation-field amplitude. This in turn leads to a limitation of the size of the covered field of view (FoV) to few millimeters. In order to still capture a larger FoV, MPI is capable to rapidly acquire volumes in a multi-patch fashion. To this end, the small excitation volume is shifted through space using the magnetic focus fields. Recently, it has been shown that the individual patches are preferably reconstructed in a joint fashion by solving a single linear system of equations taking the coupling between individual patches into account. While this improves the image quality, it is computationally and memory demanding since the size of the linear system increases in the best case quadratically with the number of patches. In this paper, we will develop a reconstruction algorithm for MPI multi-patch data exploiting the sparsity of the joint system matrix. A highly efficient implicit matrix format allows for rapid on-the-fly calculations of linear algebra operations involving the system matrix. Using this approach, the computational effort can be reduced to a linear dependence on the number of used patches. The algorithm is validated on 3-D multi-patch phantom data sets and shown to reconstruct large data sets with 15 patches in less than 22 s.",
  "keyword": "image processing"
 },
 {
  "No": 381,
  "judul": "Accelerated 3D bSSFP Using a Modified Wave-CAIPI Technique With Truncated Wave Gradients",
  "abstrak": "The Wave Controlled Aliasing In Parallel Imaging (Wave-CAIPI) technique manifests great potential to highly accelerate three-dimensional (3D) balanced steady-state free precession (bSSFP) through substantially reducing the geometric factor (g-factor) and aliasing artifacts of image reconstruction. However, severe banding artifacts appear in bSSFP imaging due to unbalanced gradients with nonzero 0 th moment applied by the conventional Wave-CAIPI technique. In this study, we propose a 3D Wave-bSSFP scheme that adopts truncated wave gradients with zero 0 th moment to avoid introducing additional banding artifacts and to maintain the advantages of wave encoding. The simulation results indicate that the number of wave cycles that are truncated and different options of applying wave gradients affect both the g-factor reduction and image quality, but the influence is limited. In phantom experiments, the proposed technique shows similar acceleration performance as the conventional Wave-CAIPI technique and effectively eliminates its introduced banding artifacts. Additionally, Wave-bSSFP obtains up to 12× retrospective acceleration at 0.8 mm isotropic resolution in in vivo 3D brain experiments and is superior to the state-of-the-art Controlled Aliasing In Parallel Imaging Results IN Higher Acceleration (CAIPIRINHA) technique, according to both visual validation and quantitative analysis. Moreover, in vivo 3D spine and abdomen imaging demonstrate the potential clinical applications of Wave-bSSFP with fast acquisition speed, improved isotropic resolution and fine image quality.",
  "keyword": "image processing"
 },
 {
  "No": 382,
  "judul": "Active Contour Image Segmentation Method for Training Talents of Computer Graphics and Image Processing Technology",
  "abstrak": "Image segmentation is a key technology in the field of computer image processing. Among them, segmentation methods based on active contour models have been developed rapidly in recent years due to their effective processing of complex images such as medical images. These methods have achieved significant results in medical, military, and industrial fields. Present research work mainly introduces the training of computer graphics and image processing technology and the method of active contour image segmentation. It focuses on the study of image segmentation methods and focuses on the segmentation methods based on active contour models. Firstly, it summarizes two types of segmentation methods based on edge and region and summarizes their advantages and disadvantages. Then, the segmentation method based on the active contour model is studied, and several typical active contour models are comprehensively compared. Finally, the local binary fitting model and the local Gaussian distribution fitting energy model are improved and simulated. Furthermore, from the development of computer graphics and image processing technology to analyze some methods and means of training this professional talent. The experimental results of this article show that the active contour image segmentation algorithm can not only ensure the image segmentation algorithm but also reduce the number of iterations and shorten the image segmentation time. Compared with the CV, LBF, and LGIF models computational efficiency of Segmentation method is increased by 9.2 times, 2.64 times, and 1.44 times, respectively.",
  "keyword": "image processing"
 },
 {
  "No": 383,
  "judul": "MISTICA: Minimum Spanning Tree-Based Coarse Image Alignment for Microscopy Image Sequences",
  "abstrak": "Registration of an in vivo microscopy image sequence is necessary in many significant studies, including studies of atherosclerosis in large arteries and the heart. Significant cardiac and respiratory motion of the living subject, occasional spells of focal plane changes, drift in the field of view, and long image sequences are the principal roadblocks. The first step in such a registration process is the removal of translational and rotational motion. Next, a deformable registration can be performed. The focus of our study here is to remove the translation and\/or rigid body motion that we refer to here as coarse alignment. The existing techniques for coarse alignment are unable to accommodate long sequences often consisting of periods of poor quality images (as quantified by a suitable perceptual measure). Many existing methods require the user to select an anchor image to which other images are registered. We propose a novel method for coarse image sequence alignment based on minimum weighted spanning trees (MISTICA) that overcomes these difficulties. The principal idea behind MISTICA is to reorder the images in shorter sequences, to demote nonconforming or poor quality images in the registration process, and to mitigate the error propagation. The anchor image is selected automatically making MISTICA completely automated. MISTICA is computationally efficient. It has a single tuning parameter that determines graph width, which can also be eliminated by the way of additional computation. MISTICA outperforms existing alignment methods when applied to microscopy image sequences of mouse arteries.",
  "keyword": "image processing"
 },
 {
  "No": 384,
  "judul": "Reduced-Complexity Delayed-Decision Algorithm for Context-Based Image Processing Systems",
  "abstrak": "It is well known that the performance of context-based image processing systems can be improved by allowing the processor (e.g., an encoder or a denoiser) a delay of several samples before making a processing decision. Often, however, for such systems, traditional delayed-decision algorithms can become computationally prohibitive due to the growth in the size of the space of possible solutions. In this paper, we propose a reduced-complexity, one-pass, delayed-decision algorithm that systematically reduces the size of the search space, while also preserving its structure. In particular, we apply the proposed algorithm to two examples of adaptive context-based image processing systems, an image coding system that employs a context-based entropy coder, and a spatially adaptive image-denoising system. For these two types of widely used systems, we show that the proposed delayed-decision search algorithm outperforms instantaneous-decision algorithms with only a small increase in complexity. We also show that the performance of the proposed algorithm is better than that of other, higher complexity, delayed-decision algorithms.",
  "keyword": "image processing"
 },
 {
  "No": 385,
  "judul": "A New Fast Accurate Nonlinear Medical Image Registration Program Including Surface Preserving Regularization",
  "abstrak": "Recently inexpensive graphical processing units (GPUs) have become established as a viable alternative to traditional CPUs for many medical image processing applications. GPUs offer the potential of very significant improvements in performance at low cost and with low power consumption. One way in which GPU programs differ from traditional CPU programs is that increasingly elaborate calculations per voxel may not impact of the overall processing time because memory accesses can dominate execution time. This paper presents a new GPU based elastic image registration program named Ezys. The Ezys image registration algorithm belongs to the wide class of diffeomorphic demons but uses surface preserving image smoothing and regularization filters designed for a GPU that would be computationally expensive on a CPU. We describe the methods used in Ezys and present results from two important neuroscience applications. Firstly inter-subject registration for transfer of anatomical labels and secondly longitudinal intra-subject registration to quantify atrophy in individual subjects. Both experiments showed that Ezys registration compares favorably with other popular elastic image registration programs. We believe Ezys is a useful tool for neuroscience and other applications, and also demonstrates the value of developing of novel image processing filters specifically designed for GPUs.",
  "keyword": "image processing"
 },
 {
  "No": 386,
  "judul": "A Novel Software Platform for Medical Image Processing and Analyzing",
  "abstrak": "The design of software platform for medical imaging application has been increasingly prioritized as the sophisticated application of medical imaging. With this demand, we have designed and implemented a novel software platform in traditional object-oriented fashion with some common design patterns. This platform integrates the mainstream algorithms for medical image processing and analyzing within a consistent framework, including reconstruction, segmentation, registration, visualization, etc., and provides a powerful tool for both scientists and engineers. The overall framework and certain key technologies are introduced in detail. Presented experiment examples, numerous downloads, extensive uses, and practical applications commendably demonstrate the validity and flexibility of the platform.",
  "keyword": "image processing"
 },
 {
  "No": 387,
  "judul": "A Complete Processing Chain for Shadow Detection and Reconstruction in VHR Images",
  "abstrak": "The presence of shadows in very high resolution (VHR) images can represent a serious obstacle for their full exploitation. This paper proposes to face this problem as a whole through the proposal of a complete processing chain, which relies on various advanced image processing and pattern recognition tools. The first key point of the chain is that shadow areas are not only detected but also classified to allow their customized compensation. The detection and classification tasks are implemented by means of the state-of-the-art support vector machine approach. A quality check mechanism is integrated in order to reduce subsequent misreconstruction problems. The reconstruction is based on a linear regression method to compensate shadow regions by adjusting the intensities of the shaded pixels according to the statistical characteristics of the corresponding nonshadow regions. Moreover, borders are explicitly handled by making use of adaptive morphological filters and linear interpolation for the prevention of possible border artifacts in the reconstructed image. Experimental results obtained on three VHR images representing different shadow conditions are reported, discussed, and compared with two other reconstruction techniques.",
  "keyword": "image processing"
 },
 {
  "No": 388,
  "judul": "Fluorescence Lifetime Imaging and Intravascular Ultrasound: Co-Registration Study Using Ex Vivo Human Coronaries",
  "abstrak": "Fluorescence lifetime imaging (FLIM) has demonstrated potential for robust assessment of atherosclerotic plaques biochemical composition and for complementing conventional intravascular ultrasound (IVUS), which provides information on plaque morphology. The success of such a bi-modal imaging modality depends on accurate segmentation of the IVUS images and proper angular registration between these two modalities. This paper reports a novel IVUS segmentation methodology addressing this issue. The image preprocessing consisted of denoising, using the Wiener filter, followed by image smoothing, implemented through the application of the alternating sequential filter on the edge separability metric images. Extraction of the lumen\/intima and media\/adventitia boundaries was achieved by tracing the gray-scale peaks over the A-lines of the IVUS preprocessed images. Cubic spline interpolation, in both cross-sectional and longitudinal directions, ensured boundary smoothness and continuity. The detection of the guide-wire artifact in both modalities is used for angular registration. Intraluminal studies were conducted in 13 ex vivo segments of human coronaries. The IVUS segmentation accuracy was assessed against independent manual tracings, providing 91.82% sensitivity and 97.55% specificity. The proposed methodology makes the bi-modal FLIM and IVUS approach feasible for comprehensive intravascular diagnosis by providing co-registered biochemical and morphological information of atherosclerotic plaques.",
  "keyword": "image processing"
 },
 {
  "No": 389,
  "judul": "PDE-Based Enhancement of Color Images in RGB Space",
  "abstrak": "A novel method for color image enhancement is proposed as an extension of the scalar-diffusion–shock-filter coupling model, where noisy and blurred images are denoised and sharpened. The proposed model is based on using the single vectors of the gradient magnitude and the second derivatives as a manner to relate different color components of the image. This model can be viewed as a generalization of the Bettahar–Stambouli filter to multivalued images. The proposed algorithm is more efficient than the mentioned filter and some previous works at color images denoising and deblurring without creating false colors.",
  "keyword": "image processing"
 },
 {
  "No": 390,
  "judul": "Deformable Image Registration Using Functions of Bounded Deformation",
  "abstrak": "Deformable image registration is a widely used technique in the field of computer vision and medical image processing. Basically, the task of deformable image registration is to find the displacement field between the moving image and the fixed image. Many variational models are proposed for deformable image registration, under the assumption that the displacement field is continuous and smooth. However, displacement fields may be discontinuous, especially for medical images with intensity inhomogeneity, pathological tissues, or heavy noises. In the mathematical theory of elastoplasticity, when the displacement fields are possibly discontinuous, a suitable framework for describing the displacement fields is the space of functions of bounded deformation (BD). Inspired by this, we propose a novel deformable registration model, called the BD model, which allows discontinuities of displacement fields in images. The BD model is formulated in a variational framework by supposing the displacement field to be a function of BD. The existence of solutions of this model is proven. Numerical experiments on 2D images show that the BD model outperforms the classical demons model, the log-domain diffeomorphic demons model, and the state-of-the-art vectorial total variation model. Numerical experiments on two public 3D databases show that the target registration error of the BD model is competitive compared with more than ten other models.",
  "keyword": "image processing"
 },
 {
  "No": 391,
  "judul": "LRTV: MR Image Super-Resolution With Low-Rank and Total Variation Regularizations",
  "abstrak": "Image super-resolution (SR) aims to recover high-resolution images from their low-resolution counterparts for improving image analysis and visualization. Interpolation methods, widely used for this purpose, often result in images with blurred edges and blocking effects. More advanced methods such as total variation (TV) retain edge sharpness during image recovery. However, these methods only utilize information from local neighborhoods, neglecting useful information from remote voxels. In this paper, we propose a novel image SR method that integrates both local and global information for effective image recovery. This is achieved by, in addition to TV, low-rank regularization that enables utilization of information throughout the image. The optimization problem can be solved effectively via alternating direction method of multipliers (ADMM). Experiments on MR images of both adult and pediatric subjects demonstrate that the proposed method enhances the details in the recovered high-resolution images, and outperforms methods such as the nearest-neighbor interpolation, cubic interpolation, iterative back projection (IBP), non-local means (NLM), and TV-based up-sampling.",
  "keyword": "image processing"
 },
 {
  "No": 392,
  "judul": "A Guided Tour of Selected Image Processing and Analysis Methods for Fluorescence and Electron Microscopy",
  "abstrak": "Microscopy imaging, including fluorescence micro scopy and electron microscopy, has taken a prominent role in life science research and medicine due to its ability to investigate the 3D interior of live cells and organisms. A long-term research in bio-imaging at the sub-cellular and cellular scales consists then in inferring the relationships between the dynamics of macromolecules and their functions. In this area, image processing and analysis methods are now essential to understand the dynamic organization of groups of interacting molecules inside molecular machineries and to address issues in fundamental biology driven by advances in molecular biology, optics and technology. In this paper, we present recent advances in fluorescence and electron microscopy and we focus on dedicated image processing and analysis methods required to quantify phenotypes for a limited number but typical studies in cell imaging.",
  "keyword": "image processing"
 },
 {
  "No": 393,
  "judul": "Hierarchical Prediction and Context Adaptive Coding for Lossless Color Image Compression",
  "abstrak": "This paper presents a new lossless color image compression algorithm, based on the hierarchical prediction and context-adaptive arithmetic coding. For the lossless compression of an RGB image, it is first decorrelated by a reversible color transform and then Y component is encoded by a conventional lossless grayscale image compression method. For encoding the chrominance images, we develop a hierarchical scheme that enables the use of upper, left, and lower pixels for the pixel prediction, whereas the conventional raster scan prediction methods use upper and left pixels. An appropriate context model for the prediction error is also defined and the arithmetic coding is applied to the error signal corresponding to each context. For several sets of images, it is shown that the proposed method further reduces the bit rates compared with JPEG2000 and JPEG-XR.",
  "keyword": "image processing"
 },
 {
  "No": 394,
  "judul": "Orthogonal Rotation-Invariant Moments for Digital Image Processing",
  "abstrak": "Orthogonal rotation-invariant moments (ORIMs), such as Zernike moments, are introduced and defined on a continuous unit disk and have been proven powerful tools in optics applications. These moments have also been digitized for applications in digital image processing. Unfortunately, digitization compromises the orthogonality of the moments and, therefore, digital ORIMs are incapable of representing subtle details in images and cannot accurately reconstruct images. Typical approaches to alleviate the digitization artifact can be divided into two categories: 1) careful selection of a set of pixels as close approximation to the unit disk and using numerical integration to determine the ORIM values, and 2) representing pixels using circular shapes such that they resemble that of the unit disk and then calculating ORIMs in polar space. These improvements still fall short of preserving the orthogonality of the ORIMs. In this paper, in contrast to the previous methods, we propose a different approach of using numerical optimization techniques to improve the orthogonality. We prove that with the improved orthogonality, image reconstruction becomes more accurate. Our simulation results also show that the optimized digital ORIMs can accurately reconstruct images and can represent subtle image details.",
  "keyword": "image processing"
 },
 {
  "No": 395,
  "judul": "BM3D Frames and Variational Image Deblurring",
  "abstrak": "A family of the block matching 3-D (BM3D) algorithms for various imaging problems has been recently proposed within the framework of nonlocal patchwise image modeling \n\n, \n \n. In this paper, we construct analysis and synthesis frames, formalizing BM3D image modeling, and use these frames to develop novel iterative deblurring algorithms. We consider two different formulations of the deblurring problem, i.e., one given by the minimization of the single-objective function and another based on the generalized Nash equilibrium (GNE) balance of two objective functions. The latter results in the algorithm where deblurring and denoising operations are decoupled. The convergence of the developed algorithms is proved. Simulation experiments show that the decoupled algorithm derived from the GNE formulation demonstrates the best numerical and visual results and shows superiority with respect to the state of the art in the field, confirming a valuable potential of BM3D-frames as an advanced image modeling tool.",
  "keyword": "image processing"
 },
 {
  "No": 396,
  "judul": "UIF: An Objective Quality Assessment for Underwater Image Enhancement",
  "abstrak": "Due to complex and volatile lighting environment, underwater imaging can be readily impaired by light scattering, warping, and noises. To improve the visual quality, Underwater Image Enhancement (UIE) techniques have been widely studied. Recent efforts have also been contributed to evaluate and compare the UIE performances with subjective and objective methods. However, the subjective evaluation is time-consuming and uneconomic for all images, while existing objective methods have limited capabilities for the newly-developed UIE approaches based on deep learning. To fill this gap, we propose an Underwater Image Fidelity (UIF) metric for objective evaluation of enhanced underwater images. By exploiting the statistical features of these images in CIELab space, we present the naturalness, sharpness, and structure indexes. Among them, the naturalness and sharpness indexes represent the visual improvements of enhanced images; the structure index indicates the structural similarity between the underwater images before and after UIE. We combine all indexes with a saliency-based spatial pooling and thus obtain the final UIF metric. To evaluate the proposed metric, we also establish a first-of-its-kind large-scale UIE database with subjective scores, namely Underwater Image Enhancement Database (UIED). Experimental results confirm that the proposed UIF metric outperforms a variety of underwater and general-purpose image quality metrics. The database and source code are available at https:\/\/github.com\/z21110008\/UIF .",
  "keyword": "image processing"
 },
 {
  "No": 397,
  "judul": "Single Traffic Image Deraining via Similarity-Diversity Model",
  "abstrak": "Single traffic image deraining technology based on deep learning is a vital branch of image preprocessing, which is of great help to intelligent monitoring systems and driving navigation system. It is well understood that established deraining methods are derived based on one specific imaging model, neglecting the underlying correlations between different weather models and thereby limiting the applicability of these standard methods in real scenarios. To ameliorate this issue, in this work, we first explore the inherent relationship between a rain model and the haze one established up to date. We discover that these two models experience similar degradations in the low-frequency components (i.e., similarity) but diverse degradations in the high-frequency areas (i.e., diversity). Based on these observations, we develop a Similarity-Diversity model to describe these characteristics. Afterwards, we introduce a novel deep neural network to restore the rain-free background embedding the similarity-diversity model, namely deep similarity-diversity network (DSDNet). Extensive experiments have been conducted to evaluate our proposed method that outperforms the other state of the art deraining techniques. On the other hand, we deploy the proposed algorithm with Google Vision API for object recognition, which also obtains satisfactory results both qualitatively and quantitatively.",
  "keyword": "image processing"
 },
 {
  "No": 398,
  "judul": "A Framework for Hexagonal Image Processing Using Hexagonal Pixel-Perfect Approximations in Subpixel Resolution",
  "abstrak": "Image processing in hexagonal lattice has many advantages rather than square lattice. Researchers have addressed benefits of hexagonal structure in applications such as binarization, rotation, scaling and edge detection. Approximately all existing hardwares for capturing and displaying images are based on square lattice. Therefore, the best way for using advantages of hexagonal lattice is to find a proper software approach to convert square pixels to hexagonal ones. This paper presents a hexagonal platform based on interpolation which addresses three existing hexagonal challenges including imperfect hexagonal shape, inaccurate intensity level of hexagonal pixels and lower resolution in hexagonal space. The proposed interpolation is computed by overlaps between square and hexagonal pixels. Overlap types are formulated mathematically in 8 separate cases. Each overlap case is detected automatically and used to compute final gray-level intensity of hexagonal pixels. It is mathematically and experimentally shown that the proposed method satisfies necessary conditions for square-to-hexagonal conversion. The proposed scheme is evaluated on synthetic and real images with 10 different levels of noise in interpolation and edge detection applications. In synthetic images, the proposed method achieves the best figure of merit (FOM) 99.92% and 98.67% in high and low SNRs 100 and 20, respectively. Also, the proposed method outperforms existing state of the art hexagonal lattices with interclass correlation coefficient (ICC) 84.18% and mean rating 7.7 (out of 9) in real images.",
  "keyword": "image processing"
 },
 {
  "No": 399,
  "judul": "Complementary DNA Microarray Image Processing Based on the Fuzzy Gaussian Mixture Model",
  "abstrak": "The objective of this paper was to investigate the segmentation ability of the fuzzy Gaussian mixture model (FGMM) clustering algorithm, applied on complementary DNA (cDNA) images. Following a standard established procedure, a simulated microarray image of 1600 cells, each containing one spot, was produced. For further evaluation of the algorithm, three real microarray images were also used, each containing 6400 spots. For the task of locating spot borders and surrounding background (BG) in each cell, an automatic gridding process was developed and applied on microarray images. The FGMM and the Gaussian mixture model (GMM) algorithms were applied to each cell with the purpose of discriminating foreground (FG) from BG. The segmentation abilities of both algorithms were evaluated by means of the segmentation matching factor, coefficient of determination, and concordance correlation, in respect to the actual classes (FG-BG pixels) of the simulated spots. Pairwise correlation and mean absolute error of the real images among replicates were also calculated. The FGMM was found to perform better and with equal processing time, as compared to the GMM, rendering the FGMM algorithm an efficient alternative for segmenting cDNA microarray images.",
  "keyword": "image processing"
 },
 {
  "No": 400,
  "judul": "Double Line Image Rotation",
  "abstrak": "This paper proposes a fast algorithm for rotating images while preserving their quality. The new approach rotates images based on vertical or horizontal lines in the original image and their rotated equation in the target image. The proposed method is a one-pass method that determines a based-line equation in the target image and extracts all corresponding pixels on the base-line. Floating-point multiplications are performed to calculate the base-line in the target image, and other line coordinates are calculated using integer addition or subtraction and logical justifications from the base-line pixel coordinates in the target image. To avoid a heterogeneous distance between rotated pixels in the target image, each line rotates to two adjacent lines. The proposed method yields good performance in terms of speed and quality according to the results of an analysis of the computation speed and accuracy.",
  "keyword": "image processing"
 },
 {
  "No": 401,
  "judul": "Power Allocation Schemes Based on Deep Learning for Distributed Antenna Systems",
  "abstrak": "In recent years, a lot of power allocation algorithms have been proposed to maximize spectral efficiency (SE) and energy efficiency (EE) for the distributed antenna systems (DAS). However, the traditional iterative power allocation algorithms are difficult to be implemented in reality because of their high computational complexity. With the development of machine learning algorithms, it has been proved that the machine learning method has excellent learning ability and low computational complexity, which can approximate the traditional iterative power allocation well and be easily to be implemented in reality. In this paper, we propose a new deep neural network (DNN) model for DAS. From the perspective of machine learning, traditional iterative algorithms can be regarded as a nonlinear mapping between user channel realizations and optimal power allocation schemes. Therefore, we train the DNN to learn the nonlinear mapping between the user channel realizations and the corresponding power allocation schemes based on the traditional iterative algorithm. Then, a power allocation schemes based on DNN method is developed to maximize SE and EE for DAS. The simulation results show that the proposed scheme can not only obtain the almost similar performance as the traditional iterative algorithm, but also reduce much online computational time.",
  "keyword": "machine learning"
 },
 {
  "No": 402,
  "judul": "A Survey on Machine Learning Techniques for Routing Optimization in SDN",
  "abstrak": "In conventional networks, there was a tight bond between the control plane and the dataplane. The introduction of Software-Defined Networking (SDN) separated these planes, and provided additional features and tools to solve some of the problems of traditional network (i.e., latency, consistency, efficiency). SDN is a flexible networking paradigm that boosts network control, programmability and automation. It proffers many benefits in many areas, including routing. More specifically, for efficiently organizing, managing and optimizing routing in networks, some intelligence is required, and SDN offers the possibility to easily integrate it. To this purpose, many researchers implemented different machine learning (ML) techniques to enhance SDN routing applications. This article surveys the use of ML techniques for routing optimization in SDN based on three core categories (i.e. supervised learning, unsupervised learning, and reinforcement learning). The main contributions of this survey are threefold. Firstly, it presents detailed summary tables related to these studies and their comparison is also discussed, including a summary of the best works according to our analysis. Secondly, it summarizes the main findings, best works and missing aspects, and it includes a quick guideline to choose the best ML technique in this field (based on available resources and objectives). Finally, it provides specific future research directions divided into six sections to conclude the survey. Our conclusion is that there is a huge trend to use intelligence-based routing in programmable networks, particularly during the last three years, but a lot of effort is still required to achieve comprehensive comparisons and synergies of approaches, meaningful evaluations based on open datasets and topologies, and detailed practical implementations (following recent standards) that could be adopted by industry. In summary, future efforts should be focused on reproducible research rather than on new isolated ideas. Otherwise, most of these applications will be barely implemented in practice.",
  "keyword": "machine learning"
 },
 {
  "No": 403,
  "judul": "Nature-Based Prediction Model of Bug Reports Based on Ensemble Machine Learning Model",
  "abstrak": "In software development systems, the maintenance process of software systems attracted the attention of researchers due to its importance in fixing the defects discovered in the software testing by using bug reports (BRs) which include detailed information like description, status, reporter, assignee, priority, and severity of the bug and other information. The main problem in this process is how to analyze these BRs to discover all defects in the system, which is a tedious and time-consuming task if done manually because the number of BRs increases dramatically. Thus, the automated solution is the best. Most of the current research focuses on automating this process from different aspects, such as detecting the severity or priority of the bug. However, they did not consider the nature of the bug, which is a multi-class classification problem. This paper solves this problem by proposing a new prediction model to analyze BRs and predict the nature of the bug. The proposed model constructs an ensemble machine learning algorithm using natural language processing (NLP) and machine learning techniques. We simulate the proposed model by using a publicly available dataset for two online software bug repositories (Mozilla and Eclipse), which includes six classes: Program Anomaly, GUI, Network or Security, Configuration, Performance, and Test-Code. The simulation results show that the proposed model can achieve better accuracy than most existing models, namely, 90.42% without text augmentation and 96.72% with text augmentation.",
  "keyword": "machine learning"
 },
 {
  "No": 404,
  "judul": "Neuroimaging and Machine Learning for Dementia Diagnosis: Recent Advancements and Future Prospects",
  "abstrak": "Dementia, a chronic and progressive cognitive declination of brain function caused by disease or impairment, is becoming more prevalent due to the aging population. A major challenge in dementia is achieving accurate and timely diagnosis. In recent years, neuroimaging with computer-aided algorithms have made remarkable advances in addressing this challenge. The success of these approaches is mostly attributed to the application of machine learning techniques for neuroimaging. In this review paper, we present a comprehensive survey of automated diagnostic approaches for dementia using medical image analysis and machine learning algorithms published in the recent years. Based on the rigorous review of the existing works, we have found that, while most of the studies focused on Alzheimer's disease, recent research has demonstrated reasonable performance in the identification of other types of dementia remains a major challenge. Multimodal imaging analysis deep learning approaches have shown promising results in the diagnosis of these other types of dementia. The main contributions of this review paper are as follows. 1) Based on the detailed analysis of the existing literature, this paper discusses neuroimaging procedures for dementia diagnosis. 2) It systematically explains the most recent machine learning techniques and, in particular, deep learning approaches for early detection of dementia.",
  "keyword": "machine learning"
 },
 {
  "No": 405,
  "judul": "Flexible Machine Learning-Based Cyberattack Detection Using Spatiotemporal Patterns for Distribution Systems",
  "abstrak": "This letter develops a flexible machine learning detection method for cyberattacks in distribution systems considering spatiotemporal patterns. Spatiotemporal patterns are recognized by the graph Laplacian based on system-wide measurements. A flexible Bayes classifier (BC) is used to train spatiotemporal patterns which could be violated when cyberattacks occur. Cyberattacks are detected by using flexible BCs online. The effectiveness of the developed method is demonstrated through standard IEEE 13and 123-node test feeders.",
  "keyword": "machine learning"
 },
 {
  "No": 406,
  "judul": "Machine Learning to Support the Presentation of Complex Pathway Graphs",
  "abstrak": "Visualization of biological mechanisms by means of pathway graphs is necessary to better understand the often complex underlying system. Manual layout of such pathways or maps of knowledge is a difficult and time consuming process. Node duplication is a technique that makes layouts with improved readability possible by reducing edge crossings and shortening edge lengths in drawn diagrams. In this article, we propose an approach using Machine Learning (ML) to facilitate parts of this task by training a Support Vector Machine (SVM) with actions taken during manual biocuration. Our training input is a series of incremental snapshots of a diagram describing mechanisms of a disease, progressively curated by a human expert employing node duplication in the process. As a test of the trained SVM models, they are applied to a single large instance and 25 medium-sized instances of hand-curated biological pathways. Finally, in a user validation study, we compare the model predictions to the outcome of a node duplication questionnaire answered by users of biological pathways with varying experience. We successfully predicted nodes for duplication and emulated human choices, demonstrating that our approach can effectively learn human-like node duplication preferences to support curation of pathway diagrams in various contexts.",
  "keyword": "machine learning"
 },
 {
  "No": 407,
  "judul": "Machine Learning and Marketing: A Systematic Literature Review",
  "abstrak": "Even though machine learning (ML) applications are not novel, they have gained popularity partly due to the advance in computing processing. This study explores the adoption of ML methods in marketing applications through a bibliographic review of the period 2008–2022. In this period, the adoption of ML in marketing has grown significantly. This growth has been quite heterogeneous, varying from the use of classical methods such as artificial neural networks to hybrid methods that combine different techniques to improve results. Generally, maturity in the use of ML in marketing and increasing specialization in the type of problems that are solved were observed. Strikingly, the types of ML methods used to solve marketing problems vary wildly, including deep learning, supervised learning, reinforcement learning, unsupervised learning, and hybrid methods. Finally, we found that the main marketing problems solved with machine learning were related to consumer behavior, recommender systems, forecasting, marketing segmentation, and text analysis—content analysis.",
  "keyword": "machine learning"
 },
 {
  "No": 408,
  "judul": "Semi-Supervised Learning Algorithm for Identifying High-Priority Drug–Drug Interactions Through Adverse Event Reports",
  "abstrak": "Identifying drug-drug interactions (DDIs) is a critical enabler for reducing adverse drug events and improving patient safety. Generating proper DDI alerts during prescribing workflow has the potential to prevent DDIrelated adverse events. However, the implementation of DDI alerting system remains a challenge as users are experiencing alert overload which causes alert fatigue. One strategy to optimize the current system is to establish a list of high-priority DDIs for alerting purposes, though it is a resource-intensive task. In this study, we propose a machine learning framework to extract useful features from the FDA adverse event reports and then identify potential highpriority DDIs using an autoencoder-based semi-supervised learning algorithm. The experimental results demonstrate the effectiveness of using adverse event feature representations in differentiating highand low-priority DDIs. Additionally, the proposed algorithm utilizes stacked autoencoders and weighted support vector machine for boosting classification performance, which outperforms other competing methods in terms of F-measure and AUC score. This framework integrates multiple information sources, leverages domain knowledge and clinical evidence, and provides a practical approach for pre-screening high-priority DDI candidates for medication alerts.",
  "keyword": "machine learning"
 },
 {
  "No": 409,
  "judul": "Unsupervised Feature Learning Classification With Radial Basis Function Extreme Learning Machine Using Graphic Processors",
  "abstrak": "Ever-increasing size and complexity of datasets create challenges and potential tradeoffs of accuracy and speed in learning algorithms. This paper offers progress on both fronts. It presents a mechanism to train the unsupervised learning features learned from only one layer to improve performance in both speed and accuracy. The features are learned by an unsupervised feature learning (UFL) algorithm. Then, those features are trained by a fast radial basis function (RBF) extreme learning machine (ELM). By exploiting the massive parallel computing attribute of modern graphics processing unit, a customized compute unified device architecture (CUDA) kernel is developed to further speed up the computing of the RBF kernel in the ELM. Results tested on Canadian Institute for Advanced Research and Mixed National Institute of Standards and Technology datasets confirm the UFL RBF ELM achieves high accuracy, and the CUDA implementation is up to 20 times faster than CPU and the naive parallel approach.",
  "keyword": "machine learning"
 },
 {
  "No": 410,
  "judul": "Exploring Deep Learning and Machine Learning Approaches for Brain Hemorrhage Detection",
  "abstrak": "Brain hemorrhage refers to a potentially fatal medical disorder that affects millions of individuals. The percentage of patients who survive can be significantly raised with the prompt identification of brain hemorrhages, due to image-guided radiography, which has emerged as the predominant treatment modality in clinical practice. A Computed Tomography Image has frequently been employed for the purpose of identifying and diagnosing neurological disorders. The manual identification of anomalies in the brain region from the Computed Tomography Image demands the radiologist to devote a greater amount of time and dedication. In the most recent studies, a variety of techniques rooted in Deep learning and traditional Machine Learning have been introduced with the purpose of promptly and reliably detecting and classifying brain hemorrhage. This overview provides a comprehensive analysis of the surveys that have been conducted by utilizing Machine Learning and Deep Learning. This research focuses on the main stages of brain hemorrhage, which involve preprocessing, feature extraction, and classification, as well as their findings and limitations. Moreover, this in-depth analysis provides a description of the existing benchmark datasets that are utilized for the analysis of the detection process. A detailed comparison of performances is analyzed. Moreover, this paper addresses some aspects of the above-mentioned technique and provides insights into prospective possibilities for future research.",
  "keyword": "machine learning"
 },
 {
  "No": 411,
  "judul": "A Survey on Learning-Based Approaches for Modeling and Classification of Human–Machine Dialog Systems",
  "abstrak": "With the rapid development from traditional machine learning (ML) to deep learning (DL) and reinforcement learning (RL), dialog system equipped with learning mechanism has become the most effective solution to address human–machine interaction problems. The purpose of this article is to provide a comprehensive survey on learning-based human–machine dialog systems with a focus on the various dialog models. More specifically, we first introduce the fundamental process of establishing a dialog model. Second, we examine the features and classifications of the system dialog model, expound some representative models, and also compare the advantages and disadvantages of different dialog models. Third, we comb the commonly used database and evaluation metrics of the dialog model. Furthermore, the evaluation metrics of these dialog models are analyzed in detail. Finally, we briefly analyze the existing issues and point out the potential future direction on the human–machine dialog systems.",
  "keyword": "machine learning"
 },
 {
  "No": 412,
  "judul": "Solar Particle Event and Single Event Upset Prediction from SRAM-Based Monitor and Supervised Machine Learning",
  "abstrak": "The intensity of cosmic radiation may differ over five orders of magnitude within a few hours or days during the Solar Particle Events (SPEs), thus increasing for several orders of magnitude the probability of Single Event Upsets (SEUs) in space-borne electronic systems. Therefore, it is vital to enable the early detection of the SEU rate changes in order to ensure timely activation of dynamic radiation hardening measures. In this paper, an embedded approach for the prediction of SPEs and SRAM SEU rate is presented. The proposed solution combines the real-time SRAM-based SEU monitor, the offline-trained machine learning model and online learning algorithm for the prediction. With respect to the state-of-the-art, our solution brings the following benefits: (1) Use of existing on-chip data storage SRAM as a particle detector, thus minimizing the hardware and power overhead, (2) Prediction of SRAM SEU rate one hour in advance, with the fine-grained hourly tracking of SEU variations during SPEs as well as under normal conditions, (3) Online optimization of the prediction model for enhancing the prediction accuracy during run-time, (4) Negligible cost of hardware accelerator design for the implementation of selected machine learning model and online learning algorithm. The proposed design is intended for a highly dependable and self-adaptive multiprocessing system employed in space applications, allowing to trigger the radiation mitigation mechanisms before the onset of high radiation levels.",
  "keyword": "machine learning"
 },
 {
  "No": 413,
  "judul": "Genetic Programming Based Automated Machine Learning in Classifying ESG Performances",
  "abstrak": "AutoML offers significant benefits in solving real-life problems because it accelerates the development of machine learning models. In contexts involving real scenarios like analyzing companies’ environmental, social and governance (ESG), where the dataset presents some challenges, AutoML is anticipated as a promising solution to address these complexities. Although researchers have shown significant interest in exploring Genetic Programming (GP) in AutoML for handling complex datasets, a critical issue that remains unresolved is the comprehensive understanding of GP hyper-parameters that influence machine learning performance. While GP-based AutoML excels in automating many aspects of the modelling, there has been a scarcity of research that provides insight into the significance of individual features and GP population size within the models of GP-based AutoML. This paper presents a comprehensive analysis of the models’ performance evaluation from multiple facets, including feature selection, GP population sizes, and different machine learning algorithms. Furthermore, this study provides insights into the association between Pearson correlations, machine learning performance, and the importance of machine learning features. The findings demonstrate that incorporating all the determinants as features in GP-based AutoML or relying solely on firm characteristics led to superior performance with an excellent trade-off between True Positive Rate and False Positive Rate. Thus, higher accuracy results exceeding 0.9 of Area Under the Curve (AUC) are presented by the proposed models. The novelty of this study lies in its empirical evaluation of different approaches to GP-based AutoML implementation. These findings provide alternative solutions for business investors to identify companies with strong sustainability practices.",
  "keyword": "machine learning"
 },
 {
  "No": 414,
  "judul": "An Interpretable and Accurate Deep-Learning Diagnosis Framework Modeled With Fully and Semi-Supervised Reciprocal Learning",
  "abstrak": "The deployment of automated deep-learning classifiers in clinical practice has the potential to streamline the diagnosis process and improve the diagnosis accuracy, but the acceptance of those classifiers relies on both their accuracy and interpretability. In general, accurate deep-learning classifiers provide little model interpretability, while interpretable models do not have competitive classification accuracy. In this paper, we introduce a new deep-learning diagnosis framework, called InterNRL, that is designed to be highly accurate and interpretable. InterNRL consists of a student-teacher framework, where the student model is an interpretable prototype-based classifier (ProtoPNet) and the teacher is an accurate global image classifier (GlobalNet). The two classifiers are mutually optimised with a novel reciprocal learning paradigm in which the student ProtoPNet learns from optimal pseudo labels produced by the teacher GlobalNet, while GlobalNet learns from ProtoPNet’s classification performance and pseudo labels. This reciprocal learning paradigm enables InterNRL to be flexibly optimised under both fully- and semi-supervised learning scenarios, reaching state-of-the-art classification performance in both scenarios for the tasks of breast cancer and retinal disease diagnosis. Moreover, relying on weakly-labelled training images, InterNRL also achieves superior breast cancer localisation and brain tumour segmentation results than other competing methods.",
  "keyword": "machine learning"
 },
 {
  "No": 415,
  "judul": "Feature Evaluation of Emerging E-Learning Systems Using Machine Learning: An Extensive Survey",
  "abstrak": "As of late, with the progression of AI and man-made brainpower, there has been a developing spotlight on versatile e-learning. As all ways to deal with e-learning lose their allure and the level of online courses builds, they move towards more customized versatile learning so as to collaborate with students and achieve better learning results. The schools focus on the examination, mindfulness, and arranging techniques that infuse innovation into the vision and educational program. E-learning issues are a standard examination issue for us all. The motivation behind this research analysis is to separate the potential outcomes of assessing e-learning models utilizing AI strategies such as Supervised, Semi Supervised, Reinforced Learning advances by investigating upsides and downsides of various methods organization. The literature review methodology is to review the cross sectional impacts of e-learning and Machine learning algorithms from existing literatures from the year 1993 to 2020 and to assess the essentialness of e-learning features to optimize the e-learning models with available Machine learning techniques from peer-inspected journals, capable destinations, and books. Second, it legitimizes the chances of e-learning structures introduction, and changes demonstrated through AI and Machine Learning algorithms. This examination assists in providing helpful new highlights to analysts, researchers and academicians. It gives an exhaustive structure of existing e-learning frameworks for the most recent innovations identified with learning framework capacities and learning tasks to envision ML research openings in appropriate spaces. The survey paper identifies and demonstrates the important role of different types of e-learning features such as Individual pertinent feature, Course pertinent feature, Context pertinent feature and Technology pertinent feature in framework performance tuning. The performance of Machine Learning algorithms to optimize the features of E-...",
  "keyword": "machine learning"
 },
 {
  "No": 416,
  "judul": "Lung and Pancreatic Tumor Characterization in the Deep Learning Era: Novel Supervised and Unsupervised Learning Approaches",
  "abstrak": "Risk stratification (characterization) of tumors from radiology images can be more accurate and faster with computer-aided diagnosis (CAD) tools. Tumor characterization through such tools can also enable non-invasive cancer staging, prognosis, and foster personalized treatment planning as a part of precision medicine. In this paper, we propose both supervised and unsupervised machine learning strategies to improve tumor characterization. Our first approach is based on supervised learning for which we demonstrate significant gains with deep learning algorithms, particularly by utilizing a 3D convolutional neural network and transfer learning. Motivated by the radiologists' interpretations of the scans, we then show how to incorporate task-dependent feature representations into a CAD system via a graph-regularized sparse multi-task learning framework. In the second approach, we explore an unsupervised learning algorithm to address the limited availability of labeled training data, a common problem in medical imaging applications. Inspired by learning from label proportion approaches in computer vision, we propose to use proportion-support vector machine for characterizing tumors. We also seek the answer to the fundamental question about the goodness of “deep features” for unsupervised tumor classification. We evaluate our proposed supervised and unsupervised learning algorithms on two different tumor diagnosis challenges: lung and pancreas with 1018 CT and 171 MRI scans, respectively, and obtain the state-of-the-art sensitivity and specificity results in both problems.",
  "keyword": "machine learning"
 },
 {
  "No": 417,
  "judul": "Analysis of Learning Behavior Characteristics and Prediction of Learning Effect for Improving College Students’ Information Literacy Based on Machine Learning",
  "abstrak": "Information literacy is a basic ability for college students to adapt to social needs at present, and it is also a necessary quality for self-learning and lifelong learning. It is an effective way to reveal the information literacy teaching mechanism to use the rich and diverse information literacy learning behavior characteristics to carry out the learning effect prediction analysis. This paper analyzes the characteristics of college students’ learning behaviors and explores the predictive learning effect by constructing a predictive model of learning effect based on information literacy learning behavior characteristics. The experiment used 320 college students’ information literacy learning data from Chinese university. Pearson algorithm is used to analyze the learning behavior characteristics of college students’ information literacy, revealing that there is a significant correlation between the characteristics of information thinking and learning effect. The supervised classification algorithms such as Decision Tree, KNN, Naive Bayes, Neural Net and Random Forest are used to classify and predict the learning effect of college students’ information literacy. It is determined that the Random Forest prediction model has the best performance in the classification prediction of learning effect. The value of Accuracy is 92.50%, Precision is 84.56%, Recall is 94.81%, F1-Score is 89.39%, and Kapaa coefficient is 0.859. This paper puts forward differentiated intervention suggestions and management decision-making reference in the information literacy teaching process of college students, with a view to adjusting the information literacy teaching behavior, improving the information literacy teaching quality, optimizing educational decision-making, and promoting the sustainable development of high-quality and innovative talents in the information society.Our work involving research of the thinking and direction of the sustainable development of information literacy training proved to be encouraging.",
  "keyword": "machine learning"
 },
 {
  "No": 418,
  "judul": "A Systematic Review on Recent Advancements in Deep and Machine Learning Based Detection and Classification of Acute Lymphoblastic Leukemia",
  "abstrak": "Automatic Leukemia or blood cancer detection is a challenging job and is very much required in healthcare centers. It has a significant role in early diagnosis and treatment planning. Leukemia is a hematological disorder that starts from the bone marrow and affects white blood cells (WBCs). Microscopic analysis of WBCs is a preferred approach for an early detection of Leukemia since it is cost-effective and less painful. Very few literature reviews have been done to demonstrate a comprehensive analysis of deep and machine learning-based Acute Lymphoblastic Leukemia (ALL) detection. This article presents a systematic review of the recent advancements in this knowledge domain. Here, various artificial intelligence-based ALL detection approaches are analyzed in a systematic manner with merits and demits. The review of these schemes is conducted in a structured manner. For this purpose, segmentation schemes are broadly categorized into signal and image processing-based techniques, conventional machine learning-based techniques, and deep learning-based techniques. Conventional machine learning-based ALL classification approaches are categorized into supervised and unsupervised machine learning is presented. In addition, deep learning-based classification methods are categorized into Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and the Autoencoder. Then, CNN-based classification schemes are further categorized into conventional CNN, transfer learning, and other advancements in CNN. A brief discussion of these schemes and their importance in ALL classification are also presented. Moreover, a critical analysis is performed to present a clear idea about the recent research in this field. Finally, various challenging issues and future scopes are discussed that may assist readers in formulating new research problems in this domain.",
  "keyword": "machine learning"
 },
 {
  "No": 419,
  "judul": "Crime Prediction Using Machine Learning and Deep Learning: A Systematic Review and Future Directions",
  "abstrak": "Predicting crime using machine learning and deep learning techniques has gained considerable attention from researchers in recent years, focusing on identifying patterns and trends in crime occurrences. This review paper examines over 150 articles to explore the various machine learning and deep learning algorithms applied to predict crime. The study provides access to the datasets used for crime prediction by researchers and analyzes prominent approaches applied in machine learning and deep learning algorithms to predict crime, offering insights into different trends and factors related to criminal activities. Additionally, the paper highlights potential gaps and future directions that can enhance the accuracy of crime prediction. Finally, the comprehensive overview of research discussed in this paper on crime prediction using machine learning and deep learning approaches serves as a valuable reference for researchers in this field. By gaining a deeper understanding of crime prediction techniques, law enforcement agencies can develop strategies to prevent and respond to criminal activities more effectively.",
  "keyword": "machine learning"
 },
 {
  "No": 420,
  "judul": "Purely Structural Protein Scoring Functions Using Support Vector Machine and Ensemble Learning",
  "abstrak": "The function of a protein is determined by its structure, which creates a need for efficient methods of protein structure determination to advance scientific and medical research. Because current experimental structure determination methods carry a high price tag, computational predictions are highly desirable. Given a protein sequence, computational methods produce numerous 3D structures known as decoys. Selection of the best quality decoys is both challenging and essential as the end users can handle only a few ones. Therefore, scoring functions are central to decoy selection. They combine measurable features into a single number indicator of decoy quality. Unfortunately, current scoring functions do not consistently select the best decoys. Machine learning techniques offer great potential to improve decoy scoring. This paper presents two machine-learning based scoring functions to predict the quality of proteins structures, i.e., the similarity between the predicted structure and the experimental one without knowing the latter. We use different metrics to compare these scoring functions against three state-of-the-art scores. This is a first attempt at comparing different scoring functions using the same non-redundant dataset for training and testing and the same features. The results show that adding informative features may be more significant than the method used.",
  "keyword": "machine learning"
 },
 {
  "No": 421,
  "judul": "A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI",
  "abstrak": "Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward databased, mathematically grounded, and technically grounded medical education are encouraged.",
  "keyword": "machine learning"
 },
 {
  "No": 422,
  "judul": "A Review on Machine Learning for EEG Signal Processing in Bioengineering",
  "abstrak": "Electroencephalography (EEG) has been a staple method for identifying certain health conditions in patients since its discovery. Due to the many different types of classifiers available to use, the analysis methods are also equally numerous. In this review, we will be examining specifically machine learning methods that have been developed for EEG analysis with bioengineering applications. We reviewed literature from 1988 to 2018 to capture previous and current classification methods for EEG in multiple applications. From this information, we are able to determine the overall effectiveness of each machine learning method as well as the key characteristics. We have found that all the primary methods used in machine learning have been applied in some form in EEG classification. This ranges from Naive-Bayes to Decision Tree\/Random Forest, to Support Vector Machine (SVM). Supervised learning methods are on average of higher accuracy than their unsupervised counterparts. This includes SVM and KNN. While each of the methods individually is limited in their accuracy in their respective applications, there is hope that the combination of methods when implemented properly has a higher overall classification accuracy. This paper provides a comprehensive overview of Machine Learning applications used in EEG analysis. It also gives an overview of each of the methods and general applications that each is best suited to.",
  "keyword": "machine learning"
 },
 {
  "No": 423,
  "judul": "Predicting Machine Learning Pipeline Runtimes in the Context of Automated Machine Learning",
  "abstrak": "Automated machine learning (AutoML) seeks to automatically find so-called machine learning pipelines that maximize the prediction performance when being used to train a model on a given dataset. One of the main and yet open challenges in AutoMLis an effective use of computational resources: An AutoML process involves the evaluation of many candidate pipelines, which are costly but often ineffective because they are canceled due to a timeout. In this paper, we present an approach to predict the runtime of two-step machine learning pipelines with up to one pre-processor, which can be used to anticipate whether or not a pipeline will time out. Separate runtime models are trained offline for each algorithm that may be used in a pipeline, and an overall prediction is derived from these models. We empirically show that the approach increases successful evaluations made by an AutoML tool while preserving or even improving on the previously best solutions.",
  "keyword": "machine learning"
 },
 {
  "No": 424,
  "judul": "Innovations in Stroke Identification: A Machine Learning-Based Diagnostic Model Using Neuroimages",
  "abstrak": "Cerebrovascular diseases such as stroke are among the most common causes of death and disability worldwide and are preventable and treatable. Early detection of strokes and their rapid intervention play an important role in reducing the burden of disease and improving clinical outcomes. In recent years, machine learning methods have attracted a lot of attention as they can be used to detect strokes. The aim of this study is to identify reliable methods, algorithms, and features that help medical professionals make informed decisions about stroke treatment and prevention. To achieve this goal, we have developed an early stroke detection system based on CT images of the brain coupled with a genetic algorithm and a bidirectional long short-term Memory (BiLSTM) to detect strokes at a very early stage. For image classification, a genetic approach based on neural networks is used to select the most relevant features for classification. The BiLSTM model is then fed with these features. Cross-validation was used to evaluate the accuracy of the diagnostic system, precision, recall, F1 score, ROC (Receiver Operating Characteristic Curve), and AUC (Area Under The Curve). All of these metrics were used to determine the system’s overall effectiveness. The proposed diagnostic system achieved an accuracy of 96.5%. We also compared the performance of the proposed model with Logistic Regression, Decision Trees, Random Forests, Naive Bayes, and Support Vector Machines. With the proposed diagnosis system, physicians can make an informed decision about stroke.",
  "keyword": "machine learning"
 },
 {
  "No": 425,
  "judul": "A Machine Learning Approach Using Effective Connectivity to Predict Response to Clozapine Treatment",
  "abstrak": "Clozapine is an anti-psychotic drug that is known to be effective in the treatment of patients with chronic treatment-resistant schizophrenia (TRS-SCZ), commonly estimated to be around one third of all cases. However, clinicians sometimes delay the initiation of this drug because of its adverse side-effects. Therefore, identification of predictive biological markers of clozapine response are extremely valuable to aid on-time initiation of treatment. In this study, we develop a machine learning (ML) algorithm based on pre-treatment electroencephalogram (EEG) datasets to predict response to clozapine treatment in 57 TRS-SCZs, where the treatment outcome, after at least one-year follow-up is determined using the positive and negative syndrome scale (PANSS). The ML algorithm has three steps: 1) a brain source localization (BSL) procedure using the linearly constrained minimum variance (LCMV) beamforming approach is employed on the EEG signals to extract source waveforms from 30 specified brain regions. 2) An effective connectivity measure named symbolic transfer entropy (STE) is applied to the source waveforms. 3) A ML algorithm is applied to the STE matrix to determine whether a set of features can be found to discriminate most-responder (MR) SCZ patients from least-responder (LR) ones. The findings of this study reveal that STE features can achieve an accuracy of 95.83%. This finding implies that analysis of pre-treatment EEG could contribute to our ability to distinguish MR from LR SCZs, and that the source STE matrix may prove to be a promising tool for the prediction of the clinical response to clozapine.",
  "keyword": "machine learning"
 },
 {
  "No": 426,
  "judul": "The Quantum Path Kernel: A Generalized Neural Tangent Kernel for Deep Quantum Machine Learning",
  "abstrak": "Building a quantum analog of classical deep neural networks represents a fundamental challenge in quantum computing. A key issue is how to address the inherent nonlinearity of classical deep learning, a problem in the quantum domain due to the fact that the composition of an arbitrary number of quantum gates, consisting of a series of sequential unitary transformations, is intrinsically linear. This problem has been variously approached in literature, principally via the introduction of measurements between layers of unitary transformations. In this article, we introduce the quantum path kernel (QPK), a formulation of quantum machine learning capable of replicating those aspects of deep machine learning typically associated with superior generalization performance in the classical domain, specifically, \nhierarchical feature learning\n. Our approach generalizes the notion of quantum neural tangent kernel, which has been used to study the dynamics of classical and quantum machine learning models. The QPK exploits the parameter trajectory, i.e., the curve delineated by model parameters as they evolve during training, enabling the representation of differential layerwise convergence behaviors, or the formation of hierarchical parametric dependencies, in terms of their manifestation in the gradient space of the predictor function. We evaluate our approach with respect to variants of the classification of Gaussian \nxor\n mixtures: an artificial but emblematic problem that intrinsically requires multilevel learning in order to achieve optimal class separation.",
  "keyword": "machine learning"
 },
 {
  "No": 427,
  "judul": "Protein–Protein Interactions Prediction via Multimodal Deep Polynomial Network and Regularized Extreme Learning Machine",
  "abstrak": "Predicting the protein-protein interactions (PPIs) has played an important role in many applications. Hence, a novel computational method for PPIs prediction is highly desirable. PPIs endow with protein amino acid mutation rate and two physicochemical properties of protein (e.g., hydrophobicity and hydrophilicity). Deep polynomial network (DPN) is well-suited to integrate these modalities since it can represent any function on a finite sample dataset via the supervised deep learning algorithm. We propose a multimodal DPN (MDPN) algorithm to effectively integrate these modalities to enhance prediction performance. MDPN consists of a two-stage DPN, the first stage feeds multiple protein features into DPN encoding to obtain high-level feature representation while the second stage fuses and learns features by cascading three types of high-level features in the DPN encoding. We employ a regularized extreme learning machine to predict PPIs. The proposed method is tested on the public dataset of H. pylori, Human, and Yeast and achieves average accuracies of 97.87%, 99.90%, and 98.11%, respectively. The proposed method also achieves good accuracies on other datasets. Furthermore, we test our method on three kinds of PPI networks and obtain superior prediction results.",
  "keyword": "machine learning"
 },
 {
  "No": 428,
  "judul": "Applying Machine Learning Algorithms for the Classification of Sleep Disorders",
  "abstrak": "Sleep disorder classification is crucial in improving human quality of life. Sleep disorders and apnoea can have a significant influence on human health. Sleep-stage classification by experts in the field is an arduous task and is prone to human error. The development of accurate machine learning algorithms (MLAs) for sleep disorder classification requires analysing, monitoring and diagnosing sleep disorders. This paper compares deep learning algorithms and conventional MLAs to classify sleep disorders. This study proposes an optimised method for the Classification of Sleep Disorders and uses the Sleep Health and Lifestyle Dataset publicly available online to evaluate the proposed model. The optimisations were conducted using a genetic algorithm to tune the parameters of different machine learning algorithms. An evaluation and comparison of the proposed algorithm against state-of-the-art machine learning algorithms to classify sleep disorders. The dataset includes 400 rows and 13 columns with various features representing sleep and daily activities. The k-nearest neighbours, support vector machine, decision tree, random forest and artificial neural network (ANN) deep learning algorithms were assessed. The experimental results reveal significant performance differences between the evaluated algorithms. The proposed algorithms obtained a classification accuracy of 83.19%, 92.04%, 88.50%, 91.15% and 92.92%, respectively. The ANN achieved the highest classification accuracy of 92.92%, and its precision, recall and F1-score values on the testing datawere 92.01%, 93.80% and 91.93%, respectively. The ANN algorithm that achieved high accuracy than other tested algorithms.",
  "keyword": "machine learning"
 },
 {
  "No": 429,
  "judul": "E2LMs : Ensemble Extreme Learning Machines for Hyperspectral Image Classification",
  "abstrak": "Extreme learning machine (ELM) has attracted attentions in pattern recognition field due to its remarkable advantages such as fast operation, straightforward solution, and strong generalization. However, the performance of ELM for high-dimensional data, such as hyperspectral image, is still an open problem. Therefore, in this paper, we introduce ELM for hyperspectral image classification. Furthermore, in order to overcome the drawbacks of ELM caused by the randomness of input weights and bias, two new algorithms of ensemble extreme learning machines (Bagging-based and AdaBoost-based ELMs) are proposed for the classification task. In order to illustrate the performance of the proposed algorithms, support vector machines (SVMs) are used for evaluation and comparison. Experimental results with real hyperspectral images collected by reflective optics spectrographic image system (ROSIS) and airborne visible\/infrared imaging spectrometer (AVIRIS) indicate that the proposed ensemble algorithms produce excellent classification performance in different scenarios with respect to spectral and spectral–spatial feature sets.",
  "keyword": "machine learning"
 },
 {
  "No": 430,
  "judul": "A Hybrid Posture Detection Framework: Integrating Machine Learning and Deep Neural Networks",
  "abstrak": "The posture detection received lots of attention in the fields of human sensing and artificial intelligence. Posture detection can be used for the monitoring health status of elderly remotely by identifying their postures such as standing, sitting and walking. Most of the current studies used traditional machine learning classifiers to identify the posture. However, these methods do not perform well to detect the postures accurately. Therefore, in this study, we proposed a novel hybrid approach based on machine learning classifiers (i. e., support vector machine (SVM), logistic regression (KNN), decision tree, Naive Bayes, random forest, Linear discrete analysis and Quadratic discrete analysis) and deep learning classifiers (i. e., 1D-convolutional neural network (1D-CNN), 2D-convolutional neural network (2D-CNN), LSTM and bidirectional LSTM) to identify posture detection. The proposed hybrid approach uses prediction of machine learning (ML) and deep learning (DL) to improve the performance of ML and DL algorithms. The experimental results on widely benchmark dataset are shown and results achieved an accuracy of more than 98%.",
  "keyword": "machine learning"
 },
 {
  "No": 431,
  "judul": "Sufficiency of Ensemble Machine Learning Methods for Phishing Websites Detection",
  "abstrak": "Phishing is a kind of worldwide spread cybercrime that uses disguised websites to trick users into downloading malware or providing personally sensitive information to attackers. With the rapid development of artificial intelligence, more and more researchers in the cybersecurity field utilize machine learning and deep learning algorithms to classify phishing websites. In order to compare the performances of various machine learning and deep learning methods, several experiments are conducted in this study. According to the experimental results, ensemble machine learning algorithms stand out among other candidates in both detection accuracy and computational consumption. Furthermore, the ensemble architectures still provide impressive capability when the amount of features decreases sharply in the dataset. Subsequently, the paper discusses the factors why ensemble machine learning methods are more suitable for the binary phishing classification challenge in up-date training and real-time detecting environment, which reflects the sufficiency of ensemble machine learning methods in anti-phishing techniques.",
  "keyword": "machine learning"
 },
 {
  "No": 432,
  "judul": "Using Machine Learning for Predicting the Effect of Mutations in the Initiation Codon",
  "abstrak": "The effect of mutations has been traditionally predicted by studying what may happen due to the substitution of one amino acid for another one. This approach may be effective for mutations with impact in the function of the protein, but ineffective for mutations in the translation initiation codon. Such mutation might avoid the generation of the protein. Consequently, specific methods for predicting the effect of mutations in the translation initiation codon are needed. We propose a method for predicting the effect of mutations in the canonical translation initiation codon based on a biological model that considers specific features of such mutations, like the distance to a potential alternative initiation codon. Our predictor has been developed using tree-based machine learning algorithms and dataextracted from Ensembl . Our final model is able to detect whether a mutation in the canonical initiation codon is deleterious or benign with a precision of 44.28% and an accuracy of 98.32%, which improves the results of state of the art tools such as PolyPhen , SIFT , or CADD for this type of mutation.",
  "keyword": "machine learning"
 },
 {
  "No": 433,
  "judul": "Prediction of Chronic Kidney Disease - A Machine Learning Perspective",
  "abstrak": "Chronic Kidney Disease is one of the most critical illness nowadays and proper diagnosis is required as soon as possible. Machine learning technique has become reliable for medical treatment. With the help of a machine learning classifier algorithms, the doctor can detect the disease on time. For this perspective, Chronic Kidney Disease prediction has been discussed in this article. Chronic Kidney Disease dataset has been taken from the UCI repository. Seven classifier algorithms have been applied in this research such as artificial neural network, C5.0, Chi-square Automatic interaction detector, logistic regression, linear support vector machine with penalty L1 & with penalty L2 and random tree. The important feature selection technique was also applied to the dataset. For each classifier, the results have been computed based on (i) full features, (ii) correlation-based feature selection, (iii) Wrapper method feature selection, (iv) Least absolute shrinkage and selection operator regression, (v) synthetic minority over-sampling technique with least absolute shrinkage and selection operator regression selected features, (vi) synthetic minority over-sampling technique with full features. From the results, it is marked that LSVM with penalty L2 is giving the highest accuracy of 98.86% in synthetic minority over-sampling technique with full features. Along with accuracy, precision, recall, F-measure, area under the curve and GINI coefficient have been computed and compared results of various algorithms have been shown in the graph. Least absolute shrinkage and selection operator regression selected features with synthetic minority over-sampling technique gave the best after synthetic minority over-sampling technique with full features. In the synthetic minority over-sampling technique with least absolute shrinkage and selection operator selected features, again linear support vector machine gave the highest accuracy of 98.46%. Along with machine learning models one deep neural network has been applied on the same dataset and it has been noted that deep neural network achieved the highest accuracy of 99.6%.",
  "keyword": "machine learning"
 },
 {
  "No": 434,
  "judul": "Extending the Tsetlin Machine With Integer-Weighted Clauses for Increased Interpretability",
  "abstrak": "Building models that are both interpretable and accurate is an unresolved challenge for many pattern recognition problems. In general, rule-based and linear models lack accuracy, while deep learning interpretability is based on rough approximations of the underlying inference. However, recently, the rule-based Tsetlin Machines (TMs) have obtained competitive performance in terms of accuracy, memory footprint, and inference speed on diverse benchmarks (imageclassification, regression, natural language understanding, and game-playing). TMs construct rules using human-interpretable conjunctive clauses in propositional logic. These, in turn, are combined linearly to solve complex pattern recognition tasks. This paper addresses the accuracy-interpretability challenge in machine learning by introducing a TM with integer weighted clauses - the Integer Weighted TM (IWTM). The intent is to increase TM interpretability by reducing the number of clauses required for competitive performance. The IWTM achieves this by weighting the clauses so that a single clause can replace multiple duplicates. Since each TM clause is formed adaptively by a Tsetlin Automata (TA) team, identifying effective weights becomes a challenging online learning problem. We solve this problem by extending each team of TA with another kind of automaton: the stochastic searching on the line (SSL) automaton. We evaluate the performance of the new scheme empirically using five datasets, along with a study of interpretability. On average, IWTM uses 6.5 times fewer literals than the vanilla TM and 120 times fewer literals than a TM with real-valued weights. Furthermore, in terms of average memory usage and F1-Score, IWTM outperforms simple Multi-Layered Artificial Neural Networks, Decision Trees, Support Vector Machines, K-Nearest Neighbor, Random Forest, Gradient Boosted Trees (XGBoost), Explainable Boosting Machines (EBMs), as well as the standard and real-value weighted TMs. IWTM finally outperforms Neural Additive Models on Fraud Detection and StructureBoost on CA-58 in terms of Area Under Curve, while performing competitively on COMPAS.",
  "keyword": "machine learning"
 },
 {
  "No": 435,
  "judul": "Machine Learning Based Suicide Ideation Prediction for Military Personnel",
  "abstrak": "Military personnel have greater psychological stress and are at higher suicide attempt risk compared with the general population. High mental stress may cause suicide ideations which are crucially driving suicide attempts. However, traditional statistical methods could only find a moderate degree of correlation between psychological stress and suicide ideation in non-psychiatric individuals. This article utilizes machine learning techniques including logistic regression, decision tree, random forest, gradient boosting regression tree, support vector machine and multilayer perceptron to predict the presence of suicide ideation by six important psychological stress domains of the military males and females. The accuracies of all the six machine learning methods are over 98%. Among them, the multilayer perceptron and support vector machine provide the best predictions of suicide ideation approximately to 100%. As compared with the BSRS-5 score ≥7, a conventional criterion, for the presence of suicide ideation ≥1, the proposed algorithms can improve the performances of accuracy, sensitivity, specificity, precision, the AUC of ROC curve and the AUC of PR curve up to 5.7%, 35.9%, 4.6%, 65.2%, 4.3% and 53.2%, respectively; and for the presence of more severely intense suicide ideation ≥2, the improvements are 6.1%, 26.2%, 5.8%, 83.5%, 2.8% and 64.7%, respectively.",
  "keyword": "machine learning"
 },
 {
  "No": 436,
  "judul": "Understanding Depth of Reflective Writing in Workplace Learning Assessments Using Machine Learning Classification",
  "abstrak": "Self-reflection and reflective writing have been pivotal for developing a deep understanding of concepts and fostering professional competency in learners. The confluence of the importance of reflective practices within the educational curriculum and the increased proliferation of technology have resulted in numerous studies of how to use automated approaches to analyze broad themes of reflection exhibited by learners in higher educational settings. However, there is a dearth of research in the context of automated analysis of reflective writing demonstrated by professionals within workplace learning. Building on a four-level reflective content analysis model, this article evaluates the depth of reflection exhibited by learners within a professional development MOOC using an automated assessment classifier. Our results identify the varying association of linguistic features across these different levels of reflection. The proposed approach can effectively support the deployment at scale of the labor-intensive evaluation of reflective writing by highly trained professionals.",
  "keyword": "machine learning"
 },
 {
  "No": 437,
  "judul": "Resilient Machine Learning for Networked Cyber Physical Systems: A Survey for Machine Learning Security to Securing Machine Learning for CPS",
  "abstrak": "Cyber Physical Systems (CPS) are characterized by their ability to integrate the physical and information or cyber worlds. Their deployment in critical infrastructure have demonstrated a potential to transform the world. However, harnessing this potential is limited by their critical nature and the far reaching effects of cyber attacks on human, infrastructure and the environment. An attraction for cyber concerns in CPS rises from the process of sending information from sensors to actuators over the wireless communication medium, thereby widening the attack surface. Traditionally, CPS security has been investigated from the perspective of preventing intruders from gaining access to the system using cryptography and other access control techniques. Most research work have therefore focused on the detection of attacks in CPS. However, in a world of increasing adversaries, it is becoming more difficult to totally prevent CPS from adversarial attacks, hence the need to focus on making CPS resilient. Resilient CPS are designed to withstand disruptions and remain functional despite the operation of adversaries. One of the dominant methodologies explored for building resilient CPS is dependent on machine learning (ML) algorithms. However, rising from recent research in adversarial ML, we posit that ML algorithms for securing CPS must themselves be resilient. This article is therefore aimed at comprehensively surveying the interactions between resilient CPS using ML and resilient ML when applied in CPS. The paper concludes with a number of research trends and promising future research directions. Furthermore, with this article, readers can have a thorough understanding of recent advances on ML-based security and securing ML for CPS and countermeasures, as well as research trends in this active research area.",
  "keyword": "machine learning"
 },
 {
  "No": 438,
  "judul": "Power Allocation Schemes Based on Deep Learning for Distributed Antenna Systems",
  "abstrak": "In recent years, a lot of power allocation algorithms have been proposed to maximize spectral efficiency (SE) and energy efficiency (EE) for the distributed antenna systems (DAS). However, the traditional iterative power allocation algorithms are difficult to be implemented in reality because of their high computational complexity. With the development of machine learning algorithms, it has been proved that the machine learning method has excellent learning ability and low computational complexity, which can approximate the traditional iterative power allocation well and be easily to be implemented in reality. In this paper, we propose a new deep neural network (DNN) model for DAS. From the perspective of machine learning, traditional iterative algorithms can be regarded as a nonlinear mapping between user channel realizations and optimal power allocation schemes. Therefore, we train the DNN to learn the nonlinear mapping between the user channel realizations and the corresponding power allocation schemes based on the traditional iterative algorithm. Then, a power allocation schemes based on DNN method is developed to maximize SE and EE for DAS. The simulation results show that the proposed scheme can not only obtain the almost similar performance as the traditional iterative algorithm, but also reduce much online computational time.",
  "keyword": "machine learning"
 },
 {
  "No": 439,
  "judul": "A Novel Approach for Polycystic Ovary Syndrome Prediction Using Machine Learning in Bioinformatics",
  "abstrak": "Polycystic ovary syndrome (PCOS) is a critical disorder in women during their reproduction phase. The PCOS disorder is commonly caused by excess male hormone and androgen levels. The follicles are the collections of fluid developed by ovaries and may fail to release eggs regularly. The PCOS results in miscarriage, infertility issues, and complications during pregnancy. According to a recent report, PCOS is diagnosed in 31.3% of women from Asia. Studies show that 69% to 70% of women did not avail of a detecting cure for PCOS. A research study is needed to save women from critical complications by identifying PCOS early. The main aim of our research is to predict PCOS using advanced machine learning techniques. The dataset based on clinical and physical parameters of women is utilized for building study models. A novel feature selection approach is proposed based on the optimized chi-squared (CS-PCOS) mechanism. The ten hyper-parametrized machine learning models are applied in comparison. Using the novel CS-PCOS approach, the gaussian naive bayes (GNB) outperformed machine learning models and state-of-the-art studies. The GNB achieved 100% accuracy, precision, recall, and f1-scores with minimal time computations of 0.002 seconds. The k-fold cross-validation of GNB achieved a 100% accuracy score. The proposed GNB model achieved accurate results for critical PCOS prediction. Our study reveals that the dataset features prolactin (PRL), blood pressure systolic, blood pressure diastolic, thyroid stimulating hormone (TSH), relative risk (RR-breaths), and pregnancy are the prominent factors having high involvement in PCOS prediction. Our research study helps the medical community overcome the miscarriage rate and provide a cure to women through the early detection of PCOS.",
  "keyword": "machine learning"
 },
 {
  "No": 440,
  "judul": "Using Machine Learning to Improve the Prediction of Functional Outcome in Ischemic Stroke Patients",
  "abstrak": "Ischemic stroke is a leading cause of disability and death worldwide among adults. The individual prognosis after stroke is extremely dependent on treatment decisions physicians take during the acute phase. In the last five years, several scores such as the ASTRAL, DRAGON, and THRIVE have been proposed as tools to help physicians predict the patient functional outcome after a stroke. These scores are rule-based classifiers that use features available when the patient is admitted to the emergency room. In this paper, we apply machine learning techniques to the problem of predicting the functional outcome of ischemic stroke patients, three months after admission. We show that a pure machine learning approach achieves only a marginally superior Area Under the ROC Curve (AUC) ( 0.808±0.085 ) than that of the best score ( 0.771±0.056 ) when using the features available at admission. However, we observed that by progressively adding features available at further points in time, we can significantly increase the AUC to a value above 0.90. We conclude that the results obtained validate the use of the scores at the time of admission, but also point to the importance of using more features, which require more advanced methods, when possible.",
  "keyword": "machine learning"
 },
 {
  "No": 441,
  "judul": "Enhanced Machine Learning Techniques for Early HARQ Feedback Prediction in 5G",
  "abstrak": "We investigate Early Hybrid Automatic Repeat reQuest (E-HARQ) feedback schemes enhanced by machine learning techniques as a path towards ultra-reliable and low-latency communication (URLLC). To this end, we propose machine learning methods to predict the outcome of the decoding process ahead of the end of the transmission. We discuss different input features and classification algorithms ranging from traditional methods to newly developed supervised autoencoders. These methods are evaluated based on their prospects of complying with the URLLC requirements of effective block error rates below 10 -5 at small latency overheads. We provide realistic performance estimates in a system model incorporating scheduling effects to demonstrate the feasibility of E-HARQ across different signal-to-noise ratios, subcode lengths, channel conditions and system loads, and show the benefit over regular HARQ and existing E-HARQ schemes without machine learning.",
  "keyword": "machine learning"
 },
 {
  "No": 442,
  "judul": "Automated Ischemic Stroke Subtyping Based on Machine Learning Approach",
  "abstrak": "Ischemic stroke subtyping was not only highly valuable for effective intervention and treatment, but also important to the prognosis of ischemic stroke. The manual adjudication of disease classification was time-consuming, error-prone, and limits scaling to large datasets. In this study, an integrated machine learning approach was used to classify the subtype of ischemic stroke on The International Stroke Trial (IST) dataset. We considered the common problems of feature selection and prediction in medical datasets. Firstly, the importances of features were ranked by the Shapiro-Wilk algorithm and Pearson correlations between features were analyzed. Then, we used Recursive Feature Elimination with Cross-Validation (RFECV), which incorporated linear SVC, Random-Forest-Classifier, Extra-Trees-Classifier, AdaBoost-Classifier, and Multinomial-Naïve-Bayes-Classifier as estimator respectively, to select robust features important to ischemic stroke subtyping. Furthermore, the importances of selected features were determined by Extra-Trees-Classifier. Finally, the selected features were used by Extra-Trees-Classifier and a simple deep learning model to classify the ischemic stroke subtype on IST dataset. It was suggested that the described method could classify ischemic stroke subtype accurately. And the result showed that the machine learning approaches outperformed human professionals.",
  "keyword": "machine learning"
 },
 {
  "No": 443,
  "judul": "Predicting Brain Age Using Machine Learning Algorithms: A Comprehensive Evaluation",
  "abstrak": "Machine learning (ML) algorithms play a vital role in the brain age estimation frameworks. The impact of regression algorithms on prediction accuracy in the brain age estimation frameworks have not been comprehensively evaluated. Here, we sought to assess the efficiency of different regression algorithms on brain age estimation. To this end, we built a brain age estimation framework based on a large set of cognitively healthy (CH) individuals ( N=788 ) as a training set followed by different regression algorithms (22 different algorithms in total). We then quantified each regression-algorithm on independent test sets composed of 88 CH individuals, 70 mild cognitive impairment patients as well as 30 Alzheimer’s disease patients. The prediction accuracy in the independent test set (i.e., CH set) varied in regression algorithms mean absolute error (MAE) from 4.63 to 7.14 yrs, R2 from 0.76 to 0.88. The highest and lowest prediction accuracies were achieved by Quadratic Support Vector Regression algorithm (MAE =4.63 yrs, R2=0.88,95% CI =[−1.26,1.42] ) and Binary Decision Tree algorithm (MAE =7.14 yrs, R2=0.76,95% CI =[−1.50,2.62] ), respectively. Our experimental results demonstrate that the prediction accuracy in brain age frameworks is affected by regression algorithms, indicating that advanced machine learning algorithms can lead to more accurate brain age predictions in clinical settings.",
  "keyword": "machine learning"
 },
 {
  "No": 444,
  "judul": "On the Use of Machine Learning for Classifying Auditory Brainstem Responses: A Scoping Review",
  "abstrak": "Recent advances in machine learning have led to a surge of interest in classification of the auditory brainstem response. In this work, we conducted a search in the PubMed, Google Scholar, SpringerLink, ScienceDirect, and Scopus databases, and identified twelve studies that explored the use of machine learning to classify the auditory brainstem response as a complementary and objective method to (a) help clinicians better diagnose hearing impairment by discerning between healthy and pathological auditory brainstem response waveforms, (b) present a neural marker for potential applications in hearing aid tuning, and (c) provide a biometric marker for discriminating between subjects. A comparison between the studies presented in this review is not possible as they used different test subjects, group sizes, and stimuli, and evaluated auditory brainstem response differently. Instead, the result of these studies will be presented and their limitations as well as their potential applications will be discussed. Overall, the findings of these studies suggest that ABR classification using machine learning is a promising tool for assessing patients with hearing loss, optimizing technologies for tuning hearing aids, and discriminating between subjects.",
  "keyword": "machine learning"
 },
 {
  "No": 445,
  "judul": "Diabetes Prediction Using Machine Learning Algorithms and Ontology",
  "abstrak": "Diabetes is one of the chronic diseases, which is increasing from year to year. The problems begin when diabetes is not detected at an early phase and diagnosed properly at the appropriate time. Different machine learning techniques, as well as ontology-based ML techniques, have recently played an important role in medical science by developing an automated system that can detect diabetes patients. This paper provides a comparative study and review of the most popular machine learning techniques and ontology-based Machine Learning classification. Various types of classification algorithms were considered namely: SVM, KNN, ANN, Naive Bayes, Logistic regression, and Decision Tree. The results are evaluated based on performance metrics like Recall, Accuracy, Precision, and F-Measure that are derived from the confusion matrix. The experimental results showed that the best accuracy goes for ontology classifiers and SVM.",
  "keyword": "machine learning"
 },
 {
  "No": 446,
  "judul": "Advance Machine Learning Methods for Dyslexia Biomarker Detection: A Review of Implementation Details and Challenges",
  "abstrak": "Dyslexia is a neurological disorder that is characterized by imprecise comprehension of words and generally poor reading performance. It affects a significant population of school-age children, with more occurrences in males, thus, putting them at risk of poor academic performance and low self-esteem for a lifetime. The long-term hope is to have a dyslexia diagnostic method that is informed by neural-biomarkers. In this regard, large numbers of machine learning methods and, more recently, deep learning methods have been implemented across various types of dataset with the above-chance classification accuracy. However, attainment of clinical acceptability of these state-of-the-art methods is bedeviled by certain challenges including lack of biologically-interpretable biomarkers, privacy of dataset and classifiers, hyper-parameter selection\/optimization, and overfitting problem among others. This review paper critically analyzes recent machine learning methods for detecting dyslexia and its biomarkers and discusses challenges that require proper attentions from the users of deep learning methods in order to enable them to attain clinically relevance and acceptable level. The review is conducted within the premise of implementation and experimental outcomes for each of the 22 selected articles using the Preferred Reporting Items for Systematic review and Meta-Analyses (PRISMA) protocol, with a view to outlining some critical challenges for achieving high accuracy and reliability of the state-of-the-art machine learning methods. As an evidence-based protocol for reporting in systematic reviews and meta-analyses, PRISMA helps to ensure clarity and transparency of this paper by showing a four-phase flow diagram of the selection process for articles used in this review. It is therefore, envisaged that higher classification performance of clinical relevance can be achieved using deep learning models for dyslexia and its biomarkers by addressing identified potential challeng...",
  "keyword": "machine learning"
 },
 {
  "No": 447,
  "judul": "A Review on Arabic Sentiment Analysis: State-of-the-Art, Taxonomy and Open Research Challenges",
  "abstrak": "Due to the significant use of Arabic language in social media networks, the demand for Arabic sentiment analysis has increased rapidly. Although, numerous sentiment analysis techniques enable people to obtain valuable insights from the opinions shared on social media. However, these techniques are still in their infancy, and the Arabic sentiment analysis domain lacks a compressive survey. Therefore, this study focused on the various characteristics, State-of-the-Art, and the level of sentiment analysis along with the natural language processing applied in the Arabic sentiment analysis. Furthermore, this study also discussed the sentiment analysis of the modern standards and the dialects of Arabic languages along with various machine learning processes and a few popular algorithms. Moreover, this study adds values by critical analysis of two case studies, which displayed an extensive set of the various research communities in this field of sentiment analysis. Finally, open research challenges are investigated, with a focus on the shortage of lexicons; availability; use of Dialect Arabic (DA); lack of corpora and datasets; right to left reading and compound phrases and idioms.",
  "keyword": "machine learning"
 },
 {
  "No": 448,
  "judul": "A Novel Tropical Geometry-Based Interpretable Machine Learning Method: Pilot Application to Delivery of Advanced Heart Failure Therapies",
  "abstrak": "A model's interpretability is essential to many practical applications such as clinical decision support systems. In this article, a novel interpretable machine learning method is presented, which can model the relationship between input variables and responses in humanly understandable rules. The method is built by applying tropical geometry to fuzzy inference systems, wherein variable encoding functions and salient rules can be discovered by supervised learning. Experiments using synthetic datasets were conducted to demonstrate the performance and capacity of the proposed algorithm in classification and rule discovery. Furthermore, we present a pilot application in identifying heart failure patients that are eligible for advanced therapies as proof of principle. From our results on this particular application, the proposed network achieves the highest F1 score. The network is capable of learning rules that can be interpreted and used by clinical providers. In addition, existing fuzzy domain knowledge can be easily transferred into the network and facilitate model training. In our application, with the existing knowledge, the F1 score was improved by over 5%. The characteristics of the proposed network make it promising in applications requiring model reliability and justification.",
  "keyword": "machine learning"
 },
 {
  "No": 449,
  "judul": "Securing Connected & Autonomous Vehicles: Challenges Posed by Adversarial Machine Learning and the Way Forward",
  "abstrak": "Connected and autonomous vehicles (CAVs) will form the backbone of future next-generation intelligent transportation systems (ITS) providing travel comfort, road safety, along with a number of value-added services. Such a transformation-which will be fuelled by concomitant advances in technologies for machine learning (ML) and wireless communications-will enable a future vehicular ecosystem that is better featured and more efficient. However, there are lurking security problems related to the use of ML in such a critical setting where an incorrect ML decision may not only be a nuisance but can lead to loss of precious lives. In this paper, we present an in-depth overview of the various challenges associated with the application of ML in vehicular networks. In addition, we formulate the ML pipeline of CAVs and present various potential security issues associated with the adoption of ML methods. In particular, we focus on the perspective of adversarial ML attacks on CAVs and outline a solution to defend against adversarial attacks in multiple settings.",
  "keyword": "machine learning"
 },
 {
  "No": 450,
  "judul": "The Age of Ransomware: A Survey on the Evolution, Taxonomy, and Research Directions",
  "abstrak": "The proliferation of ransomware has become a significant threat to cybersecurity in recent years, causing significant financial, reputational, and operational damage to individuals and organizations. This paper aims to provide a comprehensive overview of the evolution of ransomware, its taxonomy, and its state-of-the-art research contributions. We begin by tracing the origins of ransomware and its evolution over time, highlighting the key milestones and major trends. Next, we propose a taxonomy of ransomware that categorizes different types of ransomware based on their characteristics and behavior. Subsequently, we review the existing research over several years in regard to detection, prevention, mitigation, and prediction techniques. Our extensive analysis, based on more than 150 references, has revealed that significant research, specifically 72.8%, has focused on detecting ransomware. However, a lack of emphasis has been placed on predicting ransomware. Additionally, of the studies focused on ransomware detection, a significant portion, 70%, have utilized Machine Learning methods. This study uncovers a range of shortcomings in research pertaining to real-time protection and identifying zero-day ransomware, and two issues specific to Machine Learning models. Adversarial machine learning exploitation and concept drift have been identified as under-researched areas in the field. This survey is a constructive roadmap for researchers interested in ransomware research matters.",
  "keyword": "machine learning"
 },
 {
  "No": 451,
  "judul": "A Brute-Force Black-Box Method to Attack Machine Learning-Based Systems in Cybersecurity",
  "abstrak": "Machine learning algorithms are widely utilized in cybersecurity. However, recent studies show that machine learning algorithms are vulnerable to adversarial examples. This poses new threats to the security-critical applications in cybersecurity. Currently, there is still a short of study on adversarial examples in the domain of cybersecurity. In this paper, we propose a new method known as the brute-force attack method to better evaluate the robustness of the machine learning classifiers in cybersecurity against adversarial examples. The proposed method, which works in a black-box way and covers some shortages of the existing adversarial attack methods based on generative adversarial networks, is simple to implement and only needs the output of the target classifiers to generate adversarial examples. To have a comprehensive evaluation of the attack performance of the proposed method, we use our method to generate adversarial examples against the common machine learning based security systems in cybersecurity including host intrusion detection systems, Android malware detection systems, and network intrusion detection systems. We compare the attack performance of the proposed method against these security systems with that of state-of-the-art adversarial attack methods based on generative adversarial networks. The preliminary experimental results show that the proposed method, which is more efficient in computation and outperforms the state-of-the-art attack methods based on generative adversarial networks, can be used to evaluate the robustness of various machine learning based systems in cybersecurity against adversarial examples.",
  "keyword": "machine learning"
 },
 {
  "No": 452,
  "judul": "Object Detection Recognition and Robot Grasping Based on Machine Learning: A Survey",
  "abstrak": "With the rapid development of machine learning, its powerful function in the machine vision field is increasingly reflected. The combination of machine vision and robotics to achieve the same precise and fast grasping as that of humans requires high-precision target detection and recognition, location and reasonable grasp strategy generation, which is the ultimate goal of global researchers and one of the prerequisites for the large-scale application of robots. Traditional machine learning has a long history and good achievements in the field of image processing and robot control. The CNN (convolutional neural network) algorithm realizes training of large-scale image datasets, solves the disadvantages of traditional machine learning in large datasets, and greatly improves accuracy, thereby positioning CNNs as a global research hotspot. However, the increasing difficulty of labeled data acquisition limits their development. Therefore, unsupervised learning, self-supervised learning and reinforcement learning, which are less dependent on labeled data, have also undergone rapid development and achieved good performance in the fields of image processing and robot capture. According to the inherent defects of vision, this paper summarizes the research achievements of tactile feedback in the fields of target recognition and robot grasping and finds that the combination of vision and tactile feedback can improve the success rate and robustness of robot grasping. This paper provides a systematic summary and analysis of the research status of machine vision and tactile feedback in the field of robot grasping and establishes a reasonable reference for future research.",
  "keyword": "machine learning"
 },
 {
  "No": 453,
  "judul": "Cooperative Spectrum Sensing Meets Machine Learning: Deep Reinforcement Learning Approach",
  "abstrak": "Cognitive radio network (CRN) emerged to utilize the frequency bands efficiently. To use the frequency bands efficiently without any interference on the licensed user, detection of the frequency holes is the first step, which is called spectrum sensing in the context. In order to increase the quality of local spectrum sensing results, cooperative spectrum sensing (CSS) is introduced in the literature to combine the local sensing results. Recently, machine learning techniques are designed to improve the classification of the images and signals. Specifically, Deep Reinforcement Learning (DRL) is of interest for its substantial improvement in the classification problems. In this letter, we have proposed DRL based CSS algorithm, which is employed to decrease the signaling in the network of SUs. The simulation results represent the superiority of the proposed approach to state-of-the-art approaches, including Deep Cooperative Sensing (DCS), K-out-of-N, and Support Vector Machine (SVM) based CSS algorithms.",
  "keyword": "machine learning"
 },
 {
  "No": 454,
  "judul": "Predicting Agriculture Yields Based on Machine Learning Using Regression and Deep Learning",
  "abstrak": "Agriculture contributes a significant amount to the economy of India due to the dependence on human beings for their survival. The main obstacle to food security is population expansion leading to rising demand for food. Farmers must produce more on the same land to boost the supply. Through crop yield prediction, technology can assist farmers in producing more. This paper’s primary goal is to predict crop yield utilizing the variables of rainfall, crop, meteorological conditions, area, production, and yield that have posed a serious threat to the long-term viability of agriculture. Crop yield prediction is a decision-support tool that uses machine learning and deep learning that can be used to make decisions about which crops to produce and what to do in the crop’s growing season. It can decide which crops to produce and what to do in the crop’s growing season. Regardless of the distracting environment, machine learning and deep learning algorithms are utilized in crop selection to reduce agricultural yield output losses. To estimate the agricultural yield, machine learning techniques: decision tree, random forest, and XGBoost regression; deep learning techniques - convolutional neural network and long-short term memory network have been used. Accuracy, root mean square error, mean square error, mean absolute error, standard deviation, and losses are compared. Other machine learning and deep learning methods fall short compared to the random forest and convolutional neural network. The random forest has a maximum accuracy of 98.96%, mean absolute error of 1.97, root mean square error of 2.45, and standard deviation of 1.23. The convolutional neural network has been evaluated with a minimum loss of 0.00060. Consequently, a model is developed that, compared to other algorithms, predicts the yield quite well. The findings are then analyzed using the root mean square error metric to understand better how the model’s errors compare to those of the other methods.",
  "keyword": "machine learning"
 },
 {
  "No": 455,
  "judul": "Predicting Machine Learning Pipeline Runtimes in the Context of Automated Machine Learning",
  "abstrak": "Automated machine learning (AutoML) seeks to automatically find so-called machine learning pipelines that maximize the prediction performance when being used to train a model on a given dataset. One of the main and yet open challenges in AutoMLis an effective use of computational resources: An AutoML process involves the evaluation of many candidate pipelines, which are costly but often ineffective because they are canceled due to a timeout. In this paper, we present an approach to predict the runtime of two-step machine learning pipelines with up to one pre-processor, which can be used to anticipate whether or not a pipeline will time out. Separate runtime models are trained offline for each algorithm that may be used in a pipeline, and an overall prediction is derived from these models. We empirically show that the approach increases successful evaluations made by an AutoML tool while preserving or even improving on the previously best solutions.",
  "keyword": "machine learning"
 },
 {
  "No": 456,
  "judul": "SoK of Machine Learning and Deep Learning Based Anomaly Detection Methods for Automatic Dependent Surveillance- Broadcast",
  "abstrak": "This paper focuses on the vulnerabilities of ADS-B, one of the avionics systems, and the countermeasures taken against these vulnerabilities proposed in the literature. Among the proposed countermeasures against the vulnerabilities of ADS-B, anomaly detection methods based on machine learning and deep learning algorithms were analyzed in detail. The advantages and disadvantages of using an anomaly detection system on ADS-B dataare investigated. Thanks to advances in machine learning and deep learning over the last decade, it has become more appropriate to use anomaly detection systems to detect anomalies in ADS-B systems. To the best of our knowledge, this is the first survey to focus on studies using machine learning and deep learning algorithms for ADS-B security. In this context, this study addresses research on this topic from different perspectives, draws a road map for future research, and searches for five research questions related to machine learning and deep learning algorithms used in anomaly detection systems.",
  "keyword": "machine learning"
 },
 {
  "No": 457,
  "judul": "Learning Quantum Drift-Diffusion Phenomenon by Physics-Constraint Machine Learning",
  "abstrak": "Recently, deep learning (DL) is widely used to detect physical phenomena and has obtained encouraging results. Several works have shown that it can learn quantum phenomenon. Subsequently, quantum machine learning (QML) has been paid more attention by academia and industry. Quantum drift-diffusion (QDD) is a commonplace physical phenomenon, which is a macroscopic description of electrons and holes in a semiconductor. They are commonly used to attain an understanding of the property of semiconductor devices in physics and engineering. We are motivated by the relaxation-time limit from the quantum-Navier-Stokes-Poisson system (QNSP) to the QDD equation and the existence of finite energy weak solutions to the QDD equation has been proved. Therefore, in this work, the quantum drift-diffusion learning neural network (QDDLNN) is proposed to investigate the quantum drift phenomena from limited observations. Furthermore, a piece of numerical evidence is found that the NNs can describe quantum transport phenomena by simulating the quantum confinement transport equation-quantum Navier-Stokes equation.",
  "keyword": "machine learning"
 },
 {
  "No": 458,
  "judul": "Non-Line-of-Sight Identification Based on Unsupervised Machine Learning in Ultra Wideband Systems",
  "abstrak": "Identification of line-of-sight (LOS) and non-line-of-sight (NLOS) propagation conditions is very useful in ultra wideband localization systems. In the identification, supervised machine learning is often used, but it requires exorbitant efforts to maintain and label the LOS and NLOS database. In this paper, we apply unsupervised machine learning approach called “expectation maximization for Gaussian mixture models” to classify LOS and NLOS components. The key advantage of applying unsupervised machine learning is that it does not require any rigorous and explicit labeling of the database at a certain location. The simulation results demonstrate that by using the proposed algorithm, LOS and NLOS signals can be classified with 86.50% correct rate, 12.70% false negative, and 0.8% false positive rate. We also compare the proposed algorithm with the existing cutting-edge supervised machine learning algorithms in terms of computational complexity and signals' classification performance.",
  "keyword": "machine learning"
 },
 {
  "No": 459,
  "judul": "A Literature Review of Using Machine Learning in Software Development Life Cycle Stages",
  "abstrak": "The software engineering community is rapidly adopting machine learning for transitioning modern-day software towards highly intelligent and self-learning systems. However, the software engineering community is still discovering new ways how machine learning can offer help for various software development life cycle stages. In this article, we present a study on the use of machine learning across various software development life cycle stages. The overall aim of this article is to investigate the relationship between software development life cycle stages, and machine learning tools, techniques, and types. We attempt a holistic investigation in part to answer the question of whether machine learning favors certain stages and\/or certain techniques.",
  "keyword": "machine learning"
 },
 {
  "No": 460,
  "judul": "Nonlinear Electromagnetic Inversion of Damaged Experimental Data by a Receiver Approximation Machine Learning Method",
  "abstrak": "In practical applications, electromagnetic measurements often contain damaged data due to malfunctioning receivers, which can severely influence the inversion performance. Thus, in this letter, a new receiver approximation machine learning (RAML) method is proposed to repair the data from the damaged receivers and the repaired data are the input to the dual-module nonlinear mapping module-image enhancing module machine learning scheme for the 2-D inverse scattering problem. The RAML method has been tested for the inversion of both measured transverse electric and transverse magnetic datasets provided by Institute Fresnel. Excellent inversion performance has been obtained when 10 out of 241 (4.15%) receivers are damaged. Furthermore, in an extreme situation, the RAML method can also have good inversion performance even though half of the receivers are damaged.",
  "keyword": "machine learning"
 },
 {
  "No": 461,
  "judul": "A Survey of Machine Learning in Pedestrian Localization Systems: Applications, Open Issues and Challenges",
  "abstrak": "With the popularization of machine learning (ML) techniques and the increased chipset's performance, the application of ML to pedestrian localization systems has received significant attention in the last years. Several survey papers have attempted to provide a state-of-the-art overview, but they usually limit their scope to a particular type of positioning system or technology. In addition, they are written from the point of view of ML techniques and their practice, not from the point of view of the localization system and the specific problems that ML techniques can help to solve. This article is intended to offer a comprehensive state-of-the-art survey of the ML techniques that have been adopted over the last ten years to improve the performance of pedestrian localization systems, addressing the applicability of ML techniques in this domain, along with the main localization strategies. It concludes by indicating the underlying open issues and challenges associated with the existing systems, and possible future directions in which ML techniques could improve the performance of pedestrian localization systems. Among other open issues, most previous authors have focused their attention on position estimation accuracy, which wastes the potential of ML techniques to improve other performance parameters (e.g., response time, computational complexity, robustness, scalability or energy efficiency). This study shows that there is a strong trend towards the application of supervised learning. Consequently, there are many potential research opportunities in the use of other learning types, such as unsupervised and reinforcement learning, to improve the performance of pedestrian localization systems.",
  "keyword": "machine learning"
 },
 {
  "No": 462,
  "judul": "Resonant Machine Learning Based on Complex Growth Transform Dynamical Systems",
  "abstrak": "Traditional energy-based learning models associate a single energy metric to each configuration of variables involved in the underlying optimization process. Such models associate the lowest energy state with the optimal configuration of variables under consideration and are thus inherently dissipative. In this article, we propose an energy-efficient learning framework that exploits structural and functional similarities between a machine-learning network and a general electrical network satisfying Tellegen's theorem. In contrast to the standard energy-based models, the proposed formulation associates two energy components, namely, active and reactive energy with the network. The formulation ensures that the network's active power is dissipated only during the process of learning, whereas the reactive power is maintained to be zero at all times. As a result, in steady state, the learned parameters are stored and self-sustained by electrical resonance determined by the network's nodal inductances and capacitances. Based on this approach, this article introduces three novel concepts: 1) a learning framework where the network's active-power dissipation is used as a regularization for a learning objective function that is subjected to zero total reactive-power constraint; 2) a dynamical system based on complex-domain, continuous-time growth transforms that optimizes the learning objective function and drives the network toward electrical resonance under steady-state operation; and 3) an annealing procedure that controls the tradeoff between active-power dissipation and the speed of convergence. As a representative example, we show how the proposed framework can be used for designing resonant support vector machines (SVMs), where the support vectors correspond to an LC network with self-sustained oscillations. We also show that this resonant network dissipates less active power compared with its non-resonant counterpart.",
  "keyword": "machine learning"
 },
 {
  "No": 463,
  "judul": "Machine Learning Method Applied in Readout System of Superheated Droplet Detector",
  "abstrak": "Direct readability is one advantage of superheated droplet detectors in neutron dosimetry. Utilizing such a distinct characteristic, an imaging readout system analyzes image of the detector for neutron dose readout. To improve the accuracy and precision of algorithms in the imaging readout system, machine learning algorithms were developed. Deep learning neural network and support vector machine algorithms are applied and compared with generally used Hough transform and curvature analysis methods. The machine learning methods showed a much higher accuracy and better precision in recognizing circular gas bubbles.",
  "keyword": "machine learning"
 },
 {
  "No": 464,
  "judul": "Understanding Depth of Reflective Writing in Workplace Learning Assessments Using Machine Learning Classification",
  "abstrak": "Self-reflection and reflective writing have been pivotal for developing a deep understanding of concepts and fostering professional competency in learners. The confluence of the importance of reflective practices within the educational curriculum and the increased proliferation of technology have resulted in numerous studies of how to use automated approaches to analyze broad themes of reflection exhibited by learners in higher educational settings. However, there is a dearth of research in the context of automated analysis of reflective writing demonstrated by professionals within workplace learning. Building on a four-level reflective content analysis model, this article evaluates the depth of reflection exhibited by learners within a professional development MOOC using an automated assessment classifier. Our results identify the varying association of linguistic features across these different levels of reflection. The proposed approach can effectively support the deployment at scale of the labor-intensive evaluation of reflective writing by highly trained professionals.",
  "keyword": "machine learning"
 },
 {
  "No": 465,
  "judul": "A Machine Learning Perspective on fNIRS Signal Quality Control Approaches",
  "abstrak": "Despite a rise in the use of functional Near Infra-Red Spectroscopy (fNIRS) to study neural systems, fNIRS signal processing is not standardized and is highly affected by empirical and manual procedures. At the beginning of any signal processing procedure, Signal Quality Control (SQC) is critical to prevent errors and unreliable results. In fNIRS analysis, SQC currently relies on applying empirical thresholds to handcrafted Signal Quality Indicators (SQIs). In this study, we use a dataset of fNIRS signals (N = 1,340) recorded from 67 subjects, and manually label the signal quality of a subset of segments (N = 548) to investigate the pitfalls of current practices while exploring the opportunities provided by Deep Learning approaches. We show that SQIs statistically discriminate signals with bad quality, but the identification by means of empirical thresholds lacks sensitivity. Alternatively to manual thresholding, conventional machine learning models based on the SQIs have been proven more accurate, with end-to-end approaches, based on Convolutional Neural Networks, capable of further improving the performance. The proposed approach, based on machine learning, represents a more objective SQC for fNIRS and moves towards the use of fully automated and standardized procedures.",
  "keyword": "machine learning"
 },
 {
  "No": 466,
  "judul": "Potentials of Machine Learning in Vacuum Electronic Devices Demonstrated by the Design of a Magnetron Injection Gun",
  "abstrak": "Great progress has been made on machine learning and its applications are expanding rapidly nowadays. Through the case study of optimizing a magnetron injection gun for gyrotron devices, the functions of machine learning were investigated by using two supervised learning algorithms, regression trees and artificial neural networks. They showed excellent performance in predicting the outputs, exploring the importance of the input parameters and the relationship with the output parameters. Machine learning can be a useful tool in the development of microwave vacuum electron devices.",
  "keyword": "machine learning"
 },
 {
  "No": 467,
  "judul": "Application of Machine Learning to Terahertz Spectroscopic Imaging of Reagents Hidden By Thick Shielding Materials",
  "abstrak": "We achieved high identification accuracy of reagents hidden by thick shielding materials, by combining injection-seeded terahertz (THz) wave parametric generator measurements and machine learning analysis. The analysis performance of three methods, support vector machine (SVM), k-nearest neighbor, and random forest, was compared in an attempt to identify the optimal approach. SVM proved to be the best model. Conventional systems could only identify reagents through premeasured shields; however, incorporation of machine learning allowed us to identify the reagents through shielding materials that had not been premeasured. Moreover, spectroscopic imaging of the reagents revealed the distribution pattern of the reagents, even through thick shielding materials that attenuated THz frequencies such that they were close to the noise level.",
  "keyword": "machine learning"
 },
 {
  "No": 468,
  "judul": "A Multiple Gradient Descent Design for Multi-Task Learning on Edge Computing: Multi-Objective Machine Learning Approach",
  "abstrak": "Multi-task learning technique is widely utilized in machine learning modeling where commonalities and differences across multiple tasks are exploited. However, multiple conflicting objectives often occur in multi-task learning. Conventionally, a common compromise is to minimize the weighted sum of multiple objectives which may be invalid if the objectives are competing. In this paper, a novel multi-objective machine learning approach is proposed to solve this challenging issue, which reformulates the multi-task learning as multi-objective optimization. To address the issues contributed by existing multi-objective optimization algorithms, a multi-gradient descent algorithm is introduced for the multi-objective machine learning problem by which an innovative gradient-based optimization is leveraged to converge to an optimal solution of the Pareto set. Moreover, the gradient surgery for the multi-gradient descent algorithm is proposed to obtain a stable Pareto optimal solution. As most of the edge computing devices are computational resource-constrained, the proposed method is implemented for optimizing the edge device's memory, computation and communication demands. The proposed method is applied to the multiple license plate recognition problem. The experimental results show that the proposed method outperforms state-of-the-art learning methods and can successfully find solutions that balance multiple objectives of the learning task over different datasets.",
  "keyword": "machine learning"
 },
 {
  "No": 469,
  "judul": "Machine Learning Algorithms in Bipedal Robot Control",
  "abstrak": "Over the past decades, machine learning techniques, such as supervised learning, reinforcement learning, and unsupervised learning, have been increasingly used in the control engineering community. Various learning algorithms have been developed to achieve autonomous operation and intelligent decision making for many complex and challenging control problems. One of such problems is bipedal walking robot control. Although still in their early stages, learning techniques have demonstrated promising potential to build adaptive control systems for bipedal robots. This paper gives a review of recent advances on the state-of-the-art learning algorithms and their applications to bipedal robot control. The effects and limitations of different learning techniques are discussed through a representative selection of examples from the literature. Guidelines for future research on learning control of bipedal robots are provided in the end.",
  "keyword": "machine learning"
 },
 {
  "No": 470,
  "judul": "Electromechanical Wave Imaging With Machine Learning for Automated Isochrone Generation",
  "abstrak": "Standard Electromechanical Wave Imaging isochrone generation relies on manual selection of zero-crossing (ZC) locations on incremental strain curves for a number of pixels in the segmented myocardium for each echocardiographic view and patient. When considering large populations, this becomes a time-consuming process, that can be limited by inter-observer variability and operator bias. In this study, we developed and optimized an automated ZC selection algorithm, towards a faster more robust isochrone generation approach. The algorithm either relies on heuristic-based baselines or machine learning classifiers. Manually generated isochrones, previously validated against 3D intracardiac mapping, were considered as ground truth during training and performance evaluation steps. The machine learning models applied herein for the first time were: i) logistic regression; ii) support vector machine (SVM); and iii) Random Forest. The SVM and Random Forest classifiers successfully identified accessory pathways in Wolff-Parkinson-White patients, characterized sinus rhythm in humans, and localized the pacing electrode location in left ventricular paced canines on the resulting isochrones. Nevertheless, the best performing classifier was proven to be Random Forest with a precision rising from 89.5% to 97%, obtained with the voting approach that sets a probability threshold upon ZC candidate selection. Furthermore, the predictivity was not dependent on the type of testing dataset it was applied to, contrary to SVM that exhibited a 5% drop in precision on the canine testing dataset. Finally, these findings indicate that a machine learning approach can reduce user variability and considerably decrease the durations required for isochrone generation, while preserving accurate activation patterns.",
  "keyword": "machine learning"
 },
 {
  "No": 471,
  "judul": "Online Learning Control Using Adaptive Critic Designs With Sparse Kernel Machines",
  "abstrak": "In the past decade, adaptive critic designs (ACDs), including heuristic dynamic programming (HDP), dual heuristic programming (DHP), and their action-dependent ones, have been widely studied to realize online learning control of dynamical systems. However, because neural networks with manually designed features are commonly used to deal with continuous state and action spaces, the generalization capability and learning efficiency of previous ACDs still need to be improved. In this paper, a novel framework of ACDs with sparse kernel machines is presented by integrating kernel methods into the critic of ACDs. To improve the generalization capability as well as the computational efficiency of kernel machines, a sparsification method based on the approximately linear dependence analysis is used. Using the sparse kernel machines, two kernel-based ACD algorithms, that is, kernel HDP (KHDP) and kernel DHP (KDHP), are proposed and their performance is analyzed both theoretically and empirically. Because of the representation learning and generalization capability of sparse kernel machines, KHDP and KDHP can obtain much better performance than previous HDP and DHP with manually designed neural networks. Simulation and experimental results of two nonlinear control problems, that is, a continuous-action inverted pendulum problem and a ball and plate control problem, demonstrate the effectiveness of the proposed kernel ACD methods.",
  "keyword": "machine learning"
 },
 {
  "No": 472,
  "judul": "Cell Mechanics Based Computational Classification of Red Blood Cells Via Machine Intelligence Applied to Morpho-Rheological Markers",
  "abstrak": "Despite fluorescent cell-labelling being widely employed in biomedical studies, some of its drawbacks are inevitable, with unsuitable fluorescent probes or probes inducing a functional change being the main limitations. Consequently, the demand for and development of label-free methodologies to classify cells is strong and its impact on precision medicine is relevant. Towards this end, high-throughput techniques for cell mechanical phenotyping have been proposed to get a multidimensional biophysical characterization of single cells. With this motivation, our goal here is to investigate the extent to which an unsupervised machine learning methodology, which is applied exclusively on morpho-rheological markers obtained by real-time deformability and fluorescence cytometry (RT-FDC), can address the difficult task of providing label-free discrimination of reticulocytes from mature red blood cells. We focused on this problem, since the characterization of reticulocytes (their percentage and cellular features) in the blood is vital in multiple human disease conditions, especially bone-marrow disorders such as anemia and leukemia. Our approach reports promising label-free results in the classification of reticulocytes from mature red blood cells, and it represents a step forward in the development of high-throughput morpho-rheological-based methodologies for the computational categorization of single cells. Besides, our methodology can be an alternative but also a complementary method to integrate with existing cell-labelling techniques.",
  "keyword": "machine learning"
 },
 {
  "No": 473,
  "judul": "Analyzing Effective Factors of Online Learning Performance by Interpreting Machine Learning Models",
  "abstrak": "Analyzing the effective factors influencing online learning performance is a research topic that has garnered significant attention. Traditional approaches, such as multiple regression and structural equation models, tend to assume linearity, while non-linear machine learning models lack interpretability. To address this gap, we propose a framework that interprets machine learning models to analyze the effective factors of online learning performance. By applying this framework to four benchmark datasets of online learning, we examine the differential impact of various factors on performance, explore the interactions among these factors, and identify the key factors for representative learners. Our findings indicate that: 1) non-linear machine learning models, particularly Decision Regression, offer better representation of the non-linear relationship between effective factors and online learning performance compared to classical multivariate regression; 2) the factor of online learning behavior exerts a greater influence on performance than demographic features, academic background, or online curriculum design; 3) online learning behavior features exhibit additional interaction effects on performance; and 4) learners with medium performance are influenced by diverse effective factors, with active participation in online learning activities emerging as the most crucial means to improve performance. Interpreting machine learning models presents an innovative approach for analyzing the effective factors of online learning performance, which can be extended to other factor analysis studies. The results of this research provide valuable insights for optimizing machine learning models in predicting online learning performance and enhancing learner outcomes.",
  "keyword": "machine learning"
 },
 {
  "No": 474,
  "judul": "Can Wearable Devices and Machine Learning Techniques Be Used for Recognizing and Segmenting Modified Physical Performance Test Items?",
  "abstrak": "Assessment of physical performance is essential to predict the frailty level of older adults. The modified Physical Performance Test (mPPT) clinically assesses the performance of nine activities: standing balance , chair rising up & down , lifting a book , putting on and taking off a jacket , picking up a coin , turning 360° , walking , going upstairs , and going downstairs . The activity performing duration is the primary evaluation standard. In this study, wearable devices are leveraged to recognize and predict mPPT items’ duration automatically. This potentially allows frequent follow up of physical performance, and facilitates more appropriate interventions. Five devices, including accelerometers and gyroscopes, were attached to the waist, wrists and ankles of eight younger adults. The system was experimented within three aspects: machine learning models, sensor placement, and sampling frequencies, to which the non-causal six-stages temporal convolutional network using 6.25 Hz signals from the left wrist and right ankle obtained the optimal performance. The duration prediction error ranged from 0.63±0.29 s ( turning 360° ) to 8.21±16.41 s ( walking ). The results suggest the potential for the proposed system in the automatic recognition and segmentation of mPPT items. Future work includes improving the recognition performance of lifting a book and implementing the frailty score prediction.",
  "keyword": "machine learning"
 },
 {
  "No": 475,
  "judul": "A Machine Learning Based Framework for Identifying Influential Nodes in Complex Networks",
  "abstrak": "In complex networks, identifying influential nodes is of great importance for its wide applications. Traditional centrality methods are usually directly based on topological structures of networks, and different centrality methods consider different structural characteristics related to the functional importance. However, in many scenarios, it always exists a complex and nonlinear relationship between the functional importance of a node and its various features including local location, global location, etc., which is hard to be described by one centrality. In order to solve this problem, this paper proposes a framework based on machine learning to measure the importance of nodes in the propagation scenario. This framework first constructs the feature vector of each node based on the existing centrality methods which can reflect nodes' different topological structures and the infection rate which is an important factor in the propagation scenarios, then labels each node based on the real propagation ability obtained from simulated propagation experiments based on SIR model, last uses seven machine learning algorithms to learn the complex relationship between the real propagation ability of each node and its various structural features. The experimental results in real-world networks show that the classification accuracy of the model based on machine learning is generally higher than that of the traditional centrality methods based on one certain topology.",
  "keyword": "machine learning"
 },
 {
  "No": 476,
  "judul": "A Machine Learning-Oriented Survey on Tiny Machine Learning",
  "abstrak": "The emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly, we will examine the three different workflows for implementing a TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly, we propose a taxonomy that covers the learning panorama under the TinyML lens, examining in detail the different families of model optimization and design, as well as the state-of-the-art learning techniques. Thirdly, this survey will present the distinct features of hardware devices and software tools that represent the current state-of-the-art for TinyML intelligent edge applications. Finally, we discuss the challenges and future directions.",
  "keyword": "machine learning"
 },
 {
  "No": 477,
  "judul": "Multiple-Swarm Ensembles: Improving the Predictive Power and Robustness of Predictive Models and Its Use in Computational Biology",
  "abstrak": "Machine learning is an integral part of computational biology, and has already shown its use in various applications, such as prognostic tests. In the last few years in the non-biological machine learning community, ensembling techniques have shown their power in datamining competitions such as the Netflix challenge; however, such methods have not found wide use in computational biology. In this work, we endeavor to show how ensembling techniques can be applied to practical problems, including problems in the field of bioinformatics, and how they often outperform other machine learning techniques in both predictive power and robustness. Furthermore, we develop a methodology of ensembling, Multi-Swarm Ensemble (MSWE) by using multiple particle swarm optimizations and demonstrate its ability to further enhance the performance of ensembles.",
  "keyword": "machine learning"
 },
 {
  "No": 478,
  "judul": "Machine Learning and Deep Learning Techniques for Distributed Denial of Service Anomaly Detection in Software Defined Networks—Current Research Solutions",
  "abstrak": "This state-of-the-art review comprehensively examines the landscape of Distributed Denial of Service (DDoS) anomaly detection in Software Defined Networks (SDNs) through the lens of advanced Machine Learning (ML) and Deep Learning (DL) techniques. The application domain of this work is focused on addressing the inherent security vulnerabilities of SDN environments and developing an automated system for detecting and mitigating network attacks. The problem focused on in this review is the need for effective defensive mechanisms and detection methodologies to address these vulnerabilities. Conventional network measurement methodologies are limited in the context of SDNs, and the proposed ML and DL techniques aim to overcome these limitations by providing more accurate and efficient detection and mitigation of DDoS attacks. The objective of this work is to provide a comprehensive review of related works in the field of SDN anomaly detection recent advances, categorized into two groups via ML and DL techniques. The proposed systems utilize a variety of techniques, including Supervised Learning (SL), Unsupervised Learning (UL) Ensemble Learning (EL) and DL solutions, to process IP flows, profile network traffic, and identify attacks. The output comprises the mitigation policies learned by ML\/DL techniques, and the proposed systems act as sophisticated gatekeepers, applying automated mitigation policies to curtail the extent of damage resulting from these attacks. The results obtained from the evaluation metrics, including accuracy, precision, and recall, confirm the marked effectiveness of the proposed systems in detecting and mitigating various types of attacks, including Distributed Denial of Service (DDoS) attacks. The proposed systems’ foundational contributions are manifest in their efficacy for both DDoS attack detection and defense within the SDN environment. However, the review acknowledges certain inherent limitations and the pressing need for further validation within real-world scenarios to assess the proposed methods’ practicality and effectiveness. In summary, this systematic review offers valuable perspectives on the present status of Distributed Denial-of-Service detection in Software-Defined Networks employing Machine Learning and Deep Learning methodologies, highlighting the strengths and limitations of various proposed systems and identifying areas for future research and development.",
  "keyword": "machine learning"
 },
 {
  "No": 479,
  "judul": "Board-Level Functional Fault Diagnosis Using Multikernel Support Vector Machines and Incremental Learning",
  "abstrak": "Advanced machine learning techniques offer an unprecedented opportunity to increase the accuracy of board-level functional fault diagnosis and reduce product cost through successful repair. Ambiguous or incorrect diagnosis results lead to long debug times and even wrong repair actions, which significantly increase repair cost. We propose a smart diagnosis method based on multikernel support vector machines (MK-SVMs) and incremental learning. The MK-SVM method leverages a linear combination of single kernels to achieve accurate faulty-component classification based on the errors observed. The MK-SVMs thus generated can also be updated based on incremental learning, which allows the diagnosis system to quickly adapt to new error observations and provide even more accurate fault diagnosis. Two complex boards from industry, currently in volume production, are used to validate the proposed diagnosis approach in terms of diagnosis accuracy (success rate) and quantifiable improvements over previously proposed machine-learning methods based on several single-kernel SVMs and artificial neural networks.",
  "keyword": "machine learning"
 },
 {
  "No": 480,
  "judul": "A Comprehensive Review of Crop Yield Prediction Using Machine Learning Approaches With Special Emphasis on Palm Oil Yield Prediction",
  "abstrak": "An early and reliable estimation of crop yield is essential in quantitative and financial evaluation at the field level for determining strategic plans in agricultural commodities for import-export policies and doubling farmer's incomes. Crop yield predictions are carried out to estimate higher crop yield through the use of machine learning algorithms which are one of the challenging issues in the agricultural sector. Due to this developing significance of crop yield prediction, this article provides an exhaustive review on the use of machine learning algorithms to predict crop yield with special emphasis on palm oil yield prediction. Initially, the current status of palm oil yield around the world is presented, along with a brief discussion on the overview of widely used features and prediction algorithms. Then, the critical evaluation of the state-of-the-art machine learning-based crop yield prediction, machine learning application in the palm oil industry and comparative analysis of related studies are presented. Consequently, a detailed study of the advantages and difficulties related to machine learning-based crop yield prediction and proper identification of current and future challenges to the agricultural industry is presented. The potential solutions are additionally prescribed in order to alleviate existing problems in crop yield prediction. Since one of the major objectives of this study is to explore the future perspectives of machine learning-based palm oil yield prediction, the areas including application of remote sensing, plant's growth and disease recognition, mapping and tree counting, optimum features and algorithms have been broadly discussed. Finally, a prospective architecture of machine learning-based palm oil yield prediction has been proposed based on the critical evaluation of existing related studies. This technology will fulfill its promise by performing new research challenges in the analysis of crop yield prediction and the development of an extremely effective model for the prediction of palm oil yields with the most minimal computational difficulty.",
  "keyword": "machine learning"
 },
 {
  "No": 481,
  "judul": "Dynamic Autoselection and Autotuning of Machine Learning Models for Cloud Network Analytics",
  "abstrak": "Cloud network monitoring data is dynamic and distributed. Signals to monitor the cloud can appear, disappear or change their importance and clarity over time. Machine learning (ML) models tuned to a given data set can therefore quickly become inadequate. A model might be highly accurate at one point in time but may lose its accuracy at a later time due to changes in input data and their features. Distributed learning with dynamic model selection is therefore often required. Under such selection, poorly performing models (although aggressively tuned for the prior data) are retired or put on standby while new or standby models are brought in. The well-known method of Ensemble ML (EML) may potentially be applied to improve the overall accuracy of a family of ML models. Unfortunately, EML has several disadvantages, including the need for continuous training, excessive computational resources, requirement for large training datasets, high risks of overfitting, and a time-consuming model-building process. In this paper, we propose a novel cloud methodology for automatic ML model selection and tuning that automates model building and selection and is competitive with existing methods. We use unsupervised learning to better explore the data space before the generation of targeted supervised learning models in an automated fashion. In particular, we create a Cloud DevOps architecture for autotuning and selection based on container orchestration and messaging between containers, and take advantage of a new autoscaling method to dynamically create and evaluate instantiations of ML algorithms. The proposed methodology and tool are demonstrated on cloud network security datasets.",
  "keyword": "machine learning"
 },
 {
  "No": 482,
  "judul": "Fusing Sell-Side Analyst Bidirectional Forecasts Using Machine Learning",
  "abstrak": "Sell-side analysts’ recommendations are primarily targeted at institutional investors mandated to invest across many companies within client-mandated equity benchmarks, such as the FTSE\/JSE All-Share index. Given the numerous sell-side recommendations for a single stock, making unbiased investment decisions is not often straightforward for portfolio managers. This study explores the use of historical sell-side recommendations to create an unbiased fusion of analyst forecasts such that bidirectional accuracy is optimised using random forest, extreme gradient boosting, deep neural networks, and logistic regression. We introduced 12-month rolling features generated from standard sell-side recommendations, such as analyst coverage, point and directional accuracy, while avoiding forward-looking biases. We introduce a novel “AI analyst” by fusing forecast features from numerous analysts using machine learning algorithms. We observed the added benefits of using these features from more than one analyst by systematically generating unbiased and incrementally better prediction accuracy from publicly available sell-side recommendations, with the Random forest algorithm showing the highest relative performance. In highly volatile sectors, like resources, the machine learning algorithms perform better than in low volatility sectors, suggesting the importance of rolling features in bi-directional prediction in the presence of high volatility. Using feature importance, we observe the incremental contribution of rolling features, showing the relationships between analyst coverage, volatility, and bidirectional forecast accuracy. Furthermore, parameters from logistic regression identify volatility features and initial and target price as some of the essential features when modelling analysts’ directional predictions.",
  "keyword": "machine learning"
 },
 {
  "No": 483,
  "judul": "Machine Learning Approach for Prediction of Hematic Parameters in Hemodialysis Patients",
  "abstrak": "Objective:\n This paper shows the application of machine learning techniques to predict hematic parameters using blood visible spectra during ex-vivo treatments. \nMethods:\n A spectroscopic setup was prepared for acquisition of blood absorbance spectrum and tested in an operational environment. This setup is non invasive and can be applied during dialysis sessions. A support vector machine and an artificial neural network, trained with a dataset of spectra, have been implemented for the prediction of hematocrit and oxygen saturation. \nResults & Conclusion:\n Results of different machine learning algorithms are compared, showing that support vector machine is the best technique for the prediction of hematocrit and oxygen saturation.",
  "keyword": "machine learning"
 },
 {
  "No": 484,
  "judul": "A Novel Machine Learning Approach for Android Malware Detection Based on the Co-Existence of Features",
  "abstrak": "This paper proposes a machine learning model based on the co-existence of static features for Android malware detection. The proposed model assumes that Android malware requests an abnormal set of co-existed permissions and APIs in comparing to those requested by benign applications. To prove this assumption, the paper created a new dataset of co-existed permissions and API calls at different levels of combinations, which are the second level, the third level, the fourth level and the fifth level. The extracted datasets of co-existed features at different levels were applied on permissions only, APIs only, permissions and APIs, and APIs and APIs frequencies. To extract the most relevant co-existed features, the frequent pattern growth (FP-growth) algorithm, which is an association rule mining technique, was used. The new datasets were extracted using Android APK samples from the Drebin, Malgenome and MalDroid2020 datasets. To evaluate the proposed model, several conventional machine learning algorithms were used. The results show that the model can successfully classify Android malware with a high accuracy using machine learning algorithms and the co-existence of features. Moreover, the results show that the achieved classification accuracy depends on the classifier and the type of co-existed features. The maximum accuracy, which is 98%, was achieved using the Random Forest algorithm and the co-existence of permissions features at the second combination level. Furthermore, the results show that the proposed approach outperforms the state-of-the-art model. Using Malgenome dataset, the proposed approach achieved an accuracy of about 98%, while the state-of-the-art achieved an accuracy of about 87%. In addition, the experiments show that using the Drebin dataset, the proposed approach achieved an accuracy of about 95%, while the state-of-the-art achieved an accuracy of about 93%.",
  "keyword": "machine learning"
 },
 {
  "No": 485,
  "judul": "Detection of Cardiovascular Diseases in ECG Images Using Machine Learning and Deep Learning Methods",
  "abstrak": "Artificial intelligence plays an important role in improving the quality of life. In particular, early detection of diseases can help save lives. In this work, the proposed new lightweight CNN architecture has improved the accuracy rate of cardiovascular disease classification to 98.23% compared with the existing state-of-the-art methods, using the dataset of ECG images of cardiac patients, and can be performed on a single CPU, overcoming the limitation of computational power. In addition, the classification accuracy has significantly improved after applying the proposed method as a feature extraction tool for traditional machine learning algorithms. For example, an accuracy of 99.79% has been achieved using the Naïve Bayes algorithm. Thus, this method could be integrated into the IoT ecosystem in healthcare. This will encourage other AI researchers to explore other methods for cardiovascular disease detection.",
  "keyword": "machine learning"
 },
 {
  "No": 486,
  "judul": "Tomato Quality Classification Based on Transfer Learning Feature Extraction and Machine Learning Algorithm Classifiers",
  "abstrak": "The demand for high-quality tomatoes to meet consumer and market standards, combined with large-scale production, has necessitated the development of an inline quality grading. Since manual grading is time-consuming, costly, and requires a substantial amount of labor. This study introduces a novel approach for tomato quality sorting and grading. The method leverages pre-trained convolutional neural networks (CNNs) for feature extraction and traditional machine-learning algorithms for classification (hybrid model). The single-board computer NVIDIA Jetson TX1 was used to create a tomato image dataset. Image preprocessing and fine-tuning techniques were applied to enable deep layers to learn and concentrate on complex and significant features. The extracted features were then classified using traditional machine learning algorithms namely: support vector machines (SVM), random forest (RF), and k-nearest neighbors (KNN) classifiers. Among the proposed hybrid models, the CNN-SVM method has outperformed other hybrid approaches, attaining an accuracy of 97.50% in the binary classification of tomatoes as healthy or rejected and 96.67% in the multiclass classification of them as ripe, unripe, or rejected when Inceptionv3 was used as feature extractor. Once another dataset (public dataset) was used, the proposed hybrid model CNN-SVM achieved an accuracy of 97.54% in categorizing tomatoes as ripe, unripe, old, or damaged outperforming other hybrid models when Inceptionv3 was used as a feature extractor. The performance metrics accuracy, recall, precision, specificity, and F1-score of the best-performing proposed hybrid model were evaluated.",
  "keyword": "machine learning"
 },
 {
  "No": 487,
  "judul": "Transfer Learning for Visual Categorization: A Survey",
  "abstrak": "Regular machine learning and data mining techniques study the training data for future inferences under a major assumption that the future data are within the same feature space or have the same distribution as the training data. However, due to the limited availability of human labeled training data, training data that stay in the same feature space or have the same distribution as the future data cannot be guaranteed to be sufficient enough to avoid the over-fitting problem. In real-world applications, apart from data in the target domain, related data in a different domain can also be included to expand the availability of our prior knowledge about the target future data. Transfer learning addresses such cross-domain learning problems by extracting useful information from data in a related domain and transferring them for being used in target tasks. In recent years, with transfer learning being applied to visual categorization, some typical problems, e.g., view divergence in action recognition tasks and concept drifting in image classification tasks, can be efficiently solved. In this paper, we survey state-of-the-art transfer learning algorithms in visual categorization applications, such as object recognition, image classification, and human action recognition.",
  "keyword": "machine learning"
 },
 {
  "No": 488,
  "judul": "Unsupervised Machine Intelligence for Automation of Multi-Dimensional Modulation",
  "abstrak": "In this letter, we propose a new unsupervised machine learning technique for a multi-dimensional modulator that can autonomously learn key exploitable features from significant variations of multi-dimensional wireless propagation parameters, followed by a real-time prediction of the best multi-dimensional modulation mode to be used for the next resilient transmission. The proposed method aims to embrace the potential of the unsupervised K-means clustering into the physical layer of non-coherent multi-dimensional transmission. Simulation results show that the proposed scheme can outperform the benchmarks at a cost of simple offline training.",
  "keyword": "machine learning"
 },
 {
  "No": 489,
  "judul": "A Systematic Review of Machine Learning Techniques for GNSS Use Cases",
  "abstrak": "In terms of the availability and accuracy of positioning, navigation, and timing (PNT), the traditional Global Navigation Satellite System (GNSS) algorithms and models perform well under good signal conditions. In order to improve their robustness and performance in less than optimal signal environments, many researchers have proposed machine learning (ML) based GNSS models (ML models) as early as the 1990s. However, no study has been done in a systematic way to analyze the extent of the research on the utilization of ML models in GNSS and their performance. In this study, we perform a systematic review of studies from 2000 to 2021 in the literature that utilizes machine learning techniques in GNSS use cases. We assess the performance of the machine learning techniques in the existing literature on their application to GNSS. Furthermore, the strengths and weaknesses of machine learning techniques are summarized. In this paper, we have identified 213 selected studies and ten categories of machine learning techniques. The results prove the acceptable performance of machine learning techniques in several GNSS use cases. In most cases, the models using the machine learning techniques in these GNSS use cases outperform the traditional GNSS models. ML models are promising in their utilization in GNSS. However, the application of ML models in the industry is still limited. More effort and incentives are needed to facilitate the utilization of ML models in the PNT context. Therefore, based on the findings of this review, we provide recommendations for researchers and guidelines for practitioners.",
  "keyword": "machine learning"
 },
 {
  "No": 490,
  "judul": "Machine Learning to Identify Psychomotor Behaviors of Delirium for Patients in Long-Term Care Facility",
  "abstrak": "This study aimed to develop accurate and explainable machine learning models for three psychomotor behaviors of delirium for hospitalized adult patients. A prospective pilot study was conducted with 33 participants admitted to a long-term care facility between August 10 and 25, 2020. During the pilot study, we collected 560 cases that included 33 clinical variables and the survey items from the short confusion assessment method (S-CAM), and developed a mobile-based application. Multiple machine learning algorithms, including four rule-mining algorithms (C4.5, CBA, MCAR, and LEM2) and four other statistical learning algorithms (LR, ANNs, SVMs with three kernel functions, and random forest), were validated by paired Wilcoxon signed-rank tests on both macro-averaged F1 and weighted average F1-measures during the 10-times stratified 2-fold cross-validation. The LEM2 algorithm achieved the best prediction performance (macro-averaged F1-measure of 49.35%; weighted average F1-measure of 96.55%), correctly identifying adult patients at delirium risk. In the pairwise comparison between predictive powers observed from independent models, the LEM2 model showed a medium or large effect size between 0.4925 and 0.8766 when compared with LR, ANN, SVM with RBF, and MCAR models. We have confirmed that acute consciousness in S-CAM assessment is closely associated with different predictors for screening three psychomotor behaviors of delirium: 1) education level, dementia type or its level, sleep disorder, dehydration, and infection in mixed-type delirium; 2) gender, education level, dementia type, dehydration, bedsores, and foley catheter in hyperactive delirium; and 3) pain, sleep disorder, and haloperidol use in hypoactive delirium.",
  "keyword": "machine learning"
 },
 {
  "No": 491,
  "judul": "Trace Quotient with Sparsity Priors for Learning Low Dimensional Image Representations",
  "abstrak": "This work studies the problem of learning appropriate low dimensional image representations. We propose a generic algorithmic framework, which leverages two classic representation learning paradigms, i.e., sparse representation and the trace quotient criterion, to disentangle underlying factors of variation in high dimensional images. Specifically, we aim to learn simple representations of low dimensional, discriminant factors by applying the trace quotient criterion to well-engineered sparse representations. We construct a unified cost function, coined as the SPARse LOW dimensional representation (SparLow) function, for jointly learning both a sparsifying dictionary and a dimensionality reduction transformation. The SparLow function is widely applicable for developing various algorithms in three classic machine learning scenarios, namely, unsupervised, supervised, and semi-supervised learning. In order to develop efficient joint learning algorithms for maximizing the SparLow function, we deploy a framework of sparse coding with appropriate convex priors to ensure the sparse representations to be locally differentiable. Moreover, we develop an efficient geometric conjugate gradient algorithm to maximize the SparLow function on its underlying Riemannian manifold. Performance of the proposed SparLow algorithmic framework is investigated on several image processing tasks, such as 3D data visualization, face\/digit recognition, and object\/scene categorization.",
  "keyword": "machine learning"
 },
 {
  "No": 492,
  "judul": "Machine Learning in Network Anomaly Detection: A Survey",
  "abstrak": "Anomalies could be the threats to the network that have ever\/never happened. To protect networks against malicious access is always challenging even though it has been studied for a long time. Due to the evolution of network in both new technologies and fast growth of connected devices, network attacks are getting versatile as well. Comparing to the traditional detection approaches, machine learning is a novel and flexible method to detect intrusions in the network, it is applicable to any network structure. In this paper, we introduce the challenges of anomaly detection in the traditional network, as well as in the next generation network, and review the implementation of machine learning in the anomaly detection under different network contexts. The procedure of each machine learning category is explained, as well as the methodologies and advantages are presented. The comparison of using different machine learning models is also summarised.",
  "keyword": "machine learning"
 },
 {
  "No": 493,
  "judul": "Detecting Cryptography Misuses With Machine Learning: Graph Embeddings, Transfer Learning and Data Augmentation in Source Code Related Tasks",
  "abstrak": "Cryptography is a ubiquitous tool in secure software development in order to guarantee security requirements in general. However, software developers have scarce knowledge about cryptography and rely on limited support tools that cannot properly detect bad uses of cryptography, thus generating vulnerabilities in software. In this work, we extend the scarcely use of machine learning to detect cryptography misuse in source code by using a state of the art deep learning model (i.e., \ncode2vec\n) through transfer learning to generate features that feed machine learning models. In addition, we compare this approach to previous ones in different types of binary models. Also, we adapt code obfuscation to serve as data augmentation in machine learning source code related tasks. Finally, we show that through transfer learning \ncode2vec\n can be a competitive feature generator for cryptography misuse detection and simple code obfuscation can be used to generate data to enhance machine learning models training in source code related tasks.",
  "keyword": "machine learning"
 },
 {
  "No": 494,
  "judul": "A System-Driven Taxonomy of Attacks and Defenses in Adversarial Machine Learning",
  "abstrak": "Machine Learning (ML) algorithms, specifically supervised learning, are widely used in modern real-world applications, which utilize Computational Intelligence (CI) as their core technology, such as autonomous vehicles, assistive robots, and biometric systems. Attacks that cause misclassifications or mispredictions can lead to erroneous decisions resulting in unreliable operations. Designing robust ML with the ability to provide reliable results in the presence of such attacks has become a top priority in the field of adversarial machine learning. An essential characteristic for rapid development of robust ML is an arms race between attack and defense strategists. However, an important prerequisite for the arms race is access to a well-defined system model so that experiments can be repeated by independent researchers. This article proposes a fine-grained system-driven taxonomy to specify ML applications and adversarial system models in an unambiguous manner such that independent researchers can replicate experiments and escalate the arms race to develop more evolved and robust ML applications. The article provides taxonomies for: 1) the dataset, 2) the ML architecture, 3) the adversary's knowledge, capability, and goal, 4) adversary's strategy, and 5) the defense response. In addition, the relationships among these models and taxonomies are analyzed by proposing an adversarial machine learning cycle. The provided models and taxonomies are merged to form a comprehensive system-driven taxonomy, which represents the arms race between the ML applications and adversaries in recent years. The taxonomies encode best practices in the field and help evaluate and compare the contributions of research works and reveals gaps in the field.",
  "keyword": "machine learning"
 },
 {
  "No": 495,
  "judul": "Hybrid Machine Learning Models for Predicting Types of Human T-cell Lymphotropic Virus",
  "abstrak": "Life threatening diseases like adult T-cell leukemia, neurodegenerative diseases, and demyelinating diseases such as HTLV-1 based myelopathy\/tropical spastic paraparesis (HAM\/TSP), hypocalcaemia, and bone lesions are caused by a group of human retrovirus known as Human T-cell Lymphotropic virus (HTLV). Out of the four different types of HTLVs, HTLV-1 is most prominent in scourging over 20 million people around the world and still not much effort has been made in understanding the epidemiology and controlling the prevalence of this virus. This condition further worsens when most of the infected cases remain asymptomatic throughout their lifetime due to the limited diagnostic methods; that are most of the times unavailable for timely detection of infected individuals. Moreover, at present, there is no licensed vaccination for HTLV-1 infection. Therefore, there is a need to develop the faster and efficient diagnostic method for the detection of HTLV-1. Influenced from the outcomes of the machine learning techniques in the field of bio-informatics, this is the first study in which 64 hybrid machine learning techniques have been proposed for the prediction of different type of HTLVs (HTLV-1, HTLV-2, and HTLV-3). The hybrid techniques are built by permutation and combination of four classification methods, four feature weighting, and four feature selection techniques. The proposed hybrid models when evaluated on the basis of various model evaluation parameters are found to be capable of efficiently predicting the type of HTLVs. The best hybrid model has been identified by having accuracy, an AUROC value, and F1 score of 99.85 percent, 0.99, and 0.99, respectively. This kind of the system can assist the current diagnostic system for the detection of HTLV-1 as after the molecular diagnostics of HTLV by various screening tests like enzyme-linked immunoassay or particle agglutination assays there is always a need of confirmatory tests like western blotting, immuno-fluorescence assay, or radio-immuno-precipitation assay for distinguishing HTLV-1 from HTLV-2. These confirmatory tests are indeed very complex analytical techniques involving various steps. The proposed hybrid techniques can be used to support and verify the results of confirmatory test from the protein mixture. Furthermore, better insights about the virus can be obtained by exploring the physicochemical properties of the protein sequences of HTLVs.",
  "keyword": "machine learning"
 },
 {
  "No": 496,
  "judul": "Exploring Molecular Descriptors and Fingerprints to Predict mTOR Kinase Inhibitors using Machine Learning Techniques",
  "abstrak": "Mammalian Target of Rapamycin (mTOR) is a Ser\/Thr protein kinase, and its role is integral to the autophagy pathway in cancer. Targeting mTOR for therapeutic interventions in cancer through autophagy pathway is challenging due to the dual roles of autophagy in tumor progression. The architecture of mTOR reveals two complexes – mTORC1 and mTORC2, each having multiple protein subunits. mTOR kinase inhibitors target the structurally and functionally similar catalytic subunits of both mTORC1 and mTORC2. In this paper, we have explored two different categories of molecular features – descriptors and fingerprints for developing predictive models using machine learning techniques. Random Forest variable importance measures and autoencoders are used to identify molecular descriptors and fingerprints, respectively. We have built various predictive models using identified features and their combination for predicting mTOR kinase inhibitors. Finally, the best model based on the Mathew correlation co-efficient value over the validation dataset is selected for screening kinase SARfari bioactivity dataset. In this study, we have identified twenty best performing descriptors for predicting mTOR kinase inhibitors. To the best of our knowledge, it is the first study on integrating traditional machine learning and deep learning-based approaches for feature extraction to predict mTOR kinase inhibitors.",
  "keyword": "machine learning"
 },
 {
  "No": 497,
  "judul": "Personalized Adaptive Learning Technologies Based on Machine Learning Techniques to Identify Learning Styles: A Systematic Literature Review",
  "abstrak": "Artificial intelligence (AI) approaches have been used in personalised adaptive education systems to overcome the limitations of statically determined learning styles (LSs). These approaches utilise algorithms from machine learning (ML) to tackle the challenge of personalising e-learning by mapping students’ behavioural attributes to a particular LS automatically and dynamically to optimise the individual learning process. Motivated by the many influential studies in this field and the current developments in ML and AI, a comprehensive systematic literature review was conducted from 2015 to 2022. Influential scientific literature was analysed to identify the emerging trends and gaps in the literature in terms of LS models and possible ML techniques employed for personalised adaptive learning platforms. The outcomes of this paper include a review and analysis of the current trends of this emerging field in terms of the applications and developments in using ML approaches to implement more intelligent and adaptive e-learning environments to detect learners’ LSs automatically for enhancing learning. In addition, the following issues were also investigated: the platforms that stimulated research; identifying LS models utilised in e-learning; the evaluation methods used; and the learning supports provided. The results indicated an increasing interest in using artificial neural network approaches to identify LSs. However, limited work has been conducted on the comparison of deep learning methods in this context. The findings suggest the need to consider and stimulate further empirical investigation in documenting the adoption and comparison of deep learning algorithms in classifying LSs to provide higher adaptability.",
  "keyword": "machine learning"
 },
 {
  "No": 498,
  "judul": "A Fine-Grained System Driven of Attacks Over Several New Representation Techniques Using Machine Learning",
  "abstrak": "Machine Learning (ML) techniques, especially deep learning, are crucial to many contemporary real world systems that use Computational Intelligence (CI) as their core technology, including self-deriving vehicles, assisting machines, and biometric authentication systems. We encounter a lot of attacks these days. Drive-by-download is used to covertly download websites when we view them, and emails we receive often have malicious attachments. The affected hosts and networks sustain significant harm as a result of the infection. Therefore, identifying malware is crucial. Recent attacks, however, is designed to evade detection using Intrusion Detection System (IDS). It is essential to create fresh signatures as soon as new malware is found in order to stop this issue. Using a variety of cutting-edge representation methodologies, we develop attack taxonomy and examine it. 1) N-gram-based representation: In this tactic, we look at a number of random representations that consider a technique of sampling the properties of the graph. 2) Signature-based representation: This technique uses the idea of invariant representation of the graph, which is based on spectral graph theory. One of the main causes is that a ML system setup is rely on a number of variables, including the input dataset, ML architecture, attack creation process, and defense strategy. To find any hostile attacks in the network system, we employ IDS with Deep Neural Network (DNN). In conclusion, the efficacy and efficiency of the suggested framework with Convolutional Neural Network (CNN) and Support Vector Machine (SVM) are assessed using the assessment indicators, including throughput, latency rate, accuracy and precision. The findings of the suggested model with a detection rate of 93%, 14%, 95.63% and 95% in terms of throughput, latency rate, accuracy and precision, which is based on adversarial assault, were better and more effective than CNN and SVM models. Additionally at the end we contrast the performance of the suggested model with that of earlier research that makes use of the same dataset, NSL-KDD, as we do in our scenario.",
  "keyword": "machine learning"
 },
 {
  "No": 499,
  "judul": "Boltzmann Machine Learning and Regularization Methods for Inferring Evolutionary Fields and Couplings From a Multiple Sequence Alignment",
  "abstrak": "The inverse Potts problem to infer a Boltzmann distribution for homologous protein sequences from their single-site and pairwise amino acid frequencies recently attracts a great deal of attention in the studies of protein structure and evolution. We study regularization and learning methods and how to tune regularization parameters to correctly infer interactions in Boltzmann machine learning. Using L2 regularization for fields, group L1 for couplings is shown to be very effective for sparse couplings in comparison with L2 and L1 . Two regularization parameters are tuned to yield equal values for both the sample and ensemble averages of evolutionary energy. Both averages smoothly change and converge, but their learning profiles are very different between learning methods. The Adam method is modified to make stepsize proportional to the gradient for sparse couplings and to use a soft-thresholding function for group L1 . It is shown by first inferring interactions from protein sequences and then from Monte Carlo samples that the fields and couplings can be well recovered, but that recovering the pairwise correlations in the resolution of a total energy is harder for the natural proteins than for the protein-like sequences. Selective temperature for folding\/structural constrains in protein evolution is also estimated.",
  "keyword": "machine learning"
 },
 {
  "No": 500,
  "judul": "A Machine Learning Based Framework for a Stage-Wise Classification of Date Palm White Scale Disease",
  "abstrak": "Date palm production is critical to oasis agriculture, owing to its economic importance and nutritional advantages. Numerous diseases endanger this precious tree, putting a strain on the economy and environment. White scale Parlatoria blanchardi is a damaging bug that degrades the quality of dates. When an infestation reaches a specific degree, it might result in the tree's death. To counter this threat, precise detection of infected leaves and its infestation degree is important to decide if chemical treatment is necessary. This decision is crucial for farmers who wish to minimize yield losses while preserving production quality. For this purpose, we propose a feature extraction and machine learning (ML) technique based framework for classifying the stages of infestation by white scale disease (WSD) in date palm trees by investigating their leaflets images. 80 gray level co-occurrence matrix (GLCM) texture features and 9 hue, saturation, and value (HSV) color moments features are extracted from both grayscale and color images of the used dataset. To classify the WSD into its four classes (healthy, low infestation degree, medium infestation degree, and high infestation degree), two types of ML algorithms were tested; classical machine learning methods, namely, support vector machine (SVM) and k-nearest neighbors (KNN), and ensemble learning methods such as random forest (RF) and light gradient boosting machine (LightGBM). The ML models were trained and evaluated using two datasets: the first is composed of the extracted GLCM features only, and the second combines GLCM and HSV descriptors. The results indicate that SVM classifier outperformed on combined GLCM and HSV features with an accuracy of 98.29%. The proposed framework could be beneficial to the oasis agricultural community in terms of early detection of date palm white scale disease (DPWSD) and assisting in the adoption of preventive measures to protect both date palm trees and crop yield.",
  "keyword": "machine learning"
 }
]
}
